{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "8uB5dx0oh8GM",
    "outputId": "3cfd7854-f70a-4c0a-c2ab-057f8a895fd4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/team/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/team/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/team/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/team/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/team/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/team/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/team/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/team/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/team/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/team/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/team/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/team/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.14.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from konlpy.tag import Mecab\n",
    "import numpy as np\n",
    "# import tensorflow.compat.v1 as tf\n",
    "# tf.disable_v2_behavior()\n",
    "# %tensorflow_version 1.x\n",
    "import tensorflow as tf\n",
    "from tensorflow import feature_column\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import time\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
    "from tensorflow.keras import layers\n",
    "import pickle\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 322
    },
    "colab_type": "code",
    "id": "is_VN_T1iJvt",
    "outputId": "defdd45e-73ac-4f2a-9fe0-12e7c7e413aa"
   },
   "outputs": [],
   "source": [
    "news = pd.read_csv(\"./total_article_ver_half_20200427.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "tUVp_7RApGD2",
    "outputId": "d4a22e2f-dece-49b7-98eb-883b2fcfd053"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['article_category', 'article_title', 'article_content'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uErDdrQPpLkE"
   },
   "outputs": [],
   "source": [
    "news = news.drop(['article_id', 'article_url', 'article_media', 'article_date'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = news[news.article_category=='경제']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "photo = news.article_title.str.startswith('포토')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = news[~photo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "issue = news.article_title.str.startswith('이슈')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = news[~issue]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = news.drop(['article_category'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(217903, 205076)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(news), len(news.article_content.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = news.drop_duplicates(['article_content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205076"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0fL970yNpgfS"
   },
   "outputs": [],
   "source": [
    "def clean_text(text, remove_stopwords = False):\n",
    "    text = text.lower()\n",
    "    \n",
    "    if True:\n",
    "        text = text.split()\n",
    "        new_text = []\n",
    "        for word in text:\n",
    "            new_text.append(word)\n",
    "        text = \" \".join(new_text)\n",
    "    \n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\<a href', ' ', text)\n",
    "    text = re.sub(r'&amp;', '', text) \n",
    "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/’·…]', ' ', text)\n",
    "    text = re.sub(r'<br />', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "    text = re.sub(r'［[a-zA-Z가-힣]*］', '', text)\n",
    "    text = text.strip()\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        text = text.split()\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "        text = \" \".join(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "gtEOOwUVpiUL",
    "outputId": "8266a2b0-d813-4e4a-8658-f8968b419f9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries are complete.\n",
      "Texts are complete.\n"
     ]
    }
   ],
   "source": [
    "clean_summaries = []\n",
    "for summary in news.article_title:\n",
    "    clean_summaries.append(clean_text(summary))\n",
    "print(\"Summaries are complete.\")\n",
    "\n",
    "clean_texts = []\n",
    "for text in news.article_content:\n",
    "    clean_texts.append(clean_text(text))\n",
    "print(\"Texts are complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 109
    },
    "colab_type": "code",
    "id": "Ty_yGP54pj5t",
    "outputId": "5b4090f1-7729-40fc-af23-f799c198c636",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Review # 1\n",
      "carfun ℓ당 100200원 비싼 고급휘발유 돈값 할까\n",
      "수입차와 함께 급성장하는 고급휘발유시장 고성능차에 주로 쓰는 고급휘발유 국내 수요 6년새 2배 가까이 늘어 출력저하 등 노킹 방지 탁월하지만 국내 휘발유 품질 美 등보다 높아 일반차량은 보통휘발유로도 충분 연비 약간 좋아지는 효과는 있어 현대오일뱅크의 고급휘발유 전용 주유소 ‘카젠  사진제공 현대오일뱅크 서울경제 국내 수입차 이용자가 증가하며 고급휘발유 시장이 급성장하고 있다  애마 愛馬 의 엔진을 보호하고 연비를 높여주는 고급휘발유에 대한 관심이 높아지면서다  정유사들은 2010년 고유가와 함께 시장에서 자취를 감췄던 고급휘발유 마케팅에 다시 불을 붙이고 있다  1일 한국석유공사에 따르면 지난해 고급휘발유 수요는 하루 3 710배럴로 2013년 수요 하루 1 939배럴 의 약 2배 수준으로 늘었다  보통휘발유 수요가 2013년 하루 20만1 275배럴에서 지난해 22만6 948배럴로 약 12 8  증가한 것에 비해 급격한 증가율이다  연간 소비량으로 따져도 고급휘발유는 2016년 88만배럴에서 지난해 135만배럴로 연평균 15 5  늘어난 반면 보통휘발유는 연평균 1 4  증가하는 데 그쳤다  고급휘발유는 고출력 수입차에 주로 사용된다  자동차 연료유 시장에서 고급휘발유 수요의 급성장은 국내 수입차 운전자 수의 증가와 관련이 깊다  한국자동차산업협회에 따르면 국내 수입차 등록대수는 지난 2015년 139만2 035대에서 지난해 241만5 877대로 약 73 6  늘었다  2016년 이후 저유가가 계속되고 있는 점도 고급휘발유 시장 확대에 영향을 미쳤다  업계의 한 관계자는 “최근에는 젊은 층의 수입차 선호 현상이 두드러진다”며 “저유가 지속으로 과거보다는 가격 부담이 덜한 편이라 고급휘발유를 찾는 이들도 늘어나고 있다”고 분석했다  고급휘발유는 고출력 수입차에 보통휘발유를 쓸 때 발생할 수 있는 노킹 현상 방지에 탁월하다  노킹 현상은 휘발유의 이상연소로 엔진룸을 망치로 두드리는 듯한 소리가 나는 것이다  노킹 현상이 계속되면 엔진 출력이 떨어지고 심한 경우 부품까지 손상될 수 있다  하지만 엔진 특성상 일반 차량에서는 이런 현상이 발생하지 않는다  일반 차량에는 보통휘발유를 넣어도 아무 문제가 없다는 얘기다  국내에서는 노킹 현상을 방지해주는 정도인 ‘옥탄가 가 94를 넘으면 고급휘발유로 분류된다  다만 국내 정유사들은 대부분 옥탄가 97 이상의 고급휘발유를 판매하고 있다  국내 휘발유의 옥탄가 기준이 다른 나라보다 높은 편이라는 게 업계 관계자들의 설명이다  미국에서는 통상 옥탄가 91 93의 휘발유를 ‘프리미엄 으로 분류한다  국내에서는 보통휘발유의 옥탄가가 91 94 수준이다  옥탄가는 노킹 현상을 방지해주는 물질인 ‘이소옥탄 의 함유량에 따라 정해진다  휘발유에 이소옥탄이 90  들어 있으면 ‘옥탄가 90 으로 정해지는 식이다  여기에 국내 정유사들은 이소옥탄보다도 안티노킹 성능이 뛰어난 첨가제를 넣어 옥탄가를 높인다  옥탄가 100 이상의 고급휘발유가 판매될 수 있는 것은 이 때문이다  정유업계 관계자는 “첨가제를 사용하기 때문에 고급휘발유의 제조 비용이 올라간다”면서 “보통휘발유에 비해 부가세도 높아 리터당 가격이 100 200원 정도 차이 나게 되는 것”이라고 설명했다  일부 소비자들 사이에서는 ‘일반차량에 고급휘발유를 넣으면 엔진 등을 정화하는 효과가 있다 는 얘기도 나온다  하지만 업계 관계자들은 이것이 과학적으로 입증된 이야기는 아니라고 선을 그었다  정유업계의 한 관계자는 “특정 옥탄가 이상의 휘발유를 주입하라고 안내된 차 외에는 고급휘발유를 넣어서 특별히 달라지는 점이 없다”며 “연비가 약간 좋아지는 효과는 있을 것”이라고 봤다  최근에는 현대오일뱅크가 고급휘발유 브랜드 ‘카젠 kazen  을 재출시하며 시장 공략에 속도를 내고 있다  현대오일뱅크에 따르면 ‘카젠 의 옥탄가는 100 이상으로 업계 최고 수준이다  지난해 국내 최대 레이싱 대회인 슈퍼레이스 챔피언십의 공식 연료로 선정되며 기술력을 인정받기도 했다  현대오일뱅크 관계자는 “연말까지 취급점을 현재의 2배인 300개로 확대하고 10 대인 시장 점유율을 25 까지 끌어올릴 것”이라고 말했다  gs 078930 칼텍스도 2006년 고급휘발유 ‘킥스 kixx 프라임 을 출시한 이래 옥탄가 100 수준을 유지 중이다\n",
      "\n",
      "Clean Review # 2\n",
      "규제 빗겨간 ‘웅천 롯데캐슬 마리나  분양 일정 및 조건은\n",
      "청약통장 필요 없이 당첨 후 바로 전매 가능 1차 계약금 정액제 중도금 전액 무이자 등 금융혜택도 제공 서울경제 아시아신탁 위탁자 포브로스 이 시행 및 분양하고 롯데건설이 단순 시공을 맡은 ‘웅천 롯데캐슬 마리나 가 3월 2일 청약접수를 실시한다  세컨드하우스 열풍이 불고 있는 해양도시 내 오피스텔 상품인 데다 각종 규제를 비껴가 많은 관심이 예상된다  웅천 롯데캐슬 마리나는 여수시 웅천동 1887 1 일원에 지하 3층 지상 7층 5개 동 전용 28 70㎡ 총 550실 규모로 조성된다  전용면적 별 실수는 28㎡ 120실 29㎡ 10실 32㎡ 70실 33㎡ 156실 34㎡ 147실 39㎡ 5실 47㎡ 5실 54㎡ 5실 70㎡ 32실이다  단지의 청약접수는 새 청약시스템인 ‘청약홈 을 통해 받는다  청약접수는 면적별로 a군 전용 32 54 70㎡ b군 전용 33 39 47㎡ c군 전용 28 29 34㎡ 으로 나눠 진행된다  청약신청금은 100만원으로 3개군 동시 청약이 가능하다  실제 단지는 다양한 규제로부터 자유로운 것이 장점이다  먼저 만 19세 이상이라면 청약통장 없이 누구나 청약이 가능하다  또 거주지 요건 및 다주택자 규제가 적용되지 않는다  여기에 전매제한이 없어 당첨 후 바로 판매가 가능하고 1가구 2주택에도 해당되지 않는다  또한 단지는 소비자들의 비용 부담을 덜어줄 다양한 금융혜택도 제공한다  계약금 1 000만원 정액제 1차 와 중도금 전액 무이자 혜택이 도입된다  분양관계자는 “규제가 심한 아파트와 달리 각종 청약규제로부터 자유롭다는 점에서 수요자들의 많은 관심이 이어지고 있다”며 “계약자를 위한 금융혜택도 제공하는 만큼 청약에 많은 접수가 몰릴 것으로 기대하고 있다”고 말했다  한편 지난 28일 문을 연 웅천 롯데캐슬 마리나 견본주택에는 많은 인파가 몰리며 단지에 대한 높은 관심을 실감케 했다  특히 견본주택에서 만난 내방객들은 단지가 여수 웅천지구에서도 핵심인 마리나 항만 바로 앞에 자리한다는 점과 프리미엄 브랜드 롯데캐슬의 상품성에 만족도를 보였다  실제로 견본주택의 한 내방객은 “바다를 매일 볼 수 있다는 점과 다양한 특화설계로 쾌적한 주거환경을 누릴 수 있다는 점이 마음에 든다”며 “실거주도 가능하고 투자용도의 세컨드하우스로도 활용할 수 있을 것 같아 청약해 볼 생각”이라고 소감을 말했다  실제 단지는 여수 웅천지구의 미래가치를 모두 누리는 핵심입지에 들어선다  먼저 마리나 항만 바로 앞에 들어서 바다 영구 조망 일부제외 이 가능하다  또 인근에는 300척 규모의 국가 거점형 마리나 항만 2022년 완공예정 과 오션퀸즈파크 챌린지파크 등 다양한 관광 레저시설의 개발사업이 조성되고 있어 이를 가깝게 누릴 수 있는 단지로도 기대감이 높다  주거여건도 돋보인다  주변에는 여의도공원 1 5배 크기의 이순신공원과 해변문화공원 이순신마리나 오동도 돌산도 해수욕장 등이 자리해 풍부한 녹지공간을 갖췄다  또 ktx여천역 및 ktx여수엑스포역을 비롯해 여수공항 종합버스터미널 연안여객터미널 등이 인접했다  2022년 6월 개통 예정인 웅천 소호대교가 가까워 교통환경도 편리하다  이 밖에 cgv 메가박스 병원 등 다양한 생활편의시설을 손 쉽게 이용 가능하며 단지 내에는 판매시설이 함께 구성돼 입주민의 주거 편의성은 더욱 높아질 것으로 전망된다  단지는 롯데캐슬의 특화 설계도 적용된다  먼저 평면설계는 최근 증가하고 있는 1 2인 가구 위주의 구성과 다락형 설계가 도입된다  또 로드 road 형 단지배치를 통해 오션뷰 조망도 극대화 했다  이 밖에도 옥상 정원 피트니스클럽 등의 커뮤니티 시설 역시 바다를 바라볼 수 있게 꾸며 입주자에게 특별한 일상을 선사한다는 계획이다  웅천 롯데캐슬 마리나의 향후 일정으로는 3월 2일 월요일 청약접수 이후 5일 목요일에 당첨자 발표를 진행한다  정당계약은 6일 금요일에 실시한다  웅천 롯데캐슬 마리나의 견본주택은 여수시 웅천동 일대에 마련돼 있으며 입주는 2022년 6월 예정이다\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    print(\"Clean Review #\",i+1)\n",
    "    print(clean_summaries[i])\n",
    "    print(clean_texts[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "adjayr8Mpnql"
   },
   "outputs": [],
   "source": [
    "def count_words(count_dict, text):\n",
    "    for sentence in text:\n",
    "        for word in sentence.split():\n",
    "            if word not in count_dict:\n",
    "                count_dict[word] = 1\n",
    "            else:\n",
    "                count_dict[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 토크나이저_mecab\n",
    "- 대용량 텍스트 처리 시 mecab이 가장 속도가 빠르지만 window운영체제에서 작동하지 않기 때문에 proto타입 생성 시 Okt를 사용\n",
    "- 서버에서 본 모형 생성 시 mecab으로 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5SSiFp2Bq24U"
   },
   "outputs": [],
   "source": [
    "tokenizer = Mecab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MieOj2fGr-HG"
   },
   "outputs": [],
   "source": [
    "tokens_summaries = [' '.join(tokenizer.morphs(sentence)) for sentence in clean_summaries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_texts = [' '.join(tokenizer.morphs(sentence)) for sentence in clean_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "_nDdgyQRprti",
    "outputId": "7acc9794-6afd-42b8-e282-64a58985114d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Vocabulary: 153023\n"
     ]
    }
   ],
   "source": [
    "word_counts = {}\n",
    "\n",
    "count_words(word_counts, tokens_summaries)\n",
    "count_words(word_counts, tokens_texts)\n",
    "            \n",
    "print(\"Size of Vocabulary:\", len(word_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "KB1bztQAddw2",
    "outputId": "830e1f2d-fb51-4bc9-90b3-e79a5bcdaf4e"
   },
   "outputs": [],
   "source": [
    "threshold = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "u0m2_PyOdlBO",
    "outputId": "ef62ae3b-3f2a-45e0-fd58-1b5be062b996"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique words: 153023\n",
      "Number of words we will use: 115306\n",
      "Percent of words we will use: 75.35%\n"
     ]
    }
   ],
   "source": [
    "vocab_to_int = {} \n",
    "\n",
    "value = 0\n",
    "for word, count in word_counts.items():\n",
    "    if count >= threshold or word in embeddings_index:\n",
    "        vocab_to_int[word] = value\n",
    "        value += 1\n",
    "\n",
    "codes = [\"<UNK>\",\"<PAD>\",\"<EOS>\",\"<GO>\"]   \n",
    "\n",
    "for code in codes:\n",
    "    vocab_to_int[code] = len(vocab_to_int)\n",
    "\n",
    "int_to_vocab = {}\n",
    "for word, value in vocab_to_int.items():\n",
    "    int_to_vocab[value] = word\n",
    "\n",
    "usage_ratio = round(len(vocab_to_int) / len(word_counts),4)*100\n",
    "\n",
    "print(\"Total number of unique words:\", len(word_counts))\n",
    "print(\"Number of words we will use:\", len(vocab_to_int))\n",
    "print(\"Percent of words we will use: {}%\".format(usage_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "6wsgHHrddoOO",
    "outputId": "64a203ff-f248-4d04-c300-406a4f9d4f11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115306\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 300\n",
    "nb_words = len(vocab_to_int)\n",
    "\n",
    "word_embedding_matrix = np.zeros((nb_words, embedding_dim), dtype=np.float32)\n",
    "for word, i in vocab_to_int.items():\n",
    "    if word in embeddings_index:\n",
    "        word_embedding_matrix[i] = embeddings_index[word]\n",
    "    else:\n",
    "        new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
    "        embeddings_index[word] = new_embedding\n",
    "        word_embedding_matrix[i] = new_embedding\n",
    "\n",
    "print(len(word_embedding_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1viac7uEdzO2"
   },
   "outputs": [],
   "source": [
    "def convert_to_ints(text, word_count, unk_count, eos=False):\n",
    "    ints = []\n",
    "    for sentence in text:\n",
    "        sentence_ints = []\n",
    "        for word in sentence.split():\n",
    "            word_count += 1\n",
    "            if word in vocab_to_int:\n",
    "                sentence_ints.append(vocab_to_int[word])\n",
    "            else:\n",
    "                sentence_ints.append(vocab_to_int[\"<UNK>\"])\n",
    "                unk_count += 1\n",
    "        if eos:\n",
    "            sentence_ints.append(vocab_to_int[\"<EOS>\"])\n",
    "        ints.append(sentence_ints)\n",
    "    return ints, word_count, unk_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "YrUtyzBgd08m",
    "outputId": "085fa3d6-7d6f-4449-f60c-b935b874adcf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in headlines: 87091873\n",
      "Total number of UNKs in headlines: 37721\n",
      "Percent of words that are UNK: 0.04%\n"
     ]
    }
   ],
   "source": [
    "word_count = 0\n",
    "unk_count = 0\n",
    "\n",
    "int_summaries, word_count, unk_count = convert_to_ints(tokens_summaries, word_count, unk_count)\n",
    "int_texts, word_count, unk_count = convert_to_ints(tokens_texts, word_count, unk_count, eos=True)\n",
    "\n",
    "unk_percent = round(unk_count/word_count,4)*100\n",
    "\n",
    "print(\"Total number of words in headlines:\", word_count)\n",
    "print(\"Total number of UNKs in headlines:\", unk_count)\n",
    "print(\"Percent of words that are UNK: {}%\".format(unk_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZEqEWFKMd2fZ"
   },
   "outputs": [],
   "source": [
    "def create_lengths(text):\n",
    "    lengths = []\n",
    "    for sentence in text:\n",
    "        lengths.append(len(sentence))\n",
    "    return pd.DataFrame(lengths, columns=['counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 395
    },
    "colab_type": "code",
    "id": "YH6dXK-sd4Uu",
    "outputId": "8d457493-9321-4d7f-c5e5-0977405a6bbe",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries:\n",
      "              counts\n",
      "count  205076.000000\n",
      "mean       11.951233\n",
      "std         3.134620\n",
      "min         1.000000\n",
      "25%        10.000000\n",
      "50%        12.000000\n",
      "75%        14.000000\n",
      "max        38.000000\n",
      "\n",
      "Texts:\n",
      "              counts\n",
      "count  205076.000000\n",
      "mean      413.729729\n",
      "std       311.816657\n",
      "min         4.000000\n",
      "25%       214.000000\n",
      "50%       345.000000\n",
      "75%       532.000000\n",
      "max      8025.000000\n"
     ]
    }
   ],
   "source": [
    "lengths_summaries = create_lengths(int_summaries)\n",
    "lengths_texts = create_lengths(int_texts)\n",
    "\n",
    "print(\"Summaries:\")\n",
    "print(lengths_summaries.describe())\n",
    "print()\n",
    "print(\"Texts:\")\n",
    "print(lengths_texts.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "VUA5_t-Td5gO",
    "outputId": "0a5ea08c-9c5c-42b2-d67b-5f9a5dff3de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "780.0\n",
      "957.0\n",
      "1463.0\n"
     ]
    }
   ],
   "source": [
    "print(np.percentile(lengths_texts.counts, 90))\n",
    "print(np.percentile(lengths_texts.counts, 95))\n",
    "print(np.percentile(lengths_texts.counts, 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "ohTOC90Yd7bG",
    "outputId": "4f487cbe-05f4-43ba-eeac-84cea30b433e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.0\n",
      "17.0\n",
      "20.0\n"
     ]
    }
   ],
   "source": [
    "print(np.percentile(lengths_summaries.counts, 90))\n",
    "print(np.percentile(lengths_summaries.counts, 95))\n",
    "print(np.percentile(lengths_summaries.counts, 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k2delLqDeM_2"
   },
   "outputs": [],
   "source": [
    "def unk_counter(sentence):\n",
    "    unk_count = 0\n",
    "    for word in sentence:\n",
    "        if word == vocab_to_int[\"<UNK>\"]:\n",
    "            unk_count += 1\n",
    "    return unk_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "Mnd_CZjqePEJ",
    "outputId": "655b87a7-2acc-4677-cb20-9feaca621bf7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126621\n",
      "126621\n"
     ]
    }
   ],
   "source": [
    "sorted_summaries = []\n",
    "sorted_texts = []\n",
    "max_text_length = 420\n",
    "max_summary_length = 20\n",
    "min_length = 8\n",
    "unk_text_limit = 0\n",
    "unk_summary_limit = 0\n",
    "\n",
    "for length in range(min(lengths_texts.counts), max_text_length): \n",
    "    for count, words in enumerate(int_summaries):\n",
    "        if (len(int_summaries[count]) <= max_summary_length and\n",
    "            len(int_texts[count]) >= min_length and\n",
    "            length == len(int_texts[count])\n",
    "           ):\n",
    "            sorted_summaries.append(int_summaries[count])\n",
    "            sorted_texts.append(int_texts[count])\n",
    "        \n",
    "print(len(sorted_summaries))\n",
    "print(len(sorted_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 로드 시 여기서부터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "02XRGGlJeQri"
   },
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    \n",
    "    input_data = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    summary_length = tf.placeholder(tf.int32, (None,), name='summary_length')\n",
    "    max_summary_length = tf.reduce_max(summary_length, name='max_dec_len')\n",
    "    text_length = tf.placeholder(tf.int32, (None,), name='text_length')\n",
    "\n",
    "    return input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HznJc1s7frZ1"
   },
   "outputs": [],
   "source": [
    "def process_encoding_input(target_data, vocab_to_int, batch_size):\n",
    "    \n",
    "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n",
    "\n",
    "    return dec_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WxflrP8KftBN"
   },
   "outputs": [],
   "source": [
    "def encoding_layer(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob):\n",
    "    \n",
    "    for layer in range(num_layers):\n",
    "        with tf.variable_scope('encoder_{}'.format(layer)):\n",
    "            cell_fw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, \n",
    "                                                    input_keep_prob = keep_prob)\n",
    "\n",
    "            cell_bw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, \n",
    "                                                    input_keep_prob = keep_prob)\n",
    "\n",
    "            enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw, \n",
    "                                                                    cell_bw, \n",
    "                                                                    rnn_inputs,\n",
    "                                                                    sequence_length,\n",
    "                                                                    dtype=tf.float32)\n",
    "    enc_output = tf.concat(enc_output,2)\n",
    "    \n",
    "    return enc_output, enc_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4dQ4KP3hfu9d"
   },
   "outputs": [],
   "source": [
    "def training_decoding_layer(dec_embed_input, summary_length, dec_cell, initial_state, output_layer, \n",
    "                            vocab_size, max_summary_length):\n",
    "    \n",
    "    training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
    "                                                        sequence_length=summary_length,\n",
    "                                                        time_major=False)\n",
    "\n",
    "    training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                       training_helper,\n",
    "                                                       initial_state,\n",
    "                                                       output_layer) \n",
    "\n",
    "    training_logits, _ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
    "                                                           output_time_major=False,\n",
    "                                                           impute_finished=True,\n",
    "                                                           maximum_iterations=max_summary_length)\n",
    "    return training_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PKO3yyWHfwQV"
   },
   "outputs": [],
   "source": [
    "def inference_decoding_layer(embeddings, start_token, end_token, dec_cell, initial_state, output_layer,\n",
    "                             max_summary_length, batch_size):\n",
    "    \n",
    "    start_tokens = tf.tile(tf.constant([start_token], dtype=tf.int32), [batch_size], name='start_tokens')\n",
    "    \n",
    "    inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,\n",
    "                                                                start_tokens,\n",
    "                                                                end_token)\n",
    "                \n",
    "    inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                        inference_helper,\n",
    "                                                        initial_state,\n",
    "                                                        output_layer)\n",
    "                \n",
    "    inference_logits, _ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
    "                                                            output_time_major=False,\n",
    "                                                            impute_finished=True,\n",
    "                                                            maximum_iterations=max_summary_length)\n",
    "    \n",
    "    return inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "erhPVLLXfxk9"
   },
   "outputs": [],
   "source": [
    "def decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, text_length, summary_length, \n",
    "                   max_summary_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers):\n",
    "    \n",
    "    for layer in range(num_layers):\n",
    "        with tf.variable_scope('decoder_{}'.format(layer)):\n",
    "            lstm = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                           initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            dec_cell = tf.contrib.rnn.DropoutWrapper(lstm, \n",
    "                                                     input_keep_prob = keep_prob)\n",
    "    \n",
    "    output_layer = Dense(vocab_size,\n",
    "                         kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
    "    \n",
    "    attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size,\n",
    "                                                  enc_output,\n",
    "                                                  text_length,\n",
    "                                                  normalize=False,\n",
    "                                                  name='BahdanauAttention')\n",
    "    \n",
    "    with tf.name_scope(\"Attention_Wrapper\"):\n",
    "        \n",
    "        dec_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell,\n",
    "                                                          attn_mech,\n",
    "                                                          rnn_size)\n",
    "    initial_state = dec_cell.zero_state(dtype=tf.float32, batch_size=batch_size)\n",
    "    \n",
    "    with tf.variable_scope(\"decode\"):\n",
    "        \n",
    "        training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
    "                                                            sequence_length=summary_length,\n",
    "                                                            time_major=False)\n",
    "        training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                           training_helper,\n",
    "                                                           initial_state,\n",
    "                                                           output_layer) \n",
    "        training_logits, _ ,_ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
    "                                            output_time_major=False,\n",
    "                                            impute_finished=True,\n",
    "                                            maximum_iterations=max_summary_length)\n",
    "        \n",
    "    with tf.variable_scope(\"decode\", reuse=True):\n",
    "        \n",
    "        start_tokens = tf.tile(tf.constant([vocab_to_int['<GO>']], dtype=tf.int32), [batch_size], name='start_tokens')\n",
    "        end_token = (tf.constant(vocab_to_int['<EOS>'], dtype=tf.int32))\n",
    "        inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,\n",
    "                                                                    start_tokens,\n",
    "                                                                    end_token)\n",
    "        inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                            inference_helper,\n",
    "                                                            initial_state,\n",
    "                                                            output_layer)\n",
    "        inference_logits, _ ,_ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
    "                                            output_time_major=False,\n",
    "                                            impute_finished=True,\n",
    "                                            maximum_iterations=max_summary_length)\n",
    "\n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hLjL__ehf3uU"
   },
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, target_data, keep_prob, text_length, summary_length, max_summary_length, \n",
    "                  vocab_size, rnn_size, num_layers, vocab_to_int, batch_size):\n",
    "    \n",
    "    embeddings = word_embedding_matrix\n",
    "    \n",
    "    enc_embed_input = tf.nn.embedding_lookup(embeddings, input_data)\n",
    "    enc_output, enc_state = encoding_layer(rnn_size, text_length, num_layers, enc_embed_input, keep_prob)\n",
    "    \n",
    "    dec_input = process_encoding_input(target_data, vocab_to_int, batch_size)\n",
    "    dec_embed_input = tf.nn.embedding_lookup(embeddings, dec_input)\n",
    "    \n",
    "    training_logits, inference_logits  = decoding_layer(dec_embed_input, \n",
    "                                                        embeddings,\n",
    "                                                        enc_output,\n",
    "                                                        enc_state, \n",
    "                                                        vocab_size, \n",
    "                                                        text_length, \n",
    "                                                        summary_length, \n",
    "                                                        max_summary_length,\n",
    "                                                        rnn_size, \n",
    "                                                        vocab_to_int, \n",
    "                                                        keep_prob, \n",
    "                                                        batch_size,\n",
    "                                                        num_layers)\n",
    "    \n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sHBNZaEof5oO"
   },
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch):\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [vocab_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VUU6z0Jcf7Th"
   },
   "outputs": [],
   "source": [
    "def get_batches(summaries, texts, batch_size):\n",
    "    for batch_i in range(0, len(texts)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        summaries_batch = summaries[start_i:start_i + batch_size]\n",
    "        texts_batch = texts[start_i:start_i + batch_size]\n",
    "        pad_summaries_batch = np.array(pad_sentence_batch(summaries_batch))\n",
    "        pad_texts_batch = np.array(pad_sentence_batch(texts_batch))\n",
    "        \n",
    "        pad_summaries_lengths = []\n",
    "        for summary in pad_summaries_batch:\n",
    "            pad_summaries_lengths.append(len(summary))\n",
    "        \n",
    "        pad_texts_lengths = []\n",
    "        for text in pad_texts_batch:\n",
    "            pad_texts_lengths.append(len(text))\n",
    "        \n",
    "        yield pad_summaries_batch, pad_texts_batch, pad_summaries_lengths, pad_texts_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qUz99_AKf8x0"
   },
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "batch_size = 64\n",
    "rnn_size = 256\n",
    "num_layers = 3\n",
    "learning_rate = 0.005\n",
    "keep_probability = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "4wZuJMvtf-8n",
    "outputId": "657a971c-5056-461f-ebcf-8776b2756c40",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-10-9390cb26606a>:7: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-10-9390cb26606a>:20: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/team/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/team/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py:961: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/team/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/team/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Graph is built.\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/device:XLA_GPU:0'):\n",
    "    train_graph = tf.Graph()\n",
    "    with train_graph.as_default():\n",
    "\n",
    "        input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length = model_inputs()\n",
    "\n",
    "        training_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n",
    "                                                          targets, \n",
    "                                                          keep_prob,   \n",
    "                                                          text_length,\n",
    "                                                          summary_length,\n",
    "                                                          max_summary_length,\n",
    "                                                          len(vocab_to_int)+1,\n",
    "                                                          rnn_size, \n",
    "                                                          num_layers, \n",
    "                                                          vocab_to_int,\n",
    "                                                          batch_size)\n",
    "\n",
    "        training_logits = tf.identity(training_logits.rnn_output, 'logits')\n",
    "        inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
    "\n",
    "        masks = tf.sequence_mask(summary_length, max_summary_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "        with tf.name_scope(\"optimization\"):\n",
    "            cost = tf.contrib.seq2seq.sequence_loss(\n",
    "                training_logits,\n",
    "                targets,\n",
    "                masks)\n",
    "\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "            gradients = optimizer.compute_gradients(cost)\n",
    "            capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "            train_op = optimizer.apply_gradients(capped_gradients)\n",
    "    print(\"Graph is built.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "AN526RuygAfs",
    "outputId": "b34cb3f4-1f4b-423e-a87f-f9e10e07e18d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shortest text length: 11\n",
      "The longest text length: 419\n"
     ]
    }
   ],
   "source": [
    "start = 0\n",
    "end = len(sorted_texts)\n",
    "sorted_summaries_short = sorted_summaries[start:end]\n",
    "sorted_texts_short = sorted_texts[start:end]\n",
    "print(\"The shortest text length:\", len(sorted_texts_short[0]))\n",
    "print(\"The longest text length:\",len(sorted_texts_short[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/30 Batch   10/1978 - Loss:  8.511, Seconds: 2.29\n",
      "Epoch   1/30 Batch   20/1978 - Loss:  5.335, Seconds: 2.12\n",
      "Epoch   1/30 Batch   30/1978 - Loss:  4.753, Seconds: 2.23\n",
      "Epoch   1/30 Batch   40/1978 - Loss:  4.261, Seconds: 2.38\n",
      "Epoch   1/30 Batch   50/1978 - Loss:  4.155, Seconds: 2.76\n",
      "Epoch   1/30 Batch   60/1978 - Loss:  4.082, Seconds: 2.18\n",
      "Epoch   1/30 Batch   70/1978 - Loss:  4.155, Seconds: 2.25\n",
      "Epoch   1/30 Batch   80/1978 - Loss:  3.740, Seconds: 2.93\n",
      "Epoch   1/30 Batch   90/1978 - Loss:  3.624, Seconds: 2.86\n",
      "Epoch   1/30 Batch  100/1978 - Loss:  3.504, Seconds: 2.42\n",
      "Epoch   1/30 Batch  110/1978 - Loss:  3.795, Seconds: 2.81\n",
      "Epoch   1/30 Batch  120/1978 - Loss:  3.891, Seconds: 2.64\n",
      "Epoch   1/30 Batch  130/1978 - Loss:  3.524, Seconds: 2.76\n",
      "Epoch   1/30 Batch  140/1978 - Loss:  3.860, Seconds: 2.95\n",
      "Epoch   1/30 Batch  150/1978 - Loss:  3.649, Seconds: 3.06\n",
      "Epoch   1/30 Batch  160/1978 - Loss:  3.610, Seconds: 3.32\n",
      "Epoch   1/30 Batch  170/1978 - Loss:  3.658, Seconds: 2.83\n",
      "Epoch   1/30 Batch  180/1978 - Loss:  3.725, Seconds: 2.68\n",
      "Epoch   1/30 Batch  190/1978 - Loss:  3.730, Seconds: 2.94\n",
      "Epoch   1/30 Batch  200/1978 - Loss:  3.693, Seconds: 2.87\n",
      "Epoch   1/30 Batch  210/1978 - Loss:  3.427, Seconds: 3.21\n",
      "Epoch   1/30 Batch  220/1978 - Loss:  3.166, Seconds: 3.11\n",
      "Epoch   1/30 Batch  230/1978 - Loss:  3.174, Seconds: 3.51\n",
      "Epoch   1/30 Batch  240/1978 - Loss:  2.881, Seconds: 3.05\n",
      "Epoch   1/30 Batch  250/1978 - Loss:  2.862, Seconds: 3.56\n",
      "Epoch   1/30 Batch  260/1978 - Loss:  2.819, Seconds: 3.58\n",
      "Epoch   1/30 Batch  270/1978 - Loss:  2.958, Seconds: 2.99\n",
      "Epoch   1/30 Batch  280/1978 - Loss:  3.041, Seconds: 3.33\n",
      "Epoch   1/30 Batch  290/1978 - Loss:  3.012, Seconds: 3.43\n",
      "Epoch   1/30 Batch  300/1978 - Loss:  2.899, Seconds: 3.61\n",
      "Epoch   1/30 Batch  310/1978 - Loss:  2.800, Seconds: 3.88\n",
      "Epoch   1/30 Batch  320/1978 - Loss:  2.791, Seconds: 3.82\n",
      "Epoch   1/30 Batch  330/1978 - Loss:  2.800, Seconds: 3.69\n",
      "Epoch   1/30 Batch  340/1978 - Loss:  2.750, Seconds: 3.91\n",
      "Epoch   1/30 Batch  350/1978 - Loss:  2.887, Seconds: 4.09\n",
      "Epoch   1/30 Batch  360/1978 - Loss:  2.774, Seconds: 3.56\n",
      "Epoch   1/30 Batch  370/1978 - Loss:  2.963, Seconds: 3.46\n",
      "Epoch   1/30 Batch  380/1978 - Loss:  2.539, Seconds: 4.00\n",
      "Epoch   1/30 Batch  390/1978 - Loss:  2.611, Seconds: 3.57\n",
      "Epoch   1/30 Batch  400/1978 - Loss:  2.583, Seconds: 4.06\n",
      "Epoch   1/30 Batch  410/1978 - Loss:  2.885, Seconds: 3.95\n",
      "Epoch   1/30 Batch  420/1978 - Loss:  2.896, Seconds: 4.09\n",
      "Epoch   1/30 Batch  430/1978 - Loss:  2.879, Seconds: 4.01\n",
      "Epoch   1/30 Batch  440/1978 - Loss:  2.781, Seconds: 3.75\n",
      "Epoch   1/30 Batch  450/1978 - Loss:  2.801, Seconds: 4.20\n",
      "Epoch   1/30 Batch  460/1978 - Loss:  2.583, Seconds: 3.95\n",
      "Epoch   1/30 Batch  470/1978 - Loss:  2.531, Seconds: 4.24\n",
      "Epoch   1/30 Batch  480/1978 - Loss:  2.748, Seconds: 3.69\n",
      "Epoch   1/30 Batch  490/1978 - Loss:  2.639, Seconds: 4.12\n",
      "Epoch   1/30 Batch  500/1978 - Loss:  2.782, Seconds: 4.20\n",
      "Epoch   1/30 Batch  510/1978 - Loss:  2.908, Seconds: 4.33\n",
      "Epoch   1/30 Batch  520/1978 - Loss:  2.695, Seconds: 3.94\n",
      "Epoch   1/30 Batch  530/1978 - Loss:  2.705, Seconds: 4.38\n",
      "Epoch   1/30 Batch  540/1978 - Loss:  2.666, Seconds: 3.74\n",
      "Epoch   1/30 Batch  550/1978 - Loss:  2.681, Seconds: 4.46\n",
      "Epoch   1/30 Batch  560/1978 - Loss:  2.729, Seconds: 4.15\n",
      "Epoch   1/30 Batch  570/1978 - Loss:  2.732, Seconds: 4.34\n",
      "Epoch   1/30 Batch  580/1978 - Loss:  2.595, Seconds: 4.34\n",
      "Epoch   1/30 Batch  590/1978 - Loss:  2.508, Seconds: 4.37\n",
      "Epoch   1/30 Batch  600/1978 - Loss:  2.514, Seconds: 4.36\n",
      "Epoch   1/30 Batch  610/1978 - Loss:  2.344, Seconds: 4.56\n",
      "Epoch   1/30 Batch  620/1978 - Loss:  2.444, Seconds: 4.56\n",
      "Epoch   1/30 Batch  630/1978 - Loss:  2.693, Seconds: 4.58\n",
      "Epoch   1/30 Batch  640/1978 - Loss:  2.536, Seconds: 4.47\n",
      "Epoch   1/30 Batch  650/1978 - Loss:  2.418, Seconds: 4.62\n",
      "Average loss for this update: 3.181\n",
      "New Record!\n",
      "Epoch   1/30 Batch  660/1978 - Loss:  2.035, Seconds: 4.69\n",
      "Epoch   1/30 Batch  670/1978 - Loss:  1.961, Seconds: 4.71\n",
      "Epoch   1/30 Batch  680/1978 - Loss:  1.996, Seconds: 4.50\n",
      "Epoch   1/30 Batch  690/1978 - Loss:  1.965, Seconds: 4.69\n",
      "Epoch   1/30 Batch  700/1978 - Loss:  2.221, Seconds: 4.41\n",
      "Epoch   1/30 Batch  710/1978 - Loss:  1.957, Seconds: 4.69\n",
      "Epoch   1/30 Batch  720/1978 - Loss:  2.095, Seconds: 4.46\n",
      "Epoch   1/30 Batch  730/1978 - Loss:  2.159, Seconds: 4.61\n",
      "Epoch   1/30 Batch  740/1978 - Loss:  2.387, Seconds: 4.76\n",
      "Epoch   1/30 Batch  750/1978 - Loss:  2.442, Seconds: 4.80\n",
      "Epoch   1/30 Batch  760/1978 - Loss:  2.634, Seconds: 4.65\n",
      "Epoch   1/30 Batch  770/1978 - Loss:  2.484, Seconds: 4.84\n",
      "Epoch   1/30 Batch  780/1978 - Loss:  2.437, Seconds: 4.85\n",
      "Epoch   1/30 Batch  790/1978 - Loss:  2.521, Seconds: 4.85\n",
      "Epoch   1/30 Batch  800/1978 - Loss:  2.565, Seconds: 4.87\n",
      "Epoch   1/30 Batch  810/1978 - Loss:  2.616, Seconds: 4.76\n",
      "Epoch   1/30 Batch  820/1978 - Loss:  2.475, Seconds: 4.91\n",
      "Epoch   1/30 Batch  830/1978 - Loss:  2.511, Seconds: 4.94\n",
      "Epoch   1/30 Batch  840/1978 - Loss:  2.586, Seconds: 4.97\n",
      "Epoch   1/30 Batch  850/1978 - Loss:  2.615, Seconds: 4.97\n",
      "Epoch   1/30 Batch  860/1978 - Loss:  2.566, Seconds: 5.01\n",
      "Epoch   1/30 Batch  870/1978 - Loss:  2.549, Seconds: 5.05\n",
      "Epoch   1/30 Batch  880/1978 - Loss:  2.584, Seconds: 5.01\n",
      "Epoch   1/30 Batch  890/1978 - Loss:  2.635, Seconds: 5.07\n",
      "Epoch   1/30 Batch  900/1978 - Loss:  2.670, Seconds: 4.92\n",
      "Epoch   1/30 Batch  910/1978 - Loss:  2.625, Seconds: 4.79\n",
      "Epoch   1/30 Batch  920/1978 - Loss:  2.641, Seconds: 5.03\n",
      "Epoch   1/30 Batch  930/1978 - Loss:  2.636, Seconds: 4.99\n",
      "Epoch   1/30 Batch  940/1978 - Loss:  2.886, Seconds: 4.85\n",
      "Epoch   1/30 Batch  950/1978 - Loss:  2.862, Seconds: 4.87\n",
      "Epoch   1/30 Batch  960/1978 - Loss:  2.824, Seconds: 5.07\n",
      "Epoch   1/30 Batch  970/1978 - Loss:  2.796, Seconds: 4.60\n",
      "Epoch   1/30 Batch  980/1978 - Loss:  2.708, Seconds: 5.10\n",
      "Epoch   1/30 Batch  990/1978 - Loss:  2.651, Seconds: 5.23\n",
      "Epoch   1/30 Batch 1000/1978 - Loss:  2.869, Seconds: 5.12\n",
      "Epoch   1/30 Batch 1010/1978 - Loss:  2.717, Seconds: 5.28\n",
      "Epoch   1/30 Batch 1020/1978 - Loss:  2.817, Seconds: 5.17\n",
      "Epoch   1/30 Batch 1030/1978 - Loss:  2.791, Seconds: 5.33\n",
      "Epoch   1/30 Batch 1040/1978 - Loss:  2.949, Seconds: 5.31\n",
      "Epoch   1/30 Batch 1050/1978 - Loss:  2.926, Seconds: 5.23\n",
      "Epoch   1/30 Batch 1060/1978 - Loss:  2.857, Seconds: 5.38\n",
      "Epoch   1/30 Batch 1070/1978 - Loss:  2.778, Seconds: 5.23\n",
      "Epoch   1/30 Batch 1080/1978 - Loss:  2.747, Seconds: 5.26\n",
      "Epoch   1/30 Batch 1090/1978 - Loss:  2.698, Seconds: 5.13\n",
      "Epoch   1/30 Batch 1100/1978 - Loss:  2.735, Seconds: 5.30\n",
      "Epoch   1/30 Batch 1110/1978 - Loss:  2.827, Seconds: 5.31\n",
      "Epoch   1/30 Batch 1120/1978 - Loss:  2.755, Seconds: 5.37\n",
      "Epoch   1/30 Batch 1130/1978 - Loss:  2.607, Seconds: 5.20\n",
      "Epoch   1/30 Batch 1140/1978 - Loss:  2.593, Seconds: 5.51\n",
      "Epoch   1/30 Batch 1150/1978 - Loss:  2.619, Seconds: 5.53\n",
      "Epoch   1/30 Batch 1160/1978 - Loss:  2.596, Seconds: 5.55\n",
      "Epoch   1/30 Batch 1170/1978 - Loss:  2.662, Seconds: 5.26\n",
      "Epoch   1/30 Batch 1180/1978 - Loss:  2.714, Seconds: 5.28\n",
      "Epoch   1/30 Batch 1190/1978 - Loss:  2.650, Seconds: 5.44\n",
      "Epoch   1/30 Batch 1200/1978 - Loss:  2.523, Seconds: 5.46\n",
      "Epoch   1/30 Batch 1210/1978 - Loss:  2.732, Seconds: 5.48\n",
      "Epoch   1/30 Batch 1220/1978 - Loss:  2.652, Seconds: 5.67\n",
      "Epoch   1/30 Batch 1230/1978 - Loss:  2.634, Seconds: 5.22\n",
      "Epoch   1/30 Batch 1240/1978 - Loss:  2.609, Seconds: 5.55\n",
      "Epoch   1/30 Batch 1250/1978 - Loss:  2.597, Seconds: 5.58\n",
      "Epoch   1/30 Batch 1260/1978 - Loss:  2.786, Seconds: 5.59\n",
      "Epoch   1/30 Batch 1270/1978 - Loss:  2.686, Seconds: 5.60\n",
      "Epoch   1/30 Batch 1280/1978 - Loss:  2.748, Seconds: 5.80\n",
      "Epoch   1/30 Batch 1290/1978 - Loss:  2.894, Seconds: 5.64\n",
      "Epoch   1/30 Batch 1300/1978 - Loss:  2.715, Seconds: 5.67\n",
      "Epoch   1/30 Batch 1310/1978 - Loss:  2.933, Seconds: 5.86\n",
      "Average loss for this update: 2.611\n",
      "New Record!\n",
      "Epoch   1/30 Batch 1320/1978 - Loss:  2.923, Seconds: 5.54\n",
      "Epoch   1/30 Batch 1330/1978 - Loss:  3.034, Seconds: 5.57\n",
      "Epoch   1/30 Batch 1340/1978 - Loss:  3.015, Seconds: 5.43\n",
      "Epoch   1/30 Batch 1350/1978 - Loss:  2.986, Seconds: 5.46\n",
      "Epoch   1/30 Batch 1360/1978 - Loss:  3.068, Seconds: 5.31\n",
      "Epoch   1/30 Batch 1370/1978 - Loss:  3.100, Seconds: 5.47\n",
      "Epoch   1/30 Batch 1380/1978 - Loss:  3.181, Seconds: 5.98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/30 Batch 1390/1978 - Loss:  3.035, Seconds: 5.83\n",
      "Epoch   1/30 Batch 1400/1978 - Loss:  3.104, Seconds: 5.54\n",
      "Epoch   1/30 Batch 1410/1978 - Loss:  3.089, Seconds: 5.74\n",
      "Epoch   1/30 Batch 1420/1978 - Loss:  3.123, Seconds: 5.90\n",
      "Epoch   1/30 Batch 1430/1978 - Loss:  3.244, Seconds: 5.61\n",
      "Epoch   1/30 Batch 1440/1978 - Loss:  3.070, Seconds: 5.79\n",
      "Epoch   1/30 Batch 1450/1978 - Loss:  3.172, Seconds: 5.64\n",
      "Epoch   1/30 Batch 1460/1978 - Loss:  3.187, Seconds: 5.96\n",
      "Epoch   1/30 Batch 1470/1978 - Loss:  3.128, Seconds: 5.66\n",
      "Epoch   1/30 Batch 1480/1978 - Loss:  3.093, Seconds: 5.68\n",
      "Epoch   1/30 Batch 1490/1978 - Loss:  3.253, Seconds: 6.05\n",
      "Epoch   1/30 Batch 1500/1978 - Loss:  3.086, Seconds: 5.74\n",
      "Epoch   1/30 Batch 1510/1978 - Loss:  3.132, Seconds: 5.91\n",
      "Epoch   1/30 Batch 1520/1978 - Loss:  3.123, Seconds: 6.00\n",
      "Epoch   1/30 Batch 1530/1978 - Loss:  3.023, Seconds: 6.30\n",
      "Epoch   1/30 Batch 1540/1978 - Loss:  3.112, Seconds: 6.12\n",
      "Epoch   1/30 Batch 1550/1978 - Loss:  3.227, Seconds: 6.19\n",
      "Epoch   1/30 Batch 1560/1978 - Loss:  3.255, Seconds: 6.32\n",
      "Epoch   1/30 Batch 1570/1978 - Loss:  3.082, Seconds: 5.83\n",
      "Epoch   1/30 Batch 1580/1978 - Loss:  3.080, Seconds: 6.05\n",
      "Epoch   1/30 Batch 1590/1978 - Loss:  3.138, Seconds: 6.23\n",
      "Epoch   1/30 Batch 1600/1978 - Loss:  3.219, Seconds: 5.93\n",
      "Epoch   1/30 Batch 1610/1978 - Loss:  3.105, Seconds: 6.08\n",
      "Epoch   1/30 Batch 1620/1978 - Loss:  3.172, Seconds: 5.96\n",
      "Epoch   1/30 Batch 1630/1978 - Loss:  3.070, Seconds: 6.46\n",
      "Epoch   1/30 Batch 1640/1978 - Loss:  3.149, Seconds: 6.34\n",
      "Epoch   1/30 Batch 1650/1978 - Loss:  3.049, Seconds: 6.17\n",
      "Epoch   1/30 Batch 1660/1978 - Loss:  3.149, Seconds: 6.51\n",
      "Epoch   1/30 Batch 1670/1978 - Loss:  3.196, Seconds: 6.26\n",
      "Epoch   1/30 Batch 1680/1978 - Loss:  3.231, Seconds: 6.25\n",
      "Epoch   1/30 Batch 1690/1978 - Loss:  3.397, Seconds: 6.11\n",
      "Epoch   1/30 Batch 1700/1978 - Loss:  3.214, Seconds: 6.30\n",
      "Epoch   1/30 Batch 1710/1978 - Loss:  3.217, Seconds: 6.47\n",
      "Epoch   1/30 Batch 1720/1978 - Loss:  3.200, Seconds: 6.69\n",
      "Epoch   1/30 Batch 1730/1978 - Loss:  3.116, Seconds: 6.71\n",
      "Epoch   1/30 Batch 1740/1978 - Loss:  3.247, Seconds: 6.56\n",
      "Epoch   1/30 Batch 1750/1978 - Loss:  3.114, Seconds: 6.56\n",
      "Epoch   1/30 Batch 1760/1978 - Loss:  3.223, Seconds: 6.10\n",
      "Epoch   1/30 Batch 1770/1978 - Loss:  3.078, Seconds: 6.78\n",
      "Epoch   1/30 Batch 1780/1978 - Loss:  3.028, Seconds: 6.82\n",
      "Epoch   1/30 Batch 1790/1978 - Loss:  3.124, Seconds: 6.83\n",
      "Epoch   1/30 Batch 1800/1978 - Loss:  3.199, Seconds: 6.66\n",
      "Epoch   1/30 Batch 1810/1978 - Loss:  3.221, Seconds: 6.68\n",
      "Epoch   1/30 Batch 1820/1978 - Loss:  3.122, Seconds: 6.37\n",
      "Epoch   1/30 Batch 1830/1978 - Loss:  3.184, Seconds: 6.76\n",
      "Epoch   1/30 Batch 1840/1978 - Loss:  3.060, Seconds: 6.57\n",
      "Epoch   1/30 Batch 1850/1978 - Loss:  3.278, Seconds: 6.44\n",
      "Epoch   1/30 Batch 1860/1978 - Loss:  3.059, Seconds: 6.49\n",
      "Epoch   1/30 Batch 1870/1978 - Loss:  3.276, Seconds: 6.63\n",
      "Epoch   1/30 Batch 1880/1978 - Loss:  3.325, Seconds: 6.68\n",
      "Epoch   1/30 Batch 1890/1978 - Loss:  3.277, Seconds: 6.72\n",
      "Epoch   1/30 Batch 1900/1978 - Loss:  3.199, Seconds: 6.55\n",
      "Epoch   1/30 Batch 1910/1978 - Loss:  3.063, Seconds: 7.10\n",
      "Epoch   1/30 Batch 1920/1978 - Loss:  3.142, Seconds: 6.96\n",
      "Epoch   1/30 Batch 1930/1978 - Loss:  3.162, Seconds: 6.96\n",
      "Epoch   1/30 Batch 1940/1978 - Loss:  3.101, Seconds: 6.81\n",
      "Epoch   1/30 Batch 1950/1978 - Loss:  3.204, Seconds: 7.01\n",
      "Epoch   1/30 Batch 1960/1978 - Loss:  3.184, Seconds: 7.05\n",
      "Epoch   1/30 Batch 1970/1978 - Loss:  3.135, Seconds: 7.08\n",
      "Average loss for this update: 3.148\n",
      "No Improvement.\n",
      "Epoch   2/30 Batch   10/1978 - Loss:  3.762, Seconds: 2.31\n",
      "Epoch   2/30 Batch   20/1978 - Loss:  2.797, Seconds: 2.11\n",
      "Epoch   2/30 Batch   30/1978 - Loss:  2.626, Seconds: 2.22\n",
      "Epoch   2/30 Batch   40/1978 - Loss:  2.307, Seconds: 2.39\n",
      "Epoch   2/30 Batch   50/1978 - Loss:  2.365, Seconds: 2.52\n",
      "Epoch   2/30 Batch   60/1978 - Loss:  2.361, Seconds: 2.44\n",
      "Epoch   2/30 Batch   70/1978 - Loss:  2.363, Seconds: 2.35\n",
      "Epoch   2/30 Batch   80/1978 - Loss:  2.139, Seconds: 2.83\n",
      "Epoch   2/30 Batch   90/1978 - Loss:  2.076, Seconds: 2.74\n",
      "Epoch   2/30 Batch  100/1978 - Loss:  1.937, Seconds: 2.51\n",
      "Epoch   2/30 Batch  110/1978 - Loss:  2.242, Seconds: 3.09\n",
      "Epoch   2/30 Batch  120/1978 - Loss:  2.244, Seconds: 2.60\n",
      "Epoch   2/30 Batch  130/1978 - Loss:  2.067, Seconds: 2.94\n",
      "Epoch   2/30 Batch  140/1978 - Loss:  2.306, Seconds: 2.80\n",
      "Epoch   2/30 Batch  150/1978 - Loss:  2.221, Seconds: 3.20\n",
      "Epoch   2/30 Batch  160/1978 - Loss:  2.214, Seconds: 3.27\n",
      "Epoch   2/30 Batch  170/1978 - Loss:  2.299, Seconds: 2.77\n",
      "Epoch   2/30 Batch  180/1978 - Loss:  2.311, Seconds: 2.69\n",
      "Epoch   2/30 Batch  190/1978 - Loss:  2.344, Seconds: 2.90\n",
      "Epoch   2/30 Batch  200/1978 - Loss:  2.324, Seconds: 3.13\n",
      "Epoch   2/30 Batch  210/1978 - Loss:  2.138, Seconds: 3.04\n",
      "Epoch   2/30 Batch  220/1978 - Loss:  1.944, Seconds: 3.08\n",
      "Epoch   2/30 Batch  230/1978 - Loss:  2.008, Seconds: 3.51\n",
      "Epoch   2/30 Batch  240/1978 - Loss:  1.848, Seconds: 3.02\n",
      "Epoch   2/30 Batch  250/1978 - Loss:  1.848, Seconds: 3.55\n",
      "Epoch   2/30 Batch  260/1978 - Loss:  1.796, Seconds: 3.59\n",
      "Epoch   2/30 Batch  270/1978 - Loss:  1.902, Seconds: 3.05\n",
      "Epoch   2/30 Batch  280/1978 - Loss:  1.952, Seconds: 3.25\n",
      "Epoch   2/30 Batch  290/1978 - Loss:  1.942, Seconds: 3.57\n",
      "Epoch   2/30 Batch  300/1978 - Loss:  1.868, Seconds: 3.82\n",
      "Epoch   2/30 Batch  310/1978 - Loss:  1.763, Seconds: 3.81\n",
      "Epoch   2/30 Batch  320/1978 - Loss:  1.821, Seconds: 3.83\n",
      "Epoch   2/30 Batch  330/1978 - Loss:  1.810, Seconds: 3.72\n",
      "Epoch   2/30 Batch  340/1978 - Loss:  1.786, Seconds: 3.89\n",
      "Epoch   2/30 Batch  350/1978 - Loss:  1.885, Seconds: 3.92\n",
      "Epoch   2/30 Batch  360/1978 - Loss:  1.759, Seconds: 3.41\n",
      "Epoch   2/30 Batch  370/1978 - Loss:  1.879, Seconds: 3.45\n",
      "Epoch   2/30 Batch  380/1978 - Loss:  1.597, Seconds: 4.01\n",
      "Epoch   2/30 Batch  390/1978 - Loss:  1.713, Seconds: 3.70\n",
      "Epoch   2/30 Batch  400/1978 - Loss:  1.650, Seconds: 4.03\n",
      "Epoch   2/30 Batch  410/1978 - Loss:  1.926, Seconds: 3.97\n",
      "Epoch   2/30 Batch  420/1978 - Loss:  1.937, Seconds: 4.15\n",
      "Epoch   2/30 Batch  430/1978 - Loss:  1.953, Seconds: 3.99\n",
      "Epoch   2/30 Batch  440/1978 - Loss:  1.874, Seconds: 3.75\n",
      "Epoch   2/30 Batch  450/1978 - Loss:  1.859, Seconds: 4.21\n",
      "Epoch   2/30 Batch  460/1978 - Loss:  1.737, Seconds: 3.92\n",
      "Epoch   2/30 Batch  470/1978 - Loss:  1.732, Seconds: 4.23\n",
      "Epoch   2/30 Batch  480/1978 - Loss:  1.864, Seconds: 3.71\n",
      "Epoch   2/30 Batch  490/1978 - Loss:  1.766, Seconds: 4.14\n",
      "Epoch   2/30 Batch  500/1978 - Loss:  1.932, Seconds: 4.16\n",
      "Epoch   2/30 Batch  510/1978 - Loss:  1.977, Seconds: 4.36\n",
      "Epoch   2/30 Batch  520/1978 - Loss:  1.849, Seconds: 3.92\n",
      "Epoch   2/30 Batch  530/1978 - Loss:  1.829, Seconds: 4.36\n",
      "Epoch   2/30 Batch  540/1978 - Loss:  1.789, Seconds: 3.72\n",
      "Epoch   2/30 Batch  550/1978 - Loss:  1.816, Seconds: 4.41\n",
      "Epoch   2/30 Batch  560/1978 - Loss:  1.865, Seconds: 4.35\n",
      "Epoch   2/30 Batch  570/1978 - Loss:  1.855, Seconds: 4.34\n",
      "Epoch   2/30 Batch  580/1978 - Loss:  1.808, Seconds: 4.35\n",
      "Epoch   2/30 Batch  590/1978 - Loss:  1.735, Seconds: 4.35\n",
      "Epoch   2/30 Batch  600/1978 - Loss:  1.777, Seconds: 4.40\n",
      "Epoch   2/30 Batch  610/1978 - Loss:  1.646, Seconds: 4.56\n",
      "Epoch   2/30 Batch  620/1978 - Loss:  1.738, Seconds: 4.70\n",
      "Epoch   2/30 Batch  630/1978 - Loss:  1.908, Seconds: 4.56\n",
      "Epoch   2/30 Batch  640/1978 - Loss:  1.805, Seconds: 4.47\n",
      "Epoch   2/30 Batch  650/1978 - Loss:  1.711, Seconds: 4.63\n",
      "Average loss for this update: 1.991\n",
      "New Record!\n",
      "Epoch   2/30 Batch  660/1978 - Loss:  1.391, Seconds: 4.65\n",
      "Epoch   2/30 Batch  670/1978 - Loss:  1.365, Seconds: 4.66\n",
      "Epoch   2/30 Batch  680/1978 - Loss:  1.375, Seconds: 4.52\n",
      "Epoch   2/30 Batch  690/1978 - Loss:  1.382, Seconds: 4.69\n",
      "Epoch   2/30 Batch  700/1978 - Loss:  1.565, Seconds: 4.45\n",
      "Epoch   2/30 Batch  710/1978 - Loss:  1.403, Seconds: 4.71\n",
      "Epoch   2/30 Batch  720/1978 - Loss:  1.530, Seconds: 4.44\n",
      "Epoch   2/30 Batch  730/1978 - Loss:  1.578, Seconds: 4.63\n",
      "Epoch   2/30 Batch  740/1978 - Loss:  1.740, Seconds: 4.79\n",
      "Epoch   2/30 Batch  750/1978 - Loss:  1.781, Seconds: 4.79\n",
      "Epoch   2/30 Batch  760/1978 - Loss:  1.957, Seconds: 4.67\n",
      "Epoch   2/30 Batch  770/1978 - Loss:  1.850, Seconds: 4.85\n",
      "Epoch   2/30 Batch  780/1978 - Loss:  1.777, Seconds: 4.86\n",
      "Epoch   2/30 Batch  790/1978 - Loss:  1.872, Seconds: 4.89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   2/30 Batch  800/1978 - Loss:  1.889, Seconds: 4.89\n",
      "Epoch   2/30 Batch  810/1978 - Loss:  1.940, Seconds: 4.78\n",
      "Epoch   2/30 Batch  820/1978 - Loss:  1.834, Seconds: 4.90\n",
      "Epoch   2/30 Batch  830/1978 - Loss:  1.897, Seconds: 4.94\n",
      "Epoch   2/30 Batch  840/1978 - Loss:  1.953, Seconds: 4.99\n",
      "Epoch   2/30 Batch  850/1978 - Loss:  1.946, Seconds: 4.97\n",
      "Epoch   2/30 Batch  860/1978 - Loss:  1.930, Seconds: 5.02\n",
      "Epoch   2/30 Batch  870/1978 - Loss:  1.923, Seconds: 5.03\n",
      "Epoch   2/30 Batch  880/1978 - Loss:  1.961, Seconds: 5.03\n",
      "Epoch   2/30 Batch  890/1978 - Loss:  1.965, Seconds: 5.06\n",
      "Epoch   2/30 Batch  900/1978 - Loss:  2.020, Seconds: 4.92\n",
      "Epoch   2/30 Batch  910/1978 - Loss:  1.990, Seconds: 4.80\n",
      "Epoch   2/30 Batch  920/1978 - Loss:  2.037, Seconds: 4.98\n",
      "Epoch   2/30 Batch  930/1978 - Loss:  1.987, Seconds: 4.98\n",
      "Epoch   2/30 Batch  940/1978 - Loss:  2.222, Seconds: 4.86\n",
      "Epoch   2/30 Batch  950/1978 - Loss:  2.170, Seconds: 4.87\n",
      "Epoch   2/30 Batch  960/1978 - Loss:  2.182, Seconds: 5.05\n",
      "Epoch   2/30 Batch  970/1978 - Loss:  2.152, Seconds: 4.63\n",
      "Epoch   2/30 Batch  980/1978 - Loss:  2.083, Seconds: 5.09\n",
      "Epoch   2/30 Batch  990/1978 - Loss:  2.034, Seconds: 5.25\n",
      "Epoch   2/30 Batch 1000/1978 - Loss:  2.203, Seconds: 5.11\n",
      "Epoch   2/30 Batch 1010/1978 - Loss:  2.075, Seconds: 5.28\n",
      "Epoch   2/30 Batch 1020/1978 - Loss:  2.211, Seconds: 5.13\n",
      "Epoch   2/30 Batch 1030/1978 - Loss:  2.197, Seconds: 5.32\n",
      "Epoch   2/30 Batch 1040/1978 - Loss:  2.274, Seconds: 5.32\n",
      "Epoch   2/30 Batch 1050/1978 - Loss:  2.282, Seconds: 5.23\n",
      "Epoch   2/30 Batch 1060/1978 - Loss:  2.227, Seconds: 5.38\n",
      "Epoch   2/30 Batch 1070/1978 - Loss:  2.148, Seconds: 5.29\n",
      "Epoch   2/30 Batch 1080/1978 - Loss:  2.152, Seconds: 5.27\n",
      "Epoch   2/30 Batch 1090/1978 - Loss:  2.130, Seconds: 5.13\n",
      "Epoch   2/30 Batch 1100/1978 - Loss:  2.149, Seconds: 5.33\n",
      "Epoch   2/30 Batch 1110/1978 - Loss:  2.211, Seconds: 5.30\n",
      "Epoch   2/30 Batch 1120/1978 - Loss:  2.199, Seconds: 5.34\n",
      "Epoch   2/30 Batch 1130/1978 - Loss:  2.055, Seconds: 5.20\n",
      "Epoch   2/30 Batch 1140/1978 - Loss:  2.041, Seconds: 5.51\n",
      "Epoch   2/30 Batch 1150/1978 - Loss:  2.094, Seconds: 5.53\n",
      "Epoch   2/30 Batch 1160/1978 - Loss:  2.067, Seconds: 5.57\n",
      "Epoch   2/30 Batch 1170/1978 - Loss:  2.149, Seconds: 5.28\n",
      "Epoch   2/30 Batch 1180/1978 - Loss:  2.170, Seconds: 5.30\n",
      "Epoch   2/30 Batch 1190/1978 - Loss:  2.132, Seconds: 5.46\n",
      "Epoch   2/30 Batch 1200/1978 - Loss:  2.042, Seconds: 5.49\n",
      "Epoch   2/30 Batch 1210/1978 - Loss:  2.188, Seconds: 5.51\n",
      "Epoch   2/30 Batch 1220/1978 - Loss:  2.141, Seconds: 5.68\n",
      "Epoch   2/30 Batch 1230/1978 - Loss:  2.122, Seconds: 5.19\n",
      "Epoch   2/30 Batch 1240/1978 - Loss:  2.104, Seconds: 5.58\n",
      "Epoch   2/30 Batch 1250/1978 - Loss:  2.115, Seconds: 5.55\n",
      "Epoch   2/30 Batch 1260/1978 - Loss:  2.239, Seconds: 5.58\n",
      "Epoch   2/30 Batch 1270/1978 - Loss:  2.205, Seconds: 5.61\n",
      "Epoch   2/30 Batch 1280/1978 - Loss:  2.224, Seconds: 5.79\n",
      "Epoch   2/30 Batch 1290/1978 - Loss:  2.357, Seconds: 5.65\n",
      "Epoch   2/30 Batch 1300/1978 - Loss:  2.220, Seconds: 5.67\n",
      "Epoch   2/30 Batch 1310/1978 - Loss:  2.374, Seconds: 5.83\n",
      "Average loss for this update: 2.013\n",
      "No Improvement.\n",
      "Epoch   2/30 Batch 1320/1978 - Loss:  2.376, Seconds: 5.55\n",
      "Epoch   2/30 Batch 1330/1978 - Loss:  2.489, Seconds: 5.57\n",
      "Epoch   2/30 Batch 1340/1978 - Loss:  2.449, Seconds: 5.43\n",
      "Epoch   2/30 Batch 1350/1978 - Loss:  2.458, Seconds: 5.44\n",
      "Epoch   2/30 Batch 1360/1978 - Loss:  2.516, Seconds: 5.28\n",
      "Epoch   2/30 Batch 1370/1978 - Loss:  2.531, Seconds: 5.48\n",
      "Epoch   2/30 Batch 1380/1978 - Loss:  2.595, Seconds: 5.98\n",
      "Epoch   2/30 Batch 1390/1978 - Loss:  2.489, Seconds: 5.85\n",
      "Epoch   2/30 Batch 1400/1978 - Loss:  2.554, Seconds: 5.54\n",
      "Epoch   2/30 Batch 1410/1978 - Loss:  2.547, Seconds: 5.75\n",
      "Epoch   2/30 Batch 1420/1978 - Loss:  2.549, Seconds: 5.89\n",
      "Epoch   2/30 Batch 1430/1978 - Loss:  2.708, Seconds: 5.62\n",
      "Epoch   2/30 Batch 1440/1978 - Loss:  2.559, Seconds: 5.78\n",
      "Epoch   2/30 Batch 1450/1978 - Loss:  2.607, Seconds: 5.62\n",
      "Epoch   2/30 Batch 1460/1978 - Loss:  2.648, Seconds: 5.97\n",
      "Epoch   2/30 Batch 1470/1978 - Loss:  2.611, Seconds: 5.66\n",
      "Epoch   2/30 Batch 1480/1978 - Loss:  2.572, Seconds: 5.71\n",
      "Epoch   2/30 Batch 1490/1978 - Loss:  2.724, Seconds: 6.02\n",
      "Epoch   2/30 Batch 1500/1978 - Loss:  2.598, Seconds: 5.75\n",
      "Epoch   2/30 Batch 1510/1978 - Loss:  2.636, Seconds: 5.92\n",
      "Epoch   2/30 Batch 1520/1978 - Loss:  2.624, Seconds: 5.96\n",
      "Epoch   2/30 Batch 1530/1978 - Loss:  2.551, Seconds: 6.29\n",
      "Epoch   2/30 Batch 1540/1978 - Loss:  2.599, Seconds: 6.15\n",
      "Epoch   2/30 Batch 1550/1978 - Loss:  2.724, Seconds: 6.16\n",
      "Epoch   2/30 Batch 1560/1978 - Loss:  2.725, Seconds: 6.35\n",
      "Epoch   2/30 Batch 1570/1978 - Loss:  2.613, Seconds: 5.88\n",
      "Epoch   2/30 Batch 1580/1978 - Loss:  2.618, Seconds: 6.07\n",
      "Epoch   2/30 Batch 1590/1978 - Loss:  2.644, Seconds: 6.24\n",
      "Epoch   2/30 Batch 1600/1978 - Loss:  2.709, Seconds: 5.92\n",
      "Epoch   2/30 Batch 1610/1978 - Loss:  2.631, Seconds: 6.10\n",
      "Epoch   2/30 Batch 1620/1978 - Loss:  2.700, Seconds: 5.96\n",
      "Epoch   2/30 Batch 1630/1978 - Loss:  2.618, Seconds: 6.49\n",
      "Epoch   2/30 Batch 1640/1978 - Loss:  2.661, Seconds: 6.39\n",
      "Epoch   2/30 Batch 1650/1978 - Loss:  2.616, Seconds: 6.21\n",
      "Epoch   2/30 Batch 1660/1978 - Loss:  2.674, Seconds: 6.57\n",
      "Epoch   2/30 Batch 1670/1978 - Loss:  2.694, Seconds: 6.25\n",
      "Epoch   2/30 Batch 1680/1978 - Loss:  2.780, Seconds: 6.27\n",
      "Epoch   2/30 Batch 1690/1978 - Loss:  2.873, Seconds: 6.13\n",
      "Epoch   2/30 Batch 1700/1978 - Loss:  2.754, Seconds: 6.30\n",
      "Epoch   2/30 Batch 1710/1978 - Loss:  2.755, Seconds: 6.52\n",
      "Epoch   2/30 Batch 1720/1978 - Loss:  2.759, Seconds: 6.69\n",
      "Epoch   2/30 Batch 1730/1978 - Loss:  2.681, Seconds: 6.69\n",
      "Epoch   2/30 Batch 1740/1978 - Loss:  2.783, Seconds: 6.54\n",
      "Epoch   2/30 Batch 1750/1978 - Loss:  2.710, Seconds: 6.60\n",
      "Epoch   2/30 Batch 1760/1978 - Loss:  2.787, Seconds: 6.12\n",
      "Epoch   2/30 Batch 1770/1978 - Loss:  2.661, Seconds: 6.78\n",
      "Epoch   2/30 Batch 1780/1978 - Loss:  2.607, Seconds: 6.83\n",
      "Epoch   2/30 Batch 1790/1978 - Loss:  2.692, Seconds: 6.81\n",
      "Epoch   2/30 Batch 1800/1978 - Loss:  2.781, Seconds: 6.70\n",
      "Epoch   2/30 Batch 1810/1978 - Loss:  2.798, Seconds: 6.68\n",
      "Epoch   2/30 Batch 1820/1978 - Loss:  2.685, Seconds: 6.39\n",
      "Epoch   2/30 Batch 1830/1978 - Loss:  2.747, Seconds: 6.74\n",
      "Epoch   2/30 Batch 1840/1978 - Loss:  2.669, Seconds: 6.61\n",
      "Epoch   2/30 Batch 1850/1978 - Loss:  2.852, Seconds: 6.43\n",
      "Epoch   2/30 Batch 1860/1978 - Loss:  2.656, Seconds: 6.46\n",
      "Epoch   2/30 Batch 1870/1978 - Loss:  2.850, Seconds: 6.66\n",
      "Epoch   2/30 Batch 1880/1978 - Loss:  2.918, Seconds: 6.68\n",
      "Epoch   2/30 Batch 1890/1978 - Loss:  2.865, Seconds: 6.71\n",
      "Epoch   2/30 Batch 1900/1978 - Loss:  2.801, Seconds: 6.60\n",
      "Epoch   2/30 Batch 1910/1978 - Loss:  2.673, Seconds: 7.10\n",
      "Epoch   2/30 Batch 1920/1978 - Loss:  2.760, Seconds: 6.96\n",
      "Epoch   2/30 Batch 1930/1978 - Loss:  2.795, Seconds: 6.98\n",
      "Epoch   2/30 Batch 1940/1978 - Loss:  2.741, Seconds: 6.85\n",
      "Epoch   2/30 Batch 1950/1978 - Loss:  2.816, Seconds: 7.04\n",
      "Epoch   2/30 Batch 1960/1978 - Loss:  2.808, Seconds: 7.04\n",
      "Epoch   2/30 Batch 1970/1978 - Loss:  2.782, Seconds: 7.08\n",
      "Average loss for this update: 2.678\n",
      "No Improvement.\n",
      "Epoch   3/30 Batch   10/1978 - Loss:  2.981, Seconds: 2.29\n",
      "Epoch   3/30 Batch   20/1978 - Loss:  2.310, Seconds: 2.08\n",
      "Epoch   3/30 Batch   30/1978 - Loss:  2.265, Seconds: 2.23\n",
      "Epoch   3/30 Batch   40/1978 - Loss:  1.963, Seconds: 2.41\n",
      "Epoch   3/30 Batch   50/1978 - Loss:  2.015, Seconds: 2.52\n",
      "Epoch   3/30 Batch   60/1978 - Loss:  1.972, Seconds: 2.17\n",
      "Epoch   3/30 Batch   70/1978 - Loss:  2.029, Seconds: 2.23\n",
      "Epoch   3/30 Batch   80/1978 - Loss:  1.802, Seconds: 2.91\n",
      "Epoch   3/30 Batch   90/1978 - Loss:  1.808, Seconds: 2.66\n",
      "Epoch   3/30 Batch  100/1978 - Loss:  1.683, Seconds: 2.46\n",
      "Epoch   3/30 Batch  110/1978 - Loss:  1.939, Seconds: 2.84\n",
      "Epoch   3/30 Batch  120/1978 - Loss:  1.939, Seconds: 2.76\n",
      "Epoch   3/30 Batch  130/1978 - Loss:  1.777, Seconds: 2.80\n",
      "Epoch   3/30 Batch  140/1978 - Loss:  2.005, Seconds: 3.21\n",
      "Epoch   3/30 Batch  150/1978 - Loss:  1.934, Seconds: 3.08\n",
      "Epoch   3/30 Batch  160/1978 - Loss:  1.942, Seconds: 3.24\n",
      "Epoch   3/30 Batch  170/1978 - Loss:  2.040, Seconds: 3.01\n",
      "Epoch   3/30 Batch  180/1978 - Loss:  2.030, Seconds: 2.69\n",
      "Epoch   3/30 Batch  190/1978 - Loss:  2.085, Seconds: 2.86\n",
      "Epoch   3/30 Batch  200/1978 - Loss:  2.040, Seconds: 2.88\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   3/30 Batch  210/1978 - Loss:  1.907, Seconds: 3.05\n",
      "Epoch   3/30 Batch  220/1978 - Loss:  1.739, Seconds: 3.14\n",
      "Epoch   3/30 Batch  230/1978 - Loss:  1.786, Seconds: 3.51\n",
      "Epoch   3/30 Batch  240/1978 - Loss:  1.646, Seconds: 3.17\n",
      "Epoch   3/30 Batch  250/1978 - Loss:  1.642, Seconds: 3.56\n",
      "Epoch   3/30 Batch  260/1978 - Loss:  1.616, Seconds: 3.59\n",
      "Epoch   3/30 Batch  270/1978 - Loss:  1.697, Seconds: 3.21\n",
      "Epoch   3/30 Batch  280/1978 - Loss:  1.726, Seconds: 3.26\n",
      "Epoch   3/30 Batch  290/1978 - Loss:  1.741, Seconds: 3.43\n",
      "Epoch   3/30 Batch  300/1978 - Loss:  1.654, Seconds: 3.60\n",
      "Epoch   3/30 Batch  310/1978 - Loss:  1.589, Seconds: 3.77\n",
      "Epoch   3/30 Batch  320/1978 - Loss:  1.637, Seconds: 3.88\n",
      "Epoch   3/30 Batch  330/1978 - Loss:  1.618, Seconds: 3.71\n",
      "Epoch   3/30 Batch  340/1978 - Loss:  1.583, Seconds: 4.43\n",
      "Epoch   3/30 Batch  350/1978 - Loss:  1.659, Seconds: 4.06\n",
      "Epoch   3/30 Batch  360/1978 - Loss:  1.559, Seconds: 3.41\n",
      "Epoch   3/30 Batch  370/1978 - Loss:  1.643, Seconds: 3.43\n",
      "Epoch   3/30 Batch  380/1978 - Loss:  1.418, Seconds: 4.03\n",
      "Epoch   3/30 Batch  390/1978 - Loss:  1.533, Seconds: 3.49\n",
      "Epoch   3/30 Batch  400/1978 - Loss:  1.498, Seconds: 4.04\n",
      "Epoch   3/30 Batch  410/1978 - Loss:  1.744, Seconds: 3.96\n",
      "Epoch   3/30 Batch  420/1978 - Loss:  1.751, Seconds: 4.11\n",
      "Epoch   3/30 Batch  430/1978 - Loss:  1.751, Seconds: 4.02\n",
      "Epoch   3/30 Batch  440/1978 - Loss:  1.675, Seconds: 3.73\n",
      "Epoch   3/30 Batch  450/1978 - Loss:  1.679, Seconds: 4.19\n",
      "Epoch   3/30 Batch  460/1978 - Loss:  1.562, Seconds: 3.94\n",
      "Epoch   3/30 Batch  470/1978 - Loss:  1.570, Seconds: 4.25\n",
      "Epoch   3/30 Batch  480/1978 - Loss:  1.680, Seconds: 3.70\n",
      "Epoch   3/30 Batch  490/1978 - Loss:  1.585, Seconds: 4.18\n",
      "Epoch   3/30 Batch  500/1978 - Loss:  1.753, Seconds: 4.18\n",
      "Epoch   3/30 Batch  510/1978 - Loss:  1.762, Seconds: 4.33\n",
      "Epoch   3/30 Batch  520/1978 - Loss:  1.657, Seconds: 3.95\n",
      "Epoch   3/30 Batch  530/1978 - Loss:  1.623, Seconds: 4.42\n",
      "Epoch   3/30 Batch  540/1978 - Loss:  1.595, Seconds: 3.69\n",
      "Epoch   3/30 Batch  550/1978 - Loss:  1.600, Seconds: 4.43\n",
      "Epoch   3/30 Batch  560/1978 - Loss:  1.643, Seconds: 4.17\n",
      "Epoch   3/30 Batch  570/1978 - Loss:  1.644, Seconds: 4.34\n",
      "Epoch   3/30 Batch  580/1978 - Loss:  1.608, Seconds: 4.34\n",
      "Epoch   3/30 Batch  590/1978 - Loss:  1.552, Seconds: 4.36\n",
      "Epoch   3/30 Batch  600/1978 - Loss:  1.604, Seconds: 4.40\n",
      "Epoch   3/30 Batch  610/1978 - Loss:  1.474, Seconds: 4.55\n",
      "Epoch   3/30 Batch  620/1978 - Loss:  1.556, Seconds: 4.56\n",
      "Epoch   3/30 Batch  630/1978 - Loss:  1.721, Seconds: 4.60\n",
      "Epoch   3/30 Batch  640/1978 - Loss:  1.633, Seconds: 4.47\n",
      "Epoch   3/30 Batch  650/1978 - Loss:  1.530, Seconds: 4.62\n",
      "Average loss for this update: 1.754\n",
      "New Record!\n",
      "Epoch   3/30 Batch  660/1978 - Loss:  1.235, Seconds: 4.64\n",
      "Epoch   3/30 Batch  670/1978 - Loss:  1.224, Seconds: 4.68\n",
      "Epoch   3/30 Batch  680/1978 - Loss:  1.239, Seconds: 4.52\n",
      "Epoch   3/30 Batch  690/1978 - Loss:  1.242, Seconds: 4.69\n",
      "Epoch   3/30 Batch  700/1978 - Loss:  1.392, Seconds: 4.43\n",
      "Epoch   3/30 Batch  710/1978 - Loss:  1.263, Seconds: 4.71\n",
      "Epoch   3/30 Batch  720/1978 - Loss:  1.382, Seconds: 4.48\n",
      "Epoch   3/30 Batch  730/1978 - Loss:  1.435, Seconds: 4.58\n",
      "Epoch   3/30 Batch  740/1978 - Loss:  1.549, Seconds: 4.80\n",
      "Epoch   3/30 Batch  750/1978 - Loss:  1.623, Seconds: 4.77\n",
      "Epoch   3/30 Batch  760/1978 - Loss:  1.762, Seconds: 4.67\n",
      "Epoch   3/30 Batch  770/1978 - Loss:  1.689, Seconds: 4.82\n",
      "Epoch   3/30 Batch  780/1978 - Loss:  1.590, Seconds: 4.88\n",
      "Epoch   3/30 Batch  790/1978 - Loss:  1.685, Seconds: 4.86\n",
      "Epoch   3/30 Batch  800/1978 - Loss:  1.682, Seconds: 4.90\n",
      "Epoch   3/30 Batch  810/1978 - Loss:  1.771, Seconds: 4.77\n",
      "Epoch   3/30 Batch  820/1978 - Loss:  1.664, Seconds: 4.94\n",
      "Epoch   3/30 Batch  830/1978 - Loss:  1.711, Seconds: 4.94\n",
      "Epoch   3/30 Batch  840/1978 - Loss:  1.749, Seconds: 4.96\n",
      "Epoch   3/30 Batch  850/1978 - Loss:  1.777, Seconds: 4.99\n",
      "Epoch   3/30 Batch  860/1978 - Loss:  1.751, Seconds: 5.00\n",
      "Epoch   3/30 Batch  870/1978 - Loss:  1.762, Seconds: 5.04\n",
      "Epoch   3/30 Batch  880/1978 - Loss:  1.806, Seconds: 5.04\n",
      "Epoch   3/30 Batch  890/1978 - Loss:  1.803, Seconds: 5.06\n",
      "Epoch   3/30 Batch  900/1978 - Loss:  1.828, Seconds: 4.94\n",
      "Epoch   3/30 Batch  910/1978 - Loss:  1.828, Seconds: 4.81\n",
      "Epoch   3/30 Batch  920/1978 - Loss:  1.851, Seconds: 4.96\n",
      "Epoch   3/30 Batch  930/1978 - Loss:  1.811, Seconds: 5.00\n",
      "Epoch   3/30 Batch  940/1978 - Loss:  2.007, Seconds: 4.85\n",
      "Epoch   3/30 Batch  950/1978 - Loss:  1.977, Seconds: 4.89\n",
      "Epoch   3/30 Batch  960/1978 - Loss:  1.969, Seconds: 5.06\n",
      "Epoch   3/30 Batch  970/1978 - Loss:  1.937, Seconds: 4.65\n",
      "Epoch   3/30 Batch  980/1978 - Loss:  1.912, Seconds: 5.09\n",
      "Epoch   3/30 Batch  990/1978 - Loss:  1.848, Seconds: 5.25\n",
      "Epoch   3/30 Batch 1000/1978 - Loss:  2.007, Seconds: 5.09\n",
      "Epoch   3/30 Batch 1010/1978 - Loss:  1.878, Seconds: 5.29\n",
      "Epoch   3/30 Batch 1020/1978 - Loss:  2.010, Seconds: 5.19\n",
      "Epoch   3/30 Batch 1030/1978 - Loss:  1.988, Seconds: 5.35\n",
      "Epoch   3/30 Batch 1040/1978 - Loss:  2.037, Seconds: 5.34\n",
      "Epoch   3/30 Batch 1050/1978 - Loss:  2.090, Seconds: 5.19\n",
      "Epoch   3/30 Batch 1060/1978 - Loss:  2.027, Seconds: 5.40\n",
      "Epoch   3/30 Batch 1070/1978 - Loss:  1.933, Seconds: 5.26\n",
      "Epoch   3/30 Batch 1080/1978 - Loss:  1.958, Seconds: 5.27\n",
      "Epoch   3/30 Batch 1090/1978 - Loss:  1.939, Seconds: 5.12\n",
      "Epoch   3/30 Batch 1100/1978 - Loss:  1.956, Seconds: 5.30\n",
      "Epoch   3/30 Batch 1110/1978 - Loss:  2.014, Seconds: 5.32\n",
      "Epoch   3/30 Batch 1120/1978 - Loss:  2.001, Seconds: 5.36\n",
      "Epoch   3/30 Batch 1130/1978 - Loss:  1.889, Seconds: 5.17\n",
      "Epoch   3/30 Batch 1140/1978 - Loss:  1.871, Seconds: 5.55\n",
      "Epoch   3/30 Batch 1150/1978 - Loss:  1.932, Seconds: 5.53\n",
      "Epoch   3/30 Batch 1160/1978 - Loss:  1.851, Seconds: 5.57\n",
      "Epoch   3/30 Batch 1170/1978 - Loss:  1.953, Seconds: 5.28\n",
      "Epoch   3/30 Batch 1180/1978 - Loss:  1.962, Seconds: 5.28\n",
      "Epoch   3/30 Batch 1190/1978 - Loss:  1.962, Seconds: 5.47\n",
      "Epoch   3/30 Batch 1200/1978 - Loss:  1.855, Seconds: 5.48\n",
      "Epoch   3/30 Batch 1210/1978 - Loss:  2.002, Seconds: 5.48\n",
      "Epoch   3/30 Batch 1220/1978 - Loss:  1.976, Seconds: 5.69\n",
      "Epoch   3/30 Batch 1230/1978 - Loss:  1.938, Seconds: 5.22\n",
      "Epoch   3/30 Batch 1240/1978 - Loss:  1.930, Seconds: 5.56\n",
      "Epoch   3/30 Batch 1250/1978 - Loss:  1.934, Seconds: 5.61\n",
      "Epoch   3/30 Batch 1260/1978 - Loss:  2.039, Seconds: 5.59\n",
      "Epoch   3/30 Batch 1270/1978 - Loss:  2.019, Seconds: 5.64\n",
      "Epoch   3/30 Batch 1280/1978 - Loss:  2.055, Seconds: 5.78\n",
      "Epoch   3/30 Batch 1290/1978 - Loss:  2.163, Seconds: 5.68\n",
      "Epoch   3/30 Batch 1300/1978 - Loss:  2.041, Seconds: 5.68\n",
      "Epoch   3/30 Batch 1310/1978 - Loss:  2.152, Seconds: 5.83\n",
      "Average loss for this update: 1.83\n",
      "No Improvement.\n",
      "Epoch   3/30 Batch 1320/1978 - Loss:  2.165, Seconds: 5.54\n",
      "Epoch   3/30 Batch 1330/1978 - Loss:  2.294, Seconds: 5.58\n",
      "Epoch   3/30 Batch 1340/1978 - Loss:  2.289, Seconds: 5.42\n",
      "Epoch   3/30 Batch 1350/1978 - Loss:  2.274, Seconds: 5.44\n",
      "Epoch   3/30 Batch 1360/1978 - Loss:  2.349, Seconds: 5.31\n",
      "Epoch   3/30 Batch 1370/1978 - Loss:  2.367, Seconds: 5.52\n",
      "Epoch   3/30 Batch 1380/1978 - Loss:  2.427, Seconds: 5.97\n",
      "Epoch   3/30 Batch 1390/1978 - Loss:  2.329, Seconds: 5.84\n",
      "Epoch   3/30 Batch 1400/1978 - Loss:  2.393, Seconds: 5.56\n",
      "Epoch   3/30 Batch 1410/1978 - Loss:  2.367, Seconds: 5.73\n",
      "Epoch   3/30 Batch 1420/1978 - Loss:  2.404, Seconds: 5.91\n",
      "Epoch   3/30 Batch 1430/1978 - Loss:  2.523, Seconds: 5.58\n",
      "Epoch   3/30 Batch 1440/1978 - Loss:  2.386, Seconds: 5.78\n",
      "Epoch   3/30 Batch 1450/1978 - Loss:  2.430, Seconds: 5.68\n",
      "Epoch   3/30 Batch 1460/1978 - Loss:  2.477, Seconds: 5.99\n",
      "Epoch   3/30 Batch 1470/1978 - Loss:  2.421, Seconds: 5.70\n",
      "Epoch   3/30 Batch 1480/1978 - Loss:  2.418, Seconds: 5.71\n",
      "Epoch   3/30 Batch 1490/1978 - Loss:  2.509, Seconds: 6.10\n",
      "Epoch   3/30 Batch 1500/1978 - Loss:  2.412, Seconds: 5.78\n",
      "Epoch   3/30 Batch 1510/1978 - Loss:  2.430, Seconds: 5.95\n",
      "Epoch   3/30 Batch 1520/1978 - Loss:  2.439, Seconds: 5.95\n",
      "Epoch   3/30 Batch 1530/1978 - Loss:  2.381, Seconds: 6.29\n",
      "Epoch   3/30 Batch 1540/1978 - Loss:  2.432, Seconds: 6.15\n",
      "Epoch   3/30 Batch 1550/1978 - Loss:  2.553, Seconds: 6.14\n",
      "Epoch   3/30 Batch 1560/1978 - Loss:  2.535, Seconds: 6.34\n",
      "Epoch   3/30 Batch 1570/1978 - Loss:  2.434, Seconds: 5.89\n",
      "Epoch   3/30 Batch 1580/1978 - Loss:  2.448, Seconds: 6.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   3/30 Batch 1590/1978 - Loss:  2.466, Seconds: 6.24\n",
      "Epoch   3/30 Batch 1600/1978 - Loss:  2.535, Seconds: 5.95\n",
      "Epoch   3/30 Batch 1610/1978 - Loss:  2.439, Seconds: 6.13\n",
      "Epoch   3/30 Batch 1620/1978 - Loss:  2.512, Seconds: 5.95\n",
      "Epoch   3/30 Batch 1630/1978 - Loss:  2.455, Seconds: 6.51\n",
      "Epoch   3/30 Batch 1640/1978 - Loss:  2.493, Seconds: 6.38\n",
      "Epoch   3/30 Batch 1650/1978 - Loss:  2.439, Seconds: 6.18\n",
      "Epoch   3/30 Batch 1660/1978 - Loss:  2.485, Seconds: 6.55\n",
      "Epoch   3/30 Batch 1670/1978 - Loss:  2.504, Seconds: 6.24\n",
      "Epoch   3/30 Batch 1680/1978 - Loss:  2.583, Seconds: 6.09\n",
      "Epoch   3/30 Batch 1690/1978 - Loss:  2.680, Seconds: 6.00\n",
      "Epoch   3/30 Batch 1700/1978 - Loss:  2.568, Seconds: 6.26\n",
      "Epoch   3/30 Batch 1710/1978 - Loss:  2.562, Seconds: 6.48\n",
      "Epoch   3/30 Batch 1720/1978 - Loss:  2.580, Seconds: 6.68\n",
      "Epoch   3/30 Batch 1730/1978 - Loss:  2.495, Seconds: 6.73\n",
      "Epoch   3/30 Batch 1740/1978 - Loss:  2.612, Seconds: 6.61\n",
      "Epoch   3/30 Batch 1750/1978 - Loss:  2.541, Seconds: 6.61\n",
      "Epoch   3/30 Batch 1760/1978 - Loss:  2.596, Seconds: 6.10\n",
      "Epoch   3/30 Batch 1770/1978 - Loss:  2.485, Seconds: 6.80\n",
      "Epoch   3/30 Batch 1780/1978 - Loss:  2.430, Seconds: 6.80\n",
      "Epoch   3/30 Batch 1790/1978 - Loss:  2.515, Seconds: 6.82\n",
      "Epoch   3/30 Batch 1800/1978 - Loss:  2.564, Seconds: 6.68\n",
      "Epoch   3/30 Batch 1810/1978 - Loss:  2.609, Seconds: 6.69\n",
      "Epoch   3/30 Batch 1820/1978 - Loss:  2.524, Seconds: 6.38\n",
      "Epoch   3/30 Batch 1830/1978 - Loss:  2.572, Seconds: 6.77\n",
      "Epoch   3/30 Batch 1840/1978 - Loss:  2.528, Seconds: 6.58\n",
      "Epoch   3/30 Batch 1850/1978 - Loss:  2.680, Seconds: 6.47\n",
      "Epoch   3/30 Batch 1860/1978 - Loss:  2.505, Seconds: 6.47\n",
      "Epoch   3/30 Batch 1870/1978 - Loss:  2.671, Seconds: 6.68\n",
      "Epoch   3/30 Batch 1880/1978 - Loss:  2.722, Seconds: 6.71\n",
      "Epoch   3/30 Batch 1890/1978 - Loss:  2.654, Seconds: 6.74\n",
      "Epoch   3/30 Batch 1900/1978 - Loss:  2.598, Seconds: 6.57\n",
      "Epoch   3/30 Batch 1910/1978 - Loss:  2.476, Seconds: 7.08\n",
      "Epoch   3/30 Batch 1920/1978 - Loss:  2.584, Seconds: 6.97\n",
      "Epoch   3/30 Batch 1930/1978 - Loss:  2.617, Seconds: 6.98\n",
      "Epoch   3/30 Batch 1940/1978 - Loss:  2.584, Seconds: 6.83\n",
      "Epoch   3/30 Batch 1950/1978 - Loss:  2.624, Seconds: 7.03\n",
      "Epoch   3/30 Batch 1960/1978 - Loss:  2.642, Seconds: 7.03\n",
      "Epoch   3/30 Batch 1970/1978 - Loss:  2.591, Seconds: 7.07\n",
      "Average loss for this update: 2.499\n",
      "No Improvement.\n",
      "Epoch   4/30 Batch   10/1978 - Loss:  2.482, Seconds: 2.29\n",
      "Epoch   4/30 Batch   20/1978 - Loss:  2.061, Seconds: 2.08\n",
      "Epoch   4/30 Batch   30/1978 - Loss:  2.049, Seconds: 2.24\n",
      "Epoch   4/30 Batch   40/1978 - Loss:  1.760, Seconds: 2.39\n",
      "Epoch   4/30 Batch   50/1978 - Loss:  1.860, Seconds: 2.59\n",
      "Epoch   4/30 Batch   60/1978 - Loss:  1.829, Seconds: 2.25\n",
      "Epoch   4/30 Batch   70/1978 - Loss:  1.876, Seconds: 2.25\n",
      "Epoch   4/30 Batch   80/1978 - Loss:  1.649, Seconds: 2.89\n",
      "Epoch   4/30 Batch   90/1978 - Loss:  1.679, Seconds: 2.66\n",
      "Epoch   4/30 Batch  100/1978 - Loss:  1.549, Seconds: 2.51\n",
      "Epoch   4/30 Batch  110/1978 - Loss:  1.804, Seconds: 2.82\n",
      "Epoch   4/30 Batch  120/1978 - Loss:  1.791, Seconds: 2.82\n",
      "Epoch   4/30 Batch  130/1978 - Loss:  1.676, Seconds: 2.79\n",
      "Epoch   4/30 Batch  140/1978 - Loss:  1.870, Seconds: 2.79\n",
      "Epoch   4/30 Batch  150/1978 - Loss:  1.838, Seconds: 3.09\n",
      "Epoch   4/30 Batch  160/1978 - Loss:  1.822, Seconds: 3.24\n",
      "Epoch   4/30 Batch  170/1978 - Loss:  1.926, Seconds: 2.83\n",
      "Epoch   4/30 Batch  180/1978 - Loss:  1.885, Seconds: 2.67\n",
      "Epoch   4/30 Batch  190/1978 - Loss:  1.964, Seconds: 2.88\n",
      "Epoch   4/30 Batch  200/1978 - Loss:  1.909, Seconds: 3.06\n",
      "Epoch   4/30 Batch  210/1978 - Loss:  1.793, Seconds: 3.20\n",
      "Epoch   4/30 Batch  220/1978 - Loss:  1.653, Seconds: 3.24\n",
      "Epoch   4/30 Batch  230/1978 - Loss:  1.682, Seconds: 3.51\n",
      "Epoch   4/30 Batch  240/1978 - Loss:  1.550, Seconds: 3.05\n",
      "Epoch   4/30 Batch  250/1978 - Loss:  1.570, Seconds: 3.58\n",
      "Epoch   4/30 Batch  260/1978 - Loss:  1.542, Seconds: 3.77\n",
      "Epoch   4/30 Batch  270/1978 - Loss:  1.616, Seconds: 3.17\n",
      "Epoch   4/30 Batch  280/1978 - Loss:  1.640, Seconds: 3.30\n",
      "Epoch   4/30 Batch  290/1978 - Loss:  1.651, Seconds: 3.42\n",
      "Epoch   4/30 Batch  300/1978 - Loss:  1.567, Seconds: 3.61\n",
      "Epoch   4/30 Batch  310/1978 - Loss:  1.489, Seconds: 3.78\n",
      "Epoch   4/30 Batch  320/1978 - Loss:  1.571, Seconds: 3.83\n",
      "Epoch   4/30 Batch  330/1978 - Loss:  1.521, Seconds: 3.72\n",
      "Epoch   4/30 Batch  340/1978 - Loss:  1.517, Seconds: 3.88\n",
      "Epoch   4/30 Batch  350/1978 - Loss:  1.567, Seconds: 3.92\n",
      "Epoch   4/30 Batch  360/1978 - Loss:  1.466, Seconds: 3.41\n",
      "Epoch   4/30 Batch  370/1978 - Loss:  1.569, Seconds: 3.44\n",
      "Epoch   4/30 Batch  380/1978 - Loss:  1.331, Seconds: 4.02\n",
      "Epoch   4/30 Batch  390/1978 - Loss:  1.438, Seconds: 3.57\n",
      "Epoch   4/30 Batch  400/1978 - Loss:  1.422, Seconds: 4.04\n",
      "Epoch   4/30 Batch  410/1978 - Loss:  1.651, Seconds: 3.96\n",
      "Epoch   4/30 Batch  420/1978 - Loss:  1.640, Seconds: 4.13\n",
      "Epoch   4/30 Batch  430/1978 - Loss:  1.652, Seconds: 4.00\n",
      "Epoch   4/30 Batch  440/1978 - Loss:  1.590, Seconds: 3.75\n",
      "Epoch   4/30 Batch  450/1978 - Loss:  1.608, Seconds: 4.21\n",
      "Epoch   4/30 Batch  460/1978 - Loss:  1.483, Seconds: 3.94\n",
      "Epoch   4/30 Batch  470/1978 - Loss:  1.494, Seconds: 4.24\n",
      "Epoch   4/30 Batch  480/1978 - Loss:  1.560, Seconds: 3.72\n",
      "Epoch   4/30 Batch  490/1978 - Loss:  1.500, Seconds: 4.15\n",
      "Epoch   4/30 Batch  500/1978 - Loss:  1.652, Seconds: 4.16\n",
      "Epoch   4/30 Batch  510/1978 - Loss:  1.679, Seconds: 4.34\n",
      "Epoch   4/30 Batch  520/1978 - Loss:  1.579, Seconds: 3.96\n",
      "Epoch   4/30 Batch  530/1978 - Loss:  1.525, Seconds: 4.39\n",
      "Epoch   4/30 Batch  540/1978 - Loss:  1.509, Seconds: 3.71\n",
      "Epoch   4/30 Batch  550/1978 - Loss:  1.523, Seconds: 4.42\n",
      "Epoch   4/30 Batch  560/1978 - Loss:  1.555, Seconds: 4.16\n",
      "Epoch   4/30 Batch  570/1978 - Loss:  1.564, Seconds: 4.34\n",
      "Epoch   4/30 Batch  580/1978 - Loss:  1.535, Seconds: 4.35\n",
      "Epoch   4/30 Batch  590/1978 - Loss:  1.474, Seconds: 4.35\n",
      "Epoch   4/30 Batch  600/1978 - Loss:  1.490, Seconds: 4.46\n",
      "Epoch   4/30 Batch  610/1978 - Loss:  1.408, Seconds: 4.58\n",
      "Epoch   4/30 Batch  620/1978 - Loss:  1.486, Seconds: 4.60\n",
      "Epoch   4/30 Batch  630/1978 - Loss:  1.632, Seconds: 4.58\n",
      "Epoch   4/30 Batch  640/1978 - Loss:  1.547, Seconds: 4.47\n",
      "Epoch   4/30 Batch  650/1978 - Loss:  1.460, Seconds: 4.61\n",
      "Average loss for this update: 1.644\n",
      "New Record!\n",
      "Epoch   4/30 Batch  660/1978 - Loss:  1.183, Seconds: 4.66\n",
      "Epoch   4/30 Batch  670/1978 - Loss:  1.135, Seconds: 4.66\n",
      "Epoch   4/30 Batch  680/1978 - Loss:  1.185, Seconds: 4.60\n",
      "Epoch   4/30 Batch  690/1978 - Loss:  1.182, Seconds: 4.67\n",
      "Epoch   4/30 Batch  700/1978 - Loss:  1.343, Seconds: 4.41\n",
      "Epoch   4/30 Batch  710/1978 - Loss:  1.205, Seconds: 4.73\n",
      "Epoch   4/30 Batch  720/1978 - Loss:  1.312, Seconds: 4.45\n",
      "Epoch   4/30 Batch  730/1978 - Loss:  1.341, Seconds: 4.58\n",
      "Epoch   4/30 Batch  740/1978 - Loss:  1.493, Seconds: 4.79\n",
      "Epoch   4/30 Batch  750/1978 - Loss:  1.548, Seconds: 4.79\n",
      "Epoch   4/30 Batch  760/1978 - Loss:  1.686, Seconds: 4.67\n",
      "Epoch   4/30 Batch  770/1978 - Loss:  1.598, Seconds: 4.83\n",
      "Epoch   4/30 Batch  780/1978 - Loss:  1.533, Seconds: 4.85\n",
      "Epoch   4/30 Batch  790/1978 - Loss:  1.602, Seconds: 4.87\n",
      "Epoch   4/30 Batch  800/1978 - Loss:  1.604, Seconds: 4.86\n",
      "Epoch   4/30 Batch  810/1978 - Loss:  1.639, Seconds: 4.77\n",
      "Epoch   4/30 Batch  820/1978 - Loss:  1.597, Seconds: 4.94\n",
      "Epoch   4/30 Batch  830/1978 - Loss:  1.628, Seconds: 4.95\n",
      "Epoch   4/30 Batch  840/1978 - Loss:  1.679, Seconds: 4.97\n",
      "Epoch   4/30 Batch  850/1978 - Loss:  1.684, Seconds: 4.98\n",
      "Epoch   4/30 Batch  860/1978 - Loss:  1.683, Seconds: 5.01\n",
      "Epoch   4/30 Batch  870/1978 - Loss:  1.694, Seconds: 5.04\n",
      "Epoch   4/30 Batch  880/1978 - Loss:  1.707, Seconds: 5.03\n",
      "Epoch   4/30 Batch  890/1978 - Loss:  1.704, Seconds: 5.08\n",
      "Epoch   4/30 Batch  900/1978 - Loss:  1.726, Seconds: 4.93\n",
      "Epoch   4/30 Batch  910/1978 - Loss:  1.738, Seconds: 4.81\n",
      "Epoch   4/30 Batch  920/1978 - Loss:  1.775, Seconds: 4.97\n",
      "Epoch   4/30 Batch  930/1978 - Loss:  1.728, Seconds: 5.05\n",
      "Epoch   4/30 Batch  940/1978 - Loss:  1.920, Seconds: 4.84\n",
      "Epoch   4/30 Batch  950/1978 - Loss:  1.892, Seconds: 4.89\n",
      "Epoch   4/30 Batch  960/1978 - Loss:  1.891, Seconds: 5.03\n",
      "Epoch   4/30 Batch  970/1978 - Loss:  1.855, Seconds: 4.62\n",
      "Epoch   4/30 Batch  980/1978 - Loss:  1.843, Seconds: 5.12\n",
      "Epoch   4/30 Batch  990/1978 - Loss:  1.767, Seconds: 5.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   4/30 Batch 1000/1978 - Loss:  1.907, Seconds: 5.11\n",
      "Epoch   4/30 Batch 1010/1978 - Loss:  1.783, Seconds: 5.32\n",
      "Epoch   4/30 Batch 1020/1978 - Loss:  1.915, Seconds: 5.16\n",
      "Epoch   4/30 Batch 1030/1978 - Loss:  1.909, Seconds: 5.34\n",
      "Epoch   4/30 Batch 1040/1978 - Loss:  1.936, Seconds: 5.34\n",
      "Epoch   4/30 Batch 1050/1978 - Loss:  1.964, Seconds: 5.21\n",
      "Epoch   4/30 Batch 1060/1978 - Loss:  1.934, Seconds: 5.40\n",
      "Epoch   4/30 Batch 1070/1978 - Loss:  1.848, Seconds: 5.23\n",
      "Epoch   4/30 Batch 1080/1978 - Loss:  1.851, Seconds: 5.27\n",
      "Epoch   4/30 Batch 1090/1978 - Loss:  1.828, Seconds: 5.13\n",
      "Epoch   4/30 Batch 1100/1978 - Loss:  1.854, Seconds: 5.30\n",
      "Epoch   4/30 Batch 1110/1978 - Loss:  1.923, Seconds: 5.33\n",
      "Epoch   4/30 Batch 1120/1978 - Loss:  1.912, Seconds: 5.35\n",
      "Epoch   4/30 Batch 1130/1978 - Loss:  1.810, Seconds: 5.19\n",
      "Epoch   4/30 Batch 1140/1978 - Loss:  1.788, Seconds: 5.52\n",
      "Epoch   4/30 Batch 1150/1978 - Loss:  1.842, Seconds: 5.55\n",
      "Epoch   4/30 Batch 1160/1978 - Loss:  1.787, Seconds: 5.56\n",
      "Epoch   4/30 Batch 1170/1978 - Loss:  1.874, Seconds: 5.27\n",
      "Epoch   4/30 Batch 1180/1978 - Loss:  1.893, Seconds: 5.28\n",
      "Epoch   4/30 Batch 1190/1978 - Loss:  1.872, Seconds: 5.45\n",
      "Epoch   4/30 Batch 1200/1978 - Loss:  1.774, Seconds: 5.48\n",
      "Epoch   4/30 Batch 1210/1978 - Loss:  1.924, Seconds: 5.49\n",
      "Epoch   4/30 Batch 1220/1978 - Loss:  1.868, Seconds: 5.67\n",
      "Epoch   4/30 Batch 1230/1978 - Loss:  1.859, Seconds: 5.21\n",
      "Epoch   4/30 Batch 1240/1978 - Loss:  1.864, Seconds: 5.56\n",
      "Epoch   4/30 Batch 1250/1978 - Loss:  1.865, Seconds: 5.58\n",
      "Epoch   4/30 Batch 1260/1978 - Loss:  1.951, Seconds: 5.59\n",
      "Epoch   4/30 Batch 1270/1978 - Loss:  1.954, Seconds: 5.66\n",
      "Epoch   4/30 Batch 1280/1978 - Loss:  1.967, Seconds: 5.80\n",
      "Epoch   4/30 Batch 1290/1978 - Loss:  2.062, Seconds: 5.63\n",
      "Epoch   4/30 Batch 1300/1978 - Loss:  1.950, Seconds: 5.67\n",
      "Epoch   4/30 Batch 1310/1978 - Loss:  2.067, Seconds: 5.89\n",
      "Average loss for this update: 1.747\n",
      "No Improvement.\n",
      "Epoch   4/30 Batch 1320/1978 - Loss:  2.085, Seconds: 5.55\n",
      "Epoch   4/30 Batch 1330/1978 - Loss:  2.164, Seconds: 5.59\n",
      "Epoch   4/30 Batch 1340/1978 - Loss:  2.149, Seconds: 5.42\n",
      "Epoch   4/30 Batch 1350/1978 - Loss:  2.144, Seconds: 5.45\n",
      "Epoch   4/30 Batch 1360/1978 - Loss:  2.231, Seconds: 5.30\n",
      "Epoch   4/30 Batch 1370/1978 - Loss:  2.218, Seconds: 5.49\n",
      "Epoch   4/30 Batch 1380/1978 - Loss:  2.275, Seconds: 5.98\n",
      "Epoch   4/30 Batch 1390/1978 - Loss:  2.194, Seconds: 5.87\n",
      "Epoch   4/30 Batch 1400/1978 - Loss:  2.257, Seconds: 5.57\n",
      "Epoch   4/30 Batch 1410/1978 - Loss:  2.241, Seconds: 5.71\n",
      "Epoch   4/30 Batch 1420/1978 - Loss:  2.257, Seconds: 5.88\n",
      "Epoch   4/30 Batch 1430/1978 - Loss:  2.392, Seconds: 5.63\n",
      "Epoch   4/30 Batch 1440/1978 - Loss:  2.273, Seconds: 5.79\n",
      "Epoch   4/30 Batch 1450/1978 - Loss:  2.291, Seconds: 5.66\n",
      "Epoch   4/30 Batch 1460/1978 - Loss:  2.307, Seconds: 5.99\n",
      "Epoch   4/30 Batch 1470/1978 - Loss:  2.296, Seconds: 5.70\n",
      "Epoch   4/30 Batch 1480/1978 - Loss:  2.269, Seconds: 5.69\n",
      "Epoch   4/30 Batch 1490/1978 - Loss:  2.390, Seconds: 6.02\n",
      "Epoch   4/30 Batch 1500/1978 - Loss:  2.313, Seconds: 5.73\n",
      "Epoch   4/30 Batch 1510/1978 - Loss:  2.309, Seconds: 5.93\n",
      "Epoch   4/30 Batch 1520/1978 - Loss:  2.324, Seconds: 5.94\n",
      "Epoch   4/30 Batch 1530/1978 - Loss:  2.260, Seconds: 6.29\n",
      "Epoch   4/30 Batch 1540/1978 - Loss:  2.293, Seconds: 6.14\n",
      "Epoch   4/30 Batch 1550/1978 - Loss:  2.395, Seconds: 6.17\n",
      "Epoch   4/30 Batch 1560/1978 - Loss:  2.423, Seconds: 6.32\n",
      "Epoch   4/30 Batch 1570/1978 - Loss:  2.329, Seconds: 5.94\n",
      "Epoch   4/30 Batch 1580/1978 - Loss:  2.326, Seconds: 6.07\n",
      "Epoch   4/30 Batch 1590/1978 - Loss:  2.331, Seconds: 6.25\n",
      "Epoch   4/30 Batch 1600/1978 - Loss:  2.417, Seconds: 5.94\n",
      "Epoch   4/30 Batch 1610/1978 - Loss:  2.312, Seconds: 6.12\n",
      "Epoch   4/30 Batch 1620/1978 - Loss:  2.360, Seconds: 5.98\n",
      "Epoch   4/30 Batch 1630/1978 - Loss:  2.326, Seconds: 6.49\n",
      "Epoch   4/30 Batch 1640/1978 - Loss:  2.367, Seconds: 6.35\n",
      "Epoch   4/30 Batch 1650/1978 - Loss:  2.345, Seconds: 6.19\n",
      "Epoch   4/30 Batch 1660/1978 - Loss:  2.367, Seconds: 6.56\n",
      "Epoch   4/30 Batch 1670/1978 - Loss:  2.371, Seconds: 6.21\n",
      "Epoch   4/30 Batch 1680/1978 - Loss:  2.456, Seconds: 6.27\n",
      "Epoch   4/30 Batch 1690/1978 - Loss:  2.527, Seconds: 6.14\n",
      "Epoch   4/30 Batch 1700/1978 - Loss:  2.436, Seconds: 6.30\n",
      "Epoch   4/30 Batch 1710/1978 - Loss:  2.418, Seconds: 6.49\n",
      "Epoch   4/30 Batch 1720/1978 - Loss:  2.432, Seconds: 6.65\n",
      "Epoch   4/30 Batch 1730/1978 - Loss:  2.350, Seconds: 6.70\n",
      "Epoch   4/30 Batch 1740/1978 - Loss:  2.471, Seconds: 6.55\n",
      "Epoch   4/30 Batch 1750/1978 - Loss:  2.424, Seconds: 6.57\n",
      "Epoch   4/30 Batch 1760/1978 - Loss:  2.486, Seconds: 6.10\n",
      "Epoch   4/30 Batch 1770/1978 - Loss:  2.391, Seconds: 6.78\n",
      "Epoch   4/30 Batch 1780/1978 - Loss:  2.316, Seconds: 6.80\n",
      "Epoch   4/30 Batch 1790/1978 - Loss:  2.392, Seconds: 6.84\n",
      "Epoch   4/30 Batch 1800/1978 - Loss:  2.418, Seconds: 6.67\n",
      "Epoch   4/30 Batch 1810/1978 - Loss:  2.483, Seconds: 6.69\n",
      "Epoch   4/30 Batch 1820/1978 - Loss:  2.418, Seconds: 6.37\n",
      "Epoch   4/30 Batch 1830/1978 - Loss:  2.445, Seconds: 6.75\n",
      "Epoch   4/30 Batch 1840/1978 - Loss:  2.379, Seconds: 6.60\n",
      "Epoch   4/30 Batch 1850/1978 - Loss:  2.544, Seconds: 6.45\n",
      "Epoch   4/30 Batch 1860/1978 - Loss:  2.359, Seconds: 6.50\n",
      "Epoch   4/30 Batch 1870/1978 - Loss:  2.545, Seconds: 6.65\n",
      "Epoch   4/30 Batch 1880/1978 - Loss:  2.607, Seconds: 6.70\n",
      "Epoch   4/30 Batch 1890/1978 - Loss:  2.552, Seconds: 6.72\n",
      "Epoch   4/30 Batch 1900/1978 - Loss:  2.465, Seconds: 6.56\n",
      "Epoch   4/30 Batch 1910/1978 - Loss:  2.354, Seconds: 7.09\n",
      "Epoch   4/30 Batch 1920/1978 - Loss:  2.446, Seconds: 6.95\n",
      "Epoch   4/30 Batch 1930/1978 - Loss:  2.504, Seconds: 6.99\n",
      "Epoch   4/30 Batch 1940/1978 - Loss:  2.453, Seconds: 6.83\n",
      "Epoch   4/30 Batch 1950/1978 - Loss:  2.493, Seconds: 7.04\n",
      "Epoch   4/30 Batch 1960/1978 - Loss:  2.506, Seconds: 7.04\n",
      "Epoch   4/30 Batch 1970/1978 - Loss:  2.469, Seconds: 7.05\n",
      "Average loss for this update: 2.37\n",
      "No Improvement.\n",
      "Epoch   5/30 Batch   10/1978 - Loss:  2.245, Seconds: 2.30\n",
      "Epoch   5/30 Batch   20/1978 - Loss:  1.868, Seconds: 2.11\n",
      "Epoch   5/30 Batch   30/1978 - Loss:  1.954, Seconds: 2.31\n",
      "Epoch   5/30 Batch   40/1978 - Loss:  1.683, Seconds: 2.39\n",
      "Epoch   5/30 Batch   50/1978 - Loss:  1.705, Seconds: 2.53\n",
      "Epoch   5/30 Batch   60/1978 - Loss:  1.735, Seconds: 2.10\n",
      "Epoch   5/30 Batch   70/1978 - Loss:  1.752, Seconds: 2.50\n",
      "Epoch   5/30 Batch   80/1978 - Loss:  1.575, Seconds: 2.82\n",
      "Epoch   5/30 Batch   90/1978 - Loss:  1.583, Seconds: 2.74\n",
      "Epoch   5/30 Batch  100/1978 - Loss:  1.468, Seconds: 2.46\n",
      "Epoch   5/30 Batch  110/1978 - Loss:  1.686, Seconds: 3.10\n",
      "Epoch   5/30 Batch  120/1978 - Loss:  1.674, Seconds: 2.85\n",
      "Epoch   5/30 Batch  130/1978 - Loss:  1.576, Seconds: 2.76\n",
      "Epoch   5/30 Batch  140/1978 - Loss:  1.782, Seconds: 2.79\n",
      "Epoch   5/30 Batch  150/1978 - Loss:  1.733, Seconds: 3.08\n",
      "Epoch   5/30 Batch  160/1978 - Loss:  1.745, Seconds: 3.26\n",
      "Epoch   5/30 Batch  170/1978 - Loss:  1.826, Seconds: 2.83\n",
      "Epoch   5/30 Batch  180/1978 - Loss:  1.852, Seconds: 3.00\n",
      "Epoch   5/30 Batch  190/1978 - Loss:  1.873, Seconds: 2.84\n",
      "Epoch   5/30 Batch  200/1978 - Loss:  1.831, Seconds: 3.11\n",
      "Epoch   5/30 Batch  210/1978 - Loss:  1.713, Seconds: 3.15\n",
      "Epoch   5/30 Batch  220/1978 - Loss:  1.567, Seconds: 3.10\n",
      "Epoch   5/30 Batch  230/1978 - Loss:  1.615, Seconds: 3.57\n",
      "Epoch   5/30 Batch  240/1978 - Loss:  1.479, Seconds: 3.08\n",
      "Epoch   5/30 Batch  250/1978 - Loss:  1.514, Seconds: 3.56\n",
      "Epoch   5/30 Batch  260/1978 - Loss:  1.491, Seconds: 3.61\n",
      "Epoch   5/30 Batch  270/1978 - Loss:  1.538, Seconds: 3.29\n",
      "Epoch   5/30 Batch  280/1978 - Loss:  1.571, Seconds: 3.36\n",
      "Epoch   5/30 Batch  290/1978 - Loss:  1.576, Seconds: 3.47\n",
      "Epoch   5/30 Batch  300/1978 - Loss:  1.497, Seconds: 3.62\n",
      "Epoch   5/30 Batch  310/1978 - Loss:  1.444, Seconds: 3.83\n",
      "Epoch   5/30 Batch  320/1978 - Loss:  1.503, Seconds: 3.82\n",
      "Epoch   5/30 Batch  330/1978 - Loss:  1.475, Seconds: 3.71\n",
      "Epoch   5/30 Batch  340/1978 - Loss:  1.459, Seconds: 3.89\n",
      "Epoch   5/30 Batch  350/1978 - Loss:  1.503, Seconds: 3.91\n",
      "Epoch   5/30 Batch  360/1978 - Loss:  1.397, Seconds: 3.38\n",
      "Epoch   5/30 Batch  370/1978 - Loss:  1.468, Seconds: 3.45\n",
      "Epoch   5/30 Batch  380/1978 - Loss:  1.268, Seconds: 3.99\n",
      "Epoch   5/30 Batch  390/1978 - Loss:  1.377, Seconds: 3.52\n",
      "Epoch   5/30 Batch  400/1978 - Loss:  1.336, Seconds: 4.03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   5/30 Batch  410/1978 - Loss:  1.593, Seconds: 3.93\n",
      "Epoch   5/30 Batch  420/1978 - Loss:  1.577, Seconds: 4.13\n",
      "Epoch   5/30 Batch  430/1978 - Loss:  1.599, Seconds: 4.00\n",
      "Epoch   5/30 Batch  440/1978 - Loss:  1.517, Seconds: 3.74\n",
      "Epoch   5/30 Batch  450/1978 - Loss:  1.526, Seconds: 4.19\n",
      "Epoch   5/30 Batch  460/1978 - Loss:  1.421, Seconds: 4.01\n",
      "Epoch   5/30 Batch  470/1978 - Loss:  1.422, Seconds: 4.24\n",
      "Epoch   5/30 Batch  480/1978 - Loss:  1.486, Seconds: 3.77\n",
      "Epoch   5/30 Batch  490/1978 - Loss:  1.399, Seconds: 4.18\n",
      "Epoch   5/30 Batch  500/1978 - Loss:  1.595, Seconds: 4.18\n",
      "Epoch   5/30 Batch  510/1978 - Loss:  1.581, Seconds: 4.33\n",
      "Epoch   5/30 Batch  520/1978 - Loss:  1.493, Seconds: 3.93\n",
      "Epoch   5/30 Batch  530/1978 - Loss:  1.432, Seconds: 4.37\n",
      "Epoch   5/30 Batch  540/1978 - Loss:  1.439, Seconds: 4.02\n",
      "Epoch   5/30 Batch  550/1978 - Loss:  1.453, Seconds: 4.44\n",
      "Epoch   5/30 Batch  560/1978 - Loss:  1.471, Seconds: 4.16\n",
      "Epoch   5/30 Batch  570/1978 - Loss:  1.446, Seconds: 4.35\n",
      "Epoch   5/30 Batch  580/1978 - Loss:  1.440, Seconds: 4.34\n",
      "Epoch   5/30 Batch  590/1978 - Loss:  1.394, Seconds: 4.36\n",
      "Epoch   5/30 Batch  600/1978 - Loss:  1.427, Seconds: 4.40\n",
      "Epoch   5/30 Batch  610/1978 - Loss:  1.317, Seconds: 4.60\n",
      "Epoch   5/30 Batch  620/1978 - Loss:  1.408, Seconds: 4.56\n",
      "Epoch   5/30 Batch  630/1978 - Loss:  1.540, Seconds: 4.57\n",
      "Epoch   5/30 Batch  640/1978 - Loss:  1.467, Seconds: 4.46\n",
      "Epoch   5/30 Batch  650/1978 - Loss:  1.393, Seconds: 4.64\n",
      "Average loss for this update: 1.56\n",
      "New Record!\n",
      "Epoch   5/30 Batch  660/1978 - Loss:  1.108, Seconds: 4.63\n",
      "Epoch   5/30 Batch  670/1978 - Loss:  1.101, Seconds: 4.64\n",
      "Epoch   5/30 Batch  680/1978 - Loss:  1.140, Seconds: 4.50\n",
      "Epoch   5/30 Batch  690/1978 - Loss:  1.148, Seconds: 4.68\n",
      "Epoch   5/30 Batch  700/1978 - Loss:  1.266, Seconds: 4.39\n",
      "Epoch   5/30 Batch  710/1978 - Loss:  1.149, Seconds: 4.71\n",
      "Epoch   5/30 Batch  720/1978 - Loss:  1.254, Seconds: 4.49\n",
      "Epoch   5/30 Batch  730/1978 - Loss:  1.274, Seconds: 4.61\n",
      "Epoch   5/30 Batch  740/1978 - Loss:  1.406, Seconds: 4.74\n",
      "Epoch   5/30 Batch  750/1978 - Loss:  1.463, Seconds: 4.79\n",
      "Epoch   5/30 Batch  760/1978 - Loss:  1.595, Seconds: 4.66\n",
      "Epoch   5/30 Batch  770/1978 - Loss:  1.513, Seconds: 4.83\n",
      "Epoch   5/30 Batch  780/1978 - Loss:  1.460, Seconds: 4.84\n",
      "Epoch   5/30 Batch  790/1978 - Loss:  1.529, Seconds: 4.87\n",
      "Epoch   5/30 Batch  800/1978 - Loss:  1.540, Seconds: 4.90\n",
      "Epoch   5/30 Batch  810/1978 - Loss:  1.597, Seconds: 4.81\n",
      "Epoch   5/30 Batch  820/1978 - Loss:  1.514, Seconds: 4.94\n",
      "Epoch   5/30 Batch  830/1978 - Loss:  1.564, Seconds: 4.96\n",
      "Epoch   5/30 Batch  840/1978 - Loss:  1.610, Seconds: 4.96\n",
      "Epoch   5/30 Batch  850/1978 - Loss:  1.604, Seconds: 5.00\n",
      "Epoch   5/30 Batch  860/1978 - Loss:  1.608, Seconds: 5.01\n",
      "Epoch   5/30 Batch  870/1978 - Loss:  1.609, Seconds: 5.01\n",
      "Epoch   5/30 Batch  880/1978 - Loss:  1.632, Seconds: 5.05\n",
      "Epoch   5/30 Batch  890/1978 - Loss:  1.646, Seconds: 5.05\n",
      "Epoch   5/30 Batch  900/1978 - Loss:  1.658, Seconds: 4.97\n",
      "Epoch   5/30 Batch  910/1978 - Loss:  1.668, Seconds: 4.80\n",
      "Epoch   5/30 Batch  920/1978 - Loss:  1.696, Seconds: 4.98\n",
      "Epoch   5/30 Batch  930/1978 - Loss:  1.652, Seconds: 4.98\n",
      "Epoch   5/30 Batch  940/1978 - Loss:  1.856, Seconds: 4.84\n",
      "Epoch   5/30 Batch  950/1978 - Loss:  1.806, Seconds: 4.92\n",
      "Epoch   5/30 Batch  960/1978 - Loss:  1.803, Seconds: 5.07\n",
      "Epoch   5/30 Batch  970/1978 - Loss:  1.765, Seconds: 4.64\n",
      "Epoch   5/30 Batch  980/1978 - Loss:  1.760, Seconds: 5.09\n",
      "Epoch   5/30 Batch  990/1978 - Loss:  1.691, Seconds: 5.26\n",
      "Epoch   5/30 Batch 1000/1978 - Loss:  1.828, Seconds: 5.13\n",
      "Epoch   5/30 Batch 1010/1978 - Loss:  1.698, Seconds: 5.31\n",
      "Epoch   5/30 Batch 1020/1978 - Loss:  1.835, Seconds: 5.15\n",
      "Epoch   5/30 Batch 1030/1978 - Loss:  1.844, Seconds: 5.38\n",
      "Epoch   5/30 Batch 1040/1978 - Loss:  1.883, Seconds: 5.34\n",
      "Epoch   5/30 Batch 1050/1978 - Loss:  1.887, Seconds: 5.20\n",
      "Epoch   5/30 Batch 1060/1978 - Loss:  1.846, Seconds: 5.40\n",
      "Epoch   5/30 Batch 1070/1978 - Loss:  1.777, Seconds: 5.23\n",
      "Epoch   5/30 Batch 1080/1978 - Loss:  1.803, Seconds: 5.27\n",
      "Epoch   5/30 Batch 1090/1978 - Loss:  1.771, Seconds: 5.13\n",
      "Epoch   5/30 Batch 1100/1978 - Loss:  1.777, Seconds: 5.29\n",
      "Epoch   5/30 Batch 1110/1978 - Loss:  1.850, Seconds: 5.33\n",
      "Epoch   5/30 Batch 1120/1978 - Loss:  1.852, Seconds: 5.34\n",
      "Epoch   5/30 Batch 1130/1978 - Loss:  1.736, Seconds: 5.25\n",
      "Epoch   5/30 Batch 1140/1978 - Loss:  1.716, Seconds: 5.53\n",
      "Epoch   5/30 Batch 1150/1978 - Loss:  1.768, Seconds: 5.54\n",
      "Epoch   5/30 Batch 1160/1978 - Loss:  1.725, Seconds: 5.57\n",
      "Epoch   5/30 Batch 1170/1978 - Loss:  1.793, Seconds: 5.26\n",
      "Epoch   5/30 Batch 1180/1978 - Loss:  1.814, Seconds: 5.30\n",
      "Epoch   5/30 Batch 1190/1978 - Loss:  1.804, Seconds: 5.47\n",
      "Epoch   5/30 Batch 1200/1978 - Loss:  1.724, Seconds: 5.47\n",
      "Epoch   5/30 Batch 1210/1978 - Loss:  1.827, Seconds: 5.54\n",
      "Epoch   5/30 Batch 1220/1978 - Loss:  1.808, Seconds: 5.66\n",
      "Epoch   5/30 Batch 1230/1978 - Loss:  1.802, Seconds: 5.17\n",
      "Epoch   5/30 Batch 1240/1978 - Loss:  1.774, Seconds: 5.57\n",
      "Epoch   5/30 Batch 1250/1978 - Loss:  1.796, Seconds: 5.57\n",
      "Epoch   5/30 Batch 1260/1978 - Loss:  1.889, Seconds: 5.62\n",
      "Epoch   5/30 Batch 1270/1978 - Loss:  1.879, Seconds: 5.61\n",
      "Epoch   5/30 Batch 1280/1978 - Loss:  1.900, Seconds: 5.79\n",
      "Epoch   5/30 Batch 1290/1978 - Loss:  1.975, Seconds: 5.66\n",
      "Epoch   5/30 Batch 1300/1978 - Loss:  1.879, Seconds: 5.69\n",
      "Epoch   5/30 Batch 1310/1978 - Loss:  1.994, Seconds: 5.85\n",
      "Average loss for this update: 1.676\n",
      "No Improvement.\n",
      "Epoch   5/30 Batch 1320/1978 - Loss:  1.999, Seconds: 5.53\n",
      "Epoch   5/30 Batch 1330/1978 - Loss:  2.080, Seconds: 5.56\n",
      "Epoch   5/30 Batch 1340/1978 - Loss:  2.047, Seconds: 5.44\n",
      "Epoch   5/30 Batch 1350/1978 - Loss:  2.073, Seconds: 5.43\n",
      "Epoch   5/30 Batch 1360/1978 - Loss:  2.163, Seconds: 5.31\n",
      "Epoch   5/30 Batch 1370/1978 - Loss:  2.138, Seconds: 5.49\n",
      "Epoch   5/30 Batch 1380/1978 - Loss:  2.207, Seconds: 5.98\n",
      "Epoch   5/30 Batch 1390/1978 - Loss:  2.126, Seconds: 5.82\n",
      "Epoch   5/30 Batch 1400/1978 - Loss:  2.187, Seconds: 5.54\n",
      "Epoch   5/30 Batch 1410/1978 - Loss:  2.151, Seconds: 5.73\n",
      "Epoch   5/30 Batch 1420/1978 - Loss:  2.152, Seconds: 5.89\n",
      "Epoch   5/30 Batch 1430/1978 - Loss:  2.300, Seconds: 5.59\n",
      "Epoch   5/30 Batch 1440/1978 - Loss:  2.188, Seconds: 5.77\n",
      "Epoch   5/30 Batch 1450/1978 - Loss:  2.223, Seconds: 5.63\n",
      "Epoch   5/30 Batch 1460/1978 - Loss:  2.248, Seconds: 5.97\n",
      "Epoch   5/30 Batch 1470/1978 - Loss:  2.220, Seconds: 5.69\n",
      "Epoch   5/30 Batch 1480/1978 - Loss:  2.188, Seconds: 5.70\n",
      "Epoch   5/30 Batch 1490/1978 - Loss:  2.306, Seconds: 6.05\n",
      "Epoch   5/30 Batch 1500/1978 - Loss:  2.217, Seconds: 5.75\n",
      "Epoch   5/30 Batch 1510/1978 - Loss:  2.272, Seconds: 5.92\n",
      "Epoch   5/30 Batch 1520/1978 - Loss:  2.239, Seconds: 5.93\n",
      "Epoch   5/30 Batch 1530/1978 - Loss:  2.180, Seconds: 6.28\n",
      "Epoch   5/30 Batch 1540/1978 - Loss:  2.235, Seconds: 6.12\n",
      "Epoch   5/30 Batch 1550/1978 - Loss:  2.332, Seconds: 6.19\n",
      "Epoch   5/30 Batch 1560/1978 - Loss:  2.333, Seconds: 6.31\n",
      "Epoch   5/30 Batch 1570/1978 - Loss:  2.235, Seconds: 5.87\n",
      "Epoch   5/30 Batch 1580/1978 - Loss:  2.231, Seconds: 6.06\n",
      "Epoch   5/30 Batch 1590/1978 - Loss:  2.242, Seconds: 6.25\n",
      "Epoch   5/30 Batch 1600/1978 - Loss:  2.322, Seconds: 5.93\n",
      "Epoch   5/30 Batch 1610/1978 - Loss:  2.245, Seconds: 6.13\n",
      "Epoch   5/30 Batch 1620/1978 - Loss:  2.293, Seconds: 5.98\n",
      "Epoch   5/30 Batch 1630/1978 - Loss:  2.256, Seconds: 6.51\n",
      "Epoch   5/30 Batch 1640/1978 - Loss:  2.288, Seconds: 6.37\n",
      "Epoch   5/30 Batch 1650/1978 - Loss:  2.240, Seconds: 6.20\n",
      "Epoch   5/30 Batch 1660/1978 - Loss:  2.291, Seconds: 6.60\n",
      "Epoch   5/30 Batch 1670/1978 - Loss:  2.283, Seconds: 6.25\n",
      "Epoch   5/30 Batch 1680/1978 - Loss:  2.367, Seconds: 6.28\n",
      "Epoch   5/30 Batch 1690/1978 - Loss:  2.444, Seconds: 6.12\n",
      "Epoch   5/30 Batch 1700/1978 - Loss:  2.362, Seconds: 6.31\n",
      "Epoch   5/30 Batch 1710/1978 - Loss:  2.350, Seconds: 6.50\n",
      "Epoch   5/30 Batch 1720/1978 - Loss:  2.367, Seconds: 6.68\n",
      "Epoch   5/30 Batch 1730/1978 - Loss:  2.266, Seconds: 6.72\n",
      "Epoch   5/30 Batch 1740/1978 - Loss:  2.383, Seconds: 6.57\n",
      "Epoch   5/30 Batch 1750/1978 - Loss:  2.353, Seconds: 6.59\n",
      "Epoch   5/30 Batch 1760/1978 - Loss:  2.400, Seconds: 6.10\n",
      "Epoch   5/30 Batch 1770/1978 - Loss:  2.307, Seconds: 6.78\n",
      "Epoch   5/30 Batch 1780/1978 - Loss:  2.247, Seconds: 6.80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   5/30 Batch 1790/1978 - Loss:  2.312, Seconds: 6.82\n",
      "Epoch   5/30 Batch 1800/1978 - Loss:  2.361, Seconds: 6.70\n",
      "Epoch   5/30 Batch 1810/1978 - Loss:  2.400, Seconds: 6.71\n",
      "Epoch   5/30 Batch 1820/1978 - Loss:  2.307, Seconds: 6.38\n",
      "Epoch   5/30 Batch 1830/1978 - Loss:  2.369, Seconds: 6.75\n",
      "Epoch   5/30 Batch 1840/1978 - Loss:  2.297, Seconds: 6.57\n",
      "Epoch   5/30 Batch 1850/1978 - Loss:  2.447, Seconds: 6.46\n",
      "Epoch   5/30 Batch 1860/1978 - Loss:  2.303, Seconds: 6.46\n",
      "Epoch   5/30 Batch 1870/1978 - Loss:  2.482, Seconds: 6.67\n",
      "Epoch   5/30 Batch 1880/1978 - Loss:  2.526, Seconds: 6.70\n",
      "Epoch   5/30 Batch 1890/1978 - Loss:  2.451, Seconds: 6.71\n",
      "Epoch   5/30 Batch 1900/1978 - Loss:  2.407, Seconds: 6.55\n",
      "Epoch   5/30 Batch 1910/1978 - Loss:  2.293, Seconds: 7.09\n",
      "Epoch   5/30 Batch 1920/1978 - Loss:  2.392, Seconds: 6.97\n",
      "Epoch   5/30 Batch 1930/1978 - Loss:  2.424, Seconds: 6.97\n",
      "Epoch   5/30 Batch 1940/1978 - Loss:  2.377, Seconds: 6.84\n",
      "Epoch   5/30 Batch 1950/1978 - Loss:  2.410, Seconds: 7.01\n",
      "Epoch   5/30 Batch 1960/1978 - Loss:  2.419, Seconds: 7.02\n",
      "Epoch   5/30 Batch 1970/1978 - Loss:  2.420, Seconds: 7.05\n",
      "Average loss for this update: 2.292\n",
      "No Improvement.\n",
      "Epoch   6/30 Batch   10/1978 - Loss:  2.121, Seconds: 2.28\n",
      "Epoch   6/30 Batch   20/1978 - Loss:  1.785, Seconds: 2.10\n",
      "Epoch   6/30 Batch   30/1978 - Loss:  1.849, Seconds: 2.30\n",
      "Epoch   6/30 Batch   40/1978 - Loss:  1.602, Seconds: 2.40\n",
      "Epoch   6/30 Batch   50/1978 - Loss:  1.669, Seconds: 2.58\n",
      "Epoch   6/30 Batch   60/1978 - Loss:  1.656, Seconds: 2.24\n",
      "Epoch   6/30 Batch   70/1978 - Loss:  1.736, Seconds: 2.32\n",
      "Epoch   6/30 Batch   80/1978 - Loss:  1.527, Seconds: 2.82\n",
      "Epoch   6/30 Batch   90/1978 - Loss:  1.558, Seconds: 2.65\n",
      "Epoch   6/30 Batch  100/1978 - Loss:  1.393, Seconds: 2.40\n",
      "Epoch   6/30 Batch  110/1978 - Loss:  1.633, Seconds: 2.82\n",
      "Epoch   6/30 Batch  120/1978 - Loss:  1.652, Seconds: 2.65\n",
      "Epoch   6/30 Batch  130/1978 - Loss:  1.563, Seconds: 2.82\n",
      "Epoch   6/30 Batch  140/1978 - Loss:  1.747, Seconds: 2.80\n",
      "Epoch   6/30 Batch  150/1978 - Loss:  1.681, Seconds: 3.19\n",
      "Epoch   6/30 Batch  160/1978 - Loss:  1.689, Seconds: 3.24\n",
      "Epoch   6/30 Batch  170/1978 - Loss:  1.784, Seconds: 3.01\n",
      "Epoch   6/30 Batch  180/1978 - Loss:  1.782, Seconds: 2.70\n",
      "Epoch   6/30 Batch  190/1978 - Loss:  1.814, Seconds: 2.90\n",
      "Epoch   6/30 Batch  200/1978 - Loss:  1.781, Seconds: 2.88\n",
      "Epoch   6/30 Batch  210/1978 - Loss:  1.660, Seconds: 3.05\n",
      "Epoch   6/30 Batch  220/1978 - Loss:  1.505, Seconds: 3.18\n",
      "Epoch   6/30 Batch  230/1978 - Loss:  1.551, Seconds: 3.66\n",
      "Epoch   6/30 Batch  240/1978 - Loss:  1.447, Seconds: 3.03\n",
      "Epoch   6/30 Batch  250/1978 - Loss:  1.464, Seconds: 3.75\n",
      "Epoch   6/30 Batch  260/1978 - Loss:  1.442, Seconds: 3.72\n",
      "Epoch   6/30 Batch  270/1978 - Loss:  1.510, Seconds: 3.01\n",
      "Epoch   6/30 Batch  280/1978 - Loss:  1.529, Seconds: 3.27\n",
      "Epoch   6/30 Batch  290/1978 - Loss:  1.547, Seconds: 3.45\n",
      "Epoch   6/30 Batch  300/1978 - Loss:  1.458, Seconds: 3.77\n",
      "Epoch   6/30 Batch  310/1978 - Loss:  1.402, Seconds: 3.80\n",
      "Epoch   6/30 Batch  320/1978 - Loss:  1.469, Seconds: 3.79\n",
      "Epoch   6/30 Batch  330/1978 - Loss:  1.439, Seconds: 3.81\n",
      "Epoch   6/30 Batch  340/1978 - Loss:  1.422, Seconds: 4.07\n",
      "Epoch   6/30 Batch  350/1978 - Loss:  1.478, Seconds: 3.93\n",
      "Epoch   6/30 Batch  360/1978 - Loss:  1.368, Seconds: 3.39\n",
      "Epoch   6/30 Batch  370/1978 - Loss:  1.438, Seconds: 3.44\n",
      "Epoch   6/30 Batch  380/1978 - Loss:  1.223, Seconds: 4.01\n",
      "Epoch   6/30 Batch  390/1978 - Loss:  1.329, Seconds: 3.46\n",
      "Epoch   6/30 Batch  400/1978 - Loss:  1.297, Seconds: 4.04\n",
      "Epoch   6/30 Batch  410/1978 - Loss:  1.519, Seconds: 3.94\n",
      "Epoch   6/30 Batch  420/1978 - Loss:  1.540, Seconds: 4.12\n",
      "Epoch   6/30 Batch  430/1978 - Loss:  1.546, Seconds: 3.99\n",
      "Epoch   6/30 Batch  440/1978 - Loss:  1.496, Seconds: 3.78\n",
      "Epoch   6/30 Batch  450/1978 - Loss:  1.484, Seconds: 4.19\n",
      "Epoch   6/30 Batch  460/1978 - Loss:  1.391, Seconds: 3.97\n",
      "Epoch   6/30 Batch  470/1978 - Loss:  1.408, Seconds: 4.25\n",
      "Epoch   6/30 Batch  480/1978 - Loss:  1.472, Seconds: 3.99\n",
      "Epoch   6/30 Batch  490/1978 - Loss:  1.369, Seconds: 4.15\n",
      "Epoch   6/30 Batch  500/1978 - Loss:  1.541, Seconds: 4.20\n",
      "Epoch   6/30 Batch  510/1978 - Loss:  1.549, Seconds: 4.36\n",
      "Epoch   6/30 Batch  520/1978 - Loss:  1.438, Seconds: 4.12\n",
      "Epoch   6/30 Batch  530/1978 - Loss:  1.407, Seconds: 4.40\n",
      "Epoch   6/30 Batch  540/1978 - Loss:  1.374, Seconds: 3.75\n",
      "Epoch   6/30 Batch  550/1978 - Loss:  1.378, Seconds: 4.44\n",
      "Epoch   6/30 Batch  560/1978 - Loss:  1.423, Seconds: 4.19\n",
      "Epoch   6/30 Batch  570/1978 - Loss:  1.423, Seconds: 4.32\n",
      "Epoch   6/30 Batch  580/1978 - Loss:  1.390, Seconds: 4.36\n",
      "Epoch   6/30 Batch  590/1978 - Loss:  1.369, Seconds: 4.38\n",
      "Epoch   6/30 Batch  600/1978 - Loss:  1.383, Seconds: 4.39\n",
      "Epoch   6/30 Batch  610/1978 - Loss:  1.292, Seconds: 4.56\n",
      "Epoch   6/30 Batch  620/1978 - Loss:  1.370, Seconds: 4.56\n",
      "Epoch   6/30 Batch  630/1978 - Loss:  1.521, Seconds: 4.59\n",
      "Epoch   6/30 Batch  640/1978 - Loss:  1.445, Seconds: 4.48\n",
      "Epoch   6/30 Batch  650/1978 - Loss:  1.345, Seconds: 4.65\n",
      "Average loss for this update: 1.516\n",
      "New Record!\n",
      "Epoch   6/30 Batch  660/1978 - Loss:  1.083, Seconds: 4.64\n",
      "Epoch   6/30 Batch  670/1978 - Loss:  1.070, Seconds: 4.69\n",
      "Epoch   6/30 Batch  680/1978 - Loss:  1.090, Seconds: 4.51\n",
      "Epoch   6/30 Batch  690/1978 - Loss:  1.101, Seconds: 4.69\n",
      "Epoch   6/30 Batch  700/1978 - Loss:  1.244, Seconds: 4.41\n",
      "Epoch   6/30 Batch  710/1978 - Loss:  1.101, Seconds: 4.73\n",
      "Epoch   6/30 Batch  720/1978 - Loss:  1.217, Seconds: 4.45\n",
      "Epoch   6/30 Batch  730/1978 - Loss:  1.253, Seconds: 4.59\n",
      "Epoch   6/30 Batch  740/1978 - Loss:  1.367, Seconds: 4.76\n",
      "Epoch   6/30 Batch  750/1978 - Loss:  1.417, Seconds: 4.81\n",
      "Epoch   6/30 Batch  760/1978 - Loss:  1.537, Seconds: 4.67\n",
      "Epoch   6/30 Batch  770/1978 - Loss:  1.481, Seconds: 4.86\n",
      "Epoch   6/30 Batch  780/1978 - Loss:  1.409, Seconds: 4.88\n",
      "Epoch   6/30 Batch  790/1978 - Loss:  1.504, Seconds: 4.89\n",
      "Epoch   6/30 Batch  800/1978 - Loss:  1.501, Seconds: 4.88\n",
      "Epoch   6/30 Batch  810/1978 - Loss:  1.538, Seconds: 4.76\n",
      "Epoch   6/30 Batch  820/1978 - Loss:  1.490, Seconds: 4.93\n",
      "Epoch   6/30 Batch  830/1978 - Loss:  1.531, Seconds: 4.97\n",
      "Epoch   6/30 Batch  840/1978 - Loss:  1.565, Seconds: 4.99\n",
      "Epoch   6/30 Batch  850/1978 - Loss:  1.570, Seconds: 4.99\n",
      "Epoch   6/30 Batch  860/1978 - Loss:  1.566, Seconds: 4.99\n",
      "Epoch   6/30 Batch  870/1978 - Loss:  1.576, Seconds: 5.01\n",
      "Epoch   6/30 Batch  880/1978 - Loss:  1.594, Seconds: 5.02\n",
      "Epoch   6/30 Batch  890/1978 - Loss:  1.593, Seconds: 5.06\n",
      "Epoch   6/30 Batch  900/1978 - Loss:  1.609, Seconds: 4.94\n",
      "Epoch   6/30 Batch  910/1978 - Loss:  1.616, Seconds: 4.78\n",
      "Epoch   6/30 Batch  920/1978 - Loss:  1.653, Seconds: 4.97\n",
      "Epoch   6/30 Batch  930/1978 - Loss:  1.598, Seconds: 4.99\n",
      "Epoch   6/30 Batch  940/1978 - Loss:  1.803, Seconds: 4.84\n",
      "Epoch   6/30 Batch  950/1978 - Loss:  1.778, Seconds: 4.89\n",
      "Epoch   6/30 Batch  960/1978 - Loss:  1.790, Seconds: 5.06\n",
      "Epoch   6/30 Batch  970/1978 - Loss:  1.708, Seconds: 4.62\n",
      "Epoch   6/30 Batch  980/1978 - Loss:  1.713, Seconds: 5.10\n",
      "Epoch   6/30 Batch  990/1978 - Loss:  1.644, Seconds: 5.25\n",
      "Epoch   6/30 Batch 1000/1978 - Loss:  1.773, Seconds: 5.12\n",
      "Epoch   6/30 Batch 1010/1978 - Loss:  1.662, Seconds: 5.29\n",
      "Epoch   6/30 Batch 1020/1978 - Loss:  1.796, Seconds: 5.17\n",
      "Epoch   6/30 Batch 1030/1978 - Loss:  1.802, Seconds: 5.33\n",
      "Epoch   6/30 Batch 1040/1978 - Loss:  1.826, Seconds: 5.37\n",
      "Epoch   6/30 Batch 1050/1978 - Loss:  1.846, Seconds: 5.20\n",
      "Epoch   6/30 Batch 1060/1978 - Loss:  1.816, Seconds: 5.41\n",
      "Epoch   6/30 Batch 1070/1978 - Loss:  1.735, Seconds: 5.26\n",
      "Epoch   6/30 Batch 1080/1978 - Loss:  1.765, Seconds: 5.27\n",
      "Epoch   6/30 Batch 1090/1978 - Loss:  1.709, Seconds: 5.18\n",
      "Epoch   6/30 Batch 1100/1978 - Loss:  1.764, Seconds: 5.32\n",
      "Epoch   6/30 Batch 1110/1978 - Loss:  1.793, Seconds: 5.32\n",
      "Epoch   6/30 Batch 1120/1978 - Loss:  1.791, Seconds: 5.37\n",
      "Epoch   6/30 Batch 1130/1978 - Loss:  1.705, Seconds: 5.19\n",
      "Epoch   6/30 Batch 1140/1978 - Loss:  1.686, Seconds: 5.52\n",
      "Epoch   6/30 Batch 1150/1978 - Loss:  1.737, Seconds: 5.55\n",
      "Epoch   6/30 Batch 1160/1978 - Loss:  1.649, Seconds: 5.56\n",
      "Epoch   6/30 Batch 1170/1978 - Loss:  1.749, Seconds: 5.27\n",
      "Epoch   6/30 Batch 1180/1978 - Loss:  1.772, Seconds: 5.27\n",
      "Epoch   6/30 Batch 1190/1978 - Loss:  1.770, Seconds: 5.44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   6/30 Batch 1200/1978 - Loss:  1.686, Seconds: 5.46\n",
      "Epoch   6/30 Batch 1210/1978 - Loss:  1.788, Seconds: 5.49\n",
      "Epoch   6/30 Batch 1220/1978 - Loss:  1.778, Seconds: 5.67\n",
      "Epoch   6/30 Batch 1230/1978 - Loss:  1.745, Seconds: 5.21\n",
      "Epoch   6/30 Batch 1240/1978 - Loss:  1.739, Seconds: 5.61\n",
      "Epoch   6/30 Batch 1250/1978 - Loss:  1.757, Seconds: 5.59\n",
      "Epoch   6/30 Batch 1260/1978 - Loss:  1.855, Seconds: 5.60\n",
      "Epoch   6/30 Batch 1270/1978 - Loss:  1.837, Seconds: 5.59\n",
      "Epoch   6/30 Batch 1280/1978 - Loss:  1.853, Seconds: 5.83\n",
      "Epoch   6/30 Batch 1290/1978 - Loss:  1.945, Seconds: 5.66\n",
      "Epoch   6/30 Batch 1300/1978 - Loss:  1.841, Seconds: 5.72\n",
      "Epoch   6/30 Batch 1310/1978 - Loss:  1.952, Seconds: 5.85\n",
      "Average loss for this update: 1.634\n",
      "No Improvement.\n",
      "Epoch   6/30 Batch 1320/1978 - Loss:  1.963, Seconds: 5.58\n",
      "Epoch   6/30 Batch 1330/1978 - Loss:  2.049, Seconds: 5.55\n",
      "Epoch   6/30 Batch 1340/1978 - Loss:  2.023, Seconds: 5.44\n",
      "Epoch   6/30 Batch 1350/1978 - Loss:  2.034, Seconds: 5.42\n",
      "Epoch   6/30 Batch 1360/1978 - Loss:  2.111, Seconds: 5.31\n",
      "Epoch   6/30 Batch 1370/1978 - Loss:  2.094, Seconds: 5.51\n",
      "Epoch   6/30 Batch 1380/1978 - Loss:  2.152, Seconds: 6.01\n",
      "Epoch   6/30 Batch 1390/1978 - Loss:  2.078, Seconds: 5.85\n",
      "Epoch   6/30 Batch 1400/1978 - Loss:  2.148, Seconds: 5.59\n",
      "Epoch   6/30 Batch 1410/1978 - Loss:  2.103, Seconds: 5.75\n",
      "Epoch   6/30 Batch 1420/1978 - Loss:  2.103, Seconds: 5.91\n",
      "Epoch   6/30 Batch 1430/1978 - Loss:  2.255, Seconds: 5.60\n",
      "Epoch   6/30 Batch 1440/1978 - Loss:  2.141, Seconds: 5.80\n",
      "Epoch   6/30 Batch 1450/1978 - Loss:  2.185, Seconds: 5.65\n",
      "Epoch   6/30 Batch 1460/1978 - Loss:  2.217, Seconds: 5.97\n",
      "Epoch   6/30 Batch 1470/1978 - Loss:  2.168, Seconds: 5.72\n",
      "Epoch   6/30 Batch 1480/1978 - Loss:  2.159, Seconds: 5.73\n",
      "Epoch   6/30 Batch 1490/1978 - Loss:  2.259, Seconds: 6.01\n",
      "Epoch   6/30 Batch 1500/1978 - Loss:  2.192, Seconds: 5.75\n",
      "Epoch   6/30 Batch 1510/1978 - Loss:  2.204, Seconds: 5.91\n",
      "Epoch   6/30 Batch 1520/1978 - Loss:  2.213, Seconds: 5.97\n",
      "Epoch   6/30 Batch 1530/1978 - Loss:  2.135, Seconds: 6.25\n",
      "Epoch   6/30 Batch 1540/1978 - Loss:  2.194, Seconds: 6.14\n",
      "Epoch   6/30 Batch 1550/1978 - Loss:  2.301, Seconds: 6.16\n",
      "Epoch   6/30 Batch 1560/1978 - Loss:  2.294, Seconds: 6.33\n",
      "Epoch   6/30 Batch 1570/1978 - Loss:  2.187, Seconds: 5.86\n",
      "Epoch   6/30 Batch 1580/1978 - Loss:  2.196, Seconds: 6.05\n",
      "Epoch   6/30 Batch 1590/1978 - Loss:  2.195, Seconds: 6.23\n",
      "Epoch   6/30 Batch 1600/1978 - Loss:  2.278, Seconds: 5.93\n",
      "Epoch   6/30 Batch 1610/1978 - Loss:  2.201, Seconds: 6.15\n",
      "Epoch   6/30 Batch 1620/1978 - Loss:  2.232, Seconds: 5.98\n",
      "Epoch   6/30 Batch 1630/1978 - Loss:  2.189, Seconds: 6.50\n",
      "Epoch   6/30 Batch 1640/1978 - Loss:  2.246, Seconds: 6.37\n",
      "Epoch   6/30 Batch 1650/1978 - Loss:  2.206, Seconds: 6.19\n",
      "Epoch   6/30 Batch 1660/1978 - Loss:  2.253, Seconds: 6.55\n",
      "Epoch   6/30 Batch 1670/1978 - Loss:  2.236, Seconds: 6.25\n",
      "Epoch   6/30 Batch 1680/1978 - Loss:  2.328, Seconds: 6.27\n",
      "Epoch   6/30 Batch 1690/1978 - Loss:  2.385, Seconds: 6.11\n",
      "Epoch   6/30 Batch 1700/1978 - Loss:  2.320, Seconds: 6.33\n",
      "Epoch   6/30 Batch 1710/1978 - Loss:  2.284, Seconds: 6.48\n",
      "Epoch   6/30 Batch 1720/1978 - Loss:  2.322, Seconds: 6.69\n",
      "Epoch   6/30 Batch 1730/1978 - Loss:  2.245, Seconds: 6.70\n",
      "Epoch   6/30 Batch 1740/1978 - Loss:  2.348, Seconds: 6.55\n",
      "Epoch   6/30 Batch 1750/1978 - Loss:  2.305, Seconds: 6.58\n",
      "Epoch   6/30 Batch 1760/1978 - Loss:  2.354, Seconds: 6.10\n",
      "Epoch   6/30 Batch 1770/1978 - Loss:  2.265, Seconds: 6.80\n",
      "Epoch   6/30 Batch 1780/1978 - Loss:  2.214, Seconds: 6.82\n",
      "Epoch   6/30 Batch 1790/1978 - Loss:  2.265, Seconds: 6.85\n",
      "Epoch   6/30 Batch 1800/1978 - Loss:  2.326, Seconds: 6.67\n",
      "Epoch   6/30 Batch 1810/1978 - Loss:  2.359, Seconds: 6.69\n",
      "Epoch   6/30 Batch 1820/1978 - Loss:  2.269, Seconds: 6.39\n",
      "Epoch   6/30 Batch 1830/1978 - Loss:  2.320, Seconds: 6.73\n",
      "Epoch   6/30 Batch 1840/1978 - Loss:  2.262, Seconds: 6.60\n",
      "Epoch   6/30 Batch 1850/1978 - Loss:  2.408, Seconds: 6.48\n",
      "Epoch   6/30 Batch 1860/1978 - Loss:  2.263, Seconds: 6.49\n",
      "Epoch   6/30 Batch 1870/1978 - Loss:  2.425, Seconds: 6.65\n",
      "Epoch   6/30 Batch 1880/1978 - Loss:  2.481, Seconds: 6.70\n",
      "Epoch   6/30 Batch 1890/1978 - Loss:  2.432, Seconds: 6.73\n",
      "Epoch   6/30 Batch 1900/1978 - Loss:  2.358, Seconds: 6.55\n",
      "Epoch   6/30 Batch 1910/1978 - Loss:  2.265, Seconds: 7.12\n",
      "Epoch   6/30 Batch 1920/1978 - Loss:  2.345, Seconds: 6.97\n",
      "Epoch   6/30 Batch 1930/1978 - Loss:  2.360, Seconds: 6.98\n",
      "Epoch   6/30 Batch 1940/1978 - Loss:  2.336, Seconds: 6.84\n",
      "Epoch   6/30 Batch 1950/1978 - Loss:  2.370, Seconds: 7.02\n",
      "Epoch   6/30 Batch 1960/1978 - Loss:  2.379, Seconds: 7.06\n",
      "Epoch   6/30 Batch 1970/1978 - Loss:  2.370, Seconds: 7.07\n",
      "Average loss for this update: 2.249\n",
      "No Improvement.\n",
      "Epoch   7/30 Batch   10/1978 - Loss:  2.002, Seconds: 2.29\n",
      "Epoch   7/30 Batch   20/1978 - Loss:  1.732, Seconds: 2.07\n",
      "Epoch   7/30 Batch   30/1978 - Loss:  1.810, Seconds: 2.24\n",
      "Epoch   7/30 Batch   40/1978 - Loss:  1.568, Seconds: 2.38\n",
      "Epoch   7/30 Batch   50/1978 - Loss:  1.653, Seconds: 2.53\n",
      "Epoch   7/30 Batch   60/1978 - Loss:  1.636, Seconds: 2.27\n",
      "Epoch   7/30 Batch   70/1978 - Loss:  1.689, Seconds: 2.22\n",
      "Epoch   7/30 Batch   80/1978 - Loss:  1.483, Seconds: 2.74\n",
      "Epoch   7/30 Batch   90/1978 - Loss:  1.508, Seconds: 2.69\n",
      "Epoch   7/30 Batch  100/1978 - Loss:  1.401, Seconds: 2.54\n",
      "Epoch   7/30 Batch  110/1978 - Loss:  1.609, Seconds: 2.81\n",
      "Epoch   7/30 Batch  120/1978 - Loss:  1.619, Seconds: 2.63\n",
      "Epoch   7/30 Batch  130/1978 - Loss:  1.497, Seconds: 2.92\n",
      "Epoch   7/30 Batch  140/1978 - Loss:  1.691, Seconds: 2.84\n",
      "Epoch   7/30 Batch  150/1978 - Loss:  1.649, Seconds: 3.08\n",
      "Epoch   7/30 Batch  160/1978 - Loss:  1.662, Seconds: 3.24\n",
      "Epoch   7/30 Batch  170/1978 - Loss:  1.762, Seconds: 2.79\n",
      "Epoch   7/30 Batch  180/1978 - Loss:  1.762, Seconds: 2.70\n",
      "Epoch   7/30 Batch  190/1978 - Loss:  1.785, Seconds: 2.84\n",
      "Epoch   7/30 Batch  200/1978 - Loss:  1.744, Seconds: 3.00\n",
      "Epoch   7/30 Batch  210/1978 - Loss:  1.630, Seconds: 3.03\n",
      "Epoch   7/30 Batch  220/1978 - Loss:  1.500, Seconds: 3.08\n",
      "Epoch   7/30 Batch  230/1978 - Loss:  1.536, Seconds: 3.50\n",
      "Epoch   7/30 Batch  240/1978 - Loss:  1.434, Seconds: 3.02\n",
      "Epoch   7/30 Batch  250/1978 - Loss:  1.454, Seconds: 3.56\n",
      "Epoch   7/30 Batch  260/1978 - Loss:  1.429, Seconds: 3.61\n",
      "Epoch   7/30 Batch  270/1978 - Loss:  1.470, Seconds: 2.98\n",
      "Epoch   7/30 Batch  280/1978 - Loss:  1.519, Seconds: 3.28\n",
      "Epoch   7/30 Batch  290/1978 - Loss:  1.531, Seconds: 3.43\n",
      "Epoch   7/30 Batch  300/1978 - Loss:  1.443, Seconds: 3.60\n",
      "Epoch   7/30 Batch  310/1978 - Loss:  1.385, Seconds: 3.79\n",
      "Epoch   7/30 Batch  320/1978 - Loss:  1.439, Seconds: 3.80\n",
      "Epoch   7/30 Batch  330/1978 - Loss:  1.419, Seconds: 3.72\n",
      "Epoch   7/30 Batch  340/1978 - Loss:  1.405, Seconds: 3.88\n",
      "Epoch   7/30 Batch  350/1978 - Loss:  1.459, Seconds: 3.90\n",
      "Epoch   7/30 Batch  360/1978 - Loss:  1.355, Seconds: 3.40\n",
      "Epoch   7/30 Batch  370/1978 - Loss:  1.425, Seconds: 3.46\n",
      "Epoch   7/30 Batch  380/1978 - Loss:  1.187, Seconds: 4.00\n",
      "Epoch   7/30 Batch  390/1978 - Loss:  1.314, Seconds: 3.49\n",
      "Epoch   7/30 Batch  400/1978 - Loss:  1.277, Seconds: 4.05\n",
      "Epoch   7/30 Batch  410/1978 - Loss:  1.524, Seconds: 3.92\n",
      "Epoch   7/30 Batch  420/1978 - Loss:  1.516, Seconds: 4.10\n",
      "Epoch   7/30 Batch  430/1978 - Loss:  1.528, Seconds: 4.01\n",
      "Epoch   7/30 Batch  440/1978 - Loss:  1.460, Seconds: 3.73\n",
      "Epoch   7/30 Batch  450/1978 - Loss:  1.454, Seconds: 4.19\n",
      "Epoch   7/30 Batch  460/1978 - Loss:  1.371, Seconds: 3.96\n",
      "Epoch   7/30 Batch  470/1978 - Loss:  1.381, Seconds: 4.22\n",
      "Epoch   7/30 Batch  480/1978 - Loss:  1.436, Seconds: 4.07\n",
      "Epoch   7/30 Batch  490/1978 - Loss:  1.359, Seconds: 4.13\n",
      "Epoch   7/30 Batch  500/1978 - Loss:  1.517, Seconds: 4.18\n",
      "Epoch   7/30 Batch  510/1978 - Loss:  1.517, Seconds: 4.35\n",
      "Epoch   7/30 Batch  520/1978 - Loss:  1.427, Seconds: 3.94\n",
      "Epoch   7/30 Batch  530/1978 - Loss:  1.377, Seconds: 4.39\n",
      "Epoch   7/30 Batch  540/1978 - Loss:  1.350, Seconds: 3.71\n",
      "Epoch   7/30 Batch  550/1978 - Loss:  1.358, Seconds: 4.43\n",
      "Epoch   7/30 Batch  560/1978 - Loss:  1.395, Seconds: 4.17\n",
      "Epoch   7/30 Batch  570/1978 - Loss:  1.375, Seconds: 4.32\n",
      "Epoch   7/30 Batch  580/1978 - Loss:  1.357, Seconds: 4.37\n",
      "Epoch   7/30 Batch  590/1978 - Loss:  1.335, Seconds: 4.39\n",
      "Epoch   7/30 Batch  600/1978 - Loss:  1.342, Seconds: 4.39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   7/30 Batch  610/1978 - Loss:  1.272, Seconds: 4.56\n",
      "Epoch   7/30 Batch  620/1978 - Loss:  1.341, Seconds: 4.55\n",
      "Epoch   7/30 Batch  630/1978 - Loss:  1.478, Seconds: 4.60\n",
      "Epoch   7/30 Batch  640/1978 - Loss:  1.434, Seconds: 4.46\n",
      "Epoch   7/30 Batch  650/1978 - Loss:  1.334, Seconds: 4.60\n",
      "Average loss for this update: 1.489\n",
      "New Record!\n",
      "Epoch   7/30 Batch  660/1978 - Loss:  1.060, Seconds: 4.62\n",
      "Epoch   7/30 Batch  670/1978 - Loss:  1.040, Seconds: 4.66\n",
      "Epoch   7/30 Batch  680/1978 - Loss:  1.078, Seconds: 4.52\n",
      "Epoch   7/30 Batch  690/1978 - Loss:  1.071, Seconds: 4.70\n",
      "Epoch   7/30 Batch  700/1978 - Loss:  1.220, Seconds: 4.39\n",
      "Epoch   7/30 Batch  710/1978 - Loss:  1.087, Seconds: 4.74\n",
      "Epoch   7/30 Batch  720/1978 - Loss:  1.186, Seconds: 4.45\n",
      "Epoch   7/30 Batch  730/1978 - Loss:  1.194, Seconds: 4.63\n",
      "Epoch   7/30 Batch  740/1978 - Loss:  1.345, Seconds: 4.79\n",
      "Epoch   7/30 Batch  750/1978 - Loss:  1.383, Seconds: 4.81\n",
      "Epoch   7/30 Batch  760/1978 - Loss:  1.528, Seconds: 4.65\n",
      "Epoch   7/30 Batch  770/1978 - Loss:  1.471, Seconds: 4.84\n",
      "Epoch   7/30 Batch  780/1978 - Loss:  1.393, Seconds: 4.84\n",
      "Epoch   7/30 Batch  790/1978 - Loss:  1.484, Seconds: 4.87\n",
      "Epoch   7/30 Batch  800/1978 - Loss:  1.470, Seconds: 4.88\n",
      "Epoch   7/30 Batch  810/1978 - Loss:  1.525, Seconds: 4.79\n",
      "Epoch   7/30 Batch  820/1978 - Loss:  1.474, Seconds: 4.92\n",
      "Epoch   7/30 Batch  830/1978 - Loss:  1.503, Seconds: 4.96\n",
      "Epoch   7/30 Batch  840/1978 - Loss:  1.530, Seconds: 4.98\n",
      "Epoch   7/30 Batch  850/1978 - Loss:  1.542, Seconds: 4.98\n",
      "Epoch   7/30 Batch  860/1978 - Loss:  1.556, Seconds: 5.02\n",
      "Epoch   7/30 Batch  870/1978 - Loss:  1.534, Seconds: 5.02\n",
      "Epoch   7/30 Batch  880/1978 - Loss:  1.568, Seconds: 5.03\n",
      "Epoch   7/30 Batch  890/1978 - Loss:  1.555, Seconds: 5.07\n",
      "Epoch   7/30 Batch  900/1978 - Loss:  1.589, Seconds: 4.95\n",
      "Epoch   7/30 Batch  910/1978 - Loss:  1.584, Seconds: 4.81\n",
      "Epoch   7/30 Batch  920/1978 - Loss:  1.620, Seconds: 4.97\n",
      "Epoch   7/30 Batch  930/1978 - Loss:  1.574, Seconds: 5.01\n",
      "Epoch   7/30 Batch  940/1978 - Loss:  1.780, Seconds: 4.91\n",
      "Epoch   7/30 Batch  950/1978 - Loss:  1.744, Seconds: 4.87\n",
      "Epoch   7/30 Batch  960/1978 - Loss:  1.733, Seconds: 5.11\n",
      "Epoch   7/30 Batch  970/1978 - Loss:  1.684, Seconds: 4.61\n",
      "Epoch   7/30 Batch  980/1978 - Loss:  1.710, Seconds: 5.10\n",
      "Epoch   7/30 Batch  990/1978 - Loss:  1.640, Seconds: 5.28\n",
      "Epoch   7/30 Batch 1000/1978 - Loss:  1.774, Seconds: 5.10\n",
      "Epoch   7/30 Batch 1010/1978 - Loss:  1.643, Seconds: 5.30\n",
      "Epoch   7/30 Batch 1020/1978 - Loss:  1.794, Seconds: 5.16\n",
      "Epoch   7/30 Batch 1030/1978 - Loss:  1.764, Seconds: 5.33\n",
      "Epoch   7/30 Batch 1040/1978 - Loss:  1.804, Seconds: 5.33\n",
      "Epoch   7/30 Batch 1050/1978 - Loss:  1.827, Seconds: 5.22\n",
      "Epoch   7/30 Batch 1060/1978 - Loss:  1.785, Seconds: 5.38\n",
      "Epoch   7/30 Batch 1070/1978 - Loss:  1.696, Seconds: 5.25\n",
      "Epoch   7/30 Batch 1080/1978 - Loss:  1.724, Seconds: 5.26\n",
      "Epoch   7/30 Batch 1090/1978 - Loss:  1.672, Seconds: 5.11\n",
      "Epoch   7/30 Batch 1100/1978 - Loss:  1.719, Seconds: 5.30\n",
      "Epoch   7/30 Batch 1110/1978 - Loss:  1.782, Seconds: 5.32\n",
      "Epoch   7/30 Batch 1120/1978 - Loss:  1.747, Seconds: 5.34\n",
      "Epoch   7/30 Batch 1130/1978 - Loss:  1.662, Seconds: 5.20\n",
      "Epoch   7/30 Batch 1140/1978 - Loss:  1.644, Seconds: 5.55\n",
      "Epoch   7/30 Batch 1150/1978 - Loss:  1.696, Seconds: 5.56\n",
      "Epoch   7/30 Batch 1160/1978 - Loss:  1.643, Seconds: 5.59\n",
      "Epoch   7/30 Batch 1170/1978 - Loss:  1.718, Seconds: 5.27\n",
      "Epoch   7/30 Batch 1180/1978 - Loss:  1.734, Seconds: 5.27\n",
      "Epoch   7/30 Batch 1190/1978 - Loss:  1.734, Seconds: 5.45\n",
      "Epoch   7/30 Batch 1200/1978 - Loss:  1.645, Seconds: 5.47\n",
      "Epoch   7/30 Batch 1210/1978 - Loss:  1.779, Seconds: 5.49\n",
      "Epoch   7/30 Batch 1220/1978 - Loss:  1.750, Seconds: 5.69\n",
      "Epoch   7/30 Batch 1230/1978 - Loss:  1.728, Seconds: 5.21\n",
      "Epoch   7/30 Batch 1240/1978 - Loss:  1.722, Seconds: 5.56\n",
      "Epoch   7/30 Batch 1250/1978 - Loss:  1.722, Seconds: 5.59\n",
      "Epoch   7/30 Batch 1260/1978 - Loss:  1.812, Seconds: 5.61\n",
      "Epoch   7/30 Batch 1270/1978 - Loss:  1.791, Seconds: 5.59\n",
      "Epoch   7/30 Batch 1280/1978 - Loss:  1.820, Seconds: 5.76\n",
      "Epoch   7/30 Batch 1290/1978 - Loss:  1.887, Seconds: 5.65\n",
      "Epoch   7/30 Batch 1300/1978 - Loss:  1.819, Seconds: 5.67\n",
      "Epoch   7/30 Batch 1310/1978 - Loss:  1.911, Seconds: 5.82\n",
      "Average loss for this update: 1.606\n",
      "No Improvement.\n",
      "Epoch   7/30 Batch 1320/1978 - Loss:  1.920, Seconds: 5.54\n",
      "Epoch   7/30 Batch 1330/1978 - Loss:  2.014, Seconds: 5.61\n",
      "Epoch   7/30 Batch 1340/1978 - Loss:  1.966, Seconds: 5.42\n",
      "Epoch   7/30 Batch 1350/1978 - Loss:  1.988, Seconds: 5.48\n",
      "Epoch   7/30 Batch 1360/1978 - Loss:  2.054, Seconds: 5.32\n",
      "Epoch   7/30 Batch 1370/1978 - Loss:  2.050, Seconds: 5.48\n",
      "Epoch   7/30 Batch 1380/1978 - Loss:  2.113, Seconds: 5.97\n",
      "Epoch   7/30 Batch 1390/1978 - Loss:  2.050, Seconds: 5.85\n",
      "Epoch   7/30 Batch 1400/1978 - Loss:  2.114, Seconds: 5.55\n",
      "Epoch   7/30 Batch 1410/1978 - Loss:  2.053, Seconds: 5.73\n",
      "Epoch   7/30 Batch 1420/1978 - Loss:  2.079, Seconds: 5.92\n",
      "Epoch   7/30 Batch 1430/1978 - Loss:  2.210, Seconds: 5.59\n",
      "Epoch   7/30 Batch 1440/1978 - Loss:  2.098, Seconds: 5.79\n",
      "Epoch   7/30 Batch 1450/1978 - Loss:  2.144, Seconds: 5.66\n",
      "Epoch   7/30 Batch 1460/1978 - Loss:  2.162, Seconds: 6.00\n",
      "Epoch   7/30 Batch 1470/1978 - Loss:  2.142, Seconds: 5.68\n",
      "Epoch   7/30 Batch 1480/1978 - Loss:  2.114, Seconds: 5.70\n",
      "Epoch   7/30 Batch 1490/1978 - Loss:  2.218, Seconds: 6.04\n",
      "Epoch   7/30 Batch 1500/1978 - Loss:  2.151, Seconds: 5.71\n",
      "Epoch   7/30 Batch 1510/1978 - Loss:  2.170, Seconds: 5.96\n",
      "Epoch   7/30 Batch 1520/1978 - Loss:  2.157, Seconds: 5.92\n",
      "Epoch   7/30 Batch 1530/1978 - Loss:  2.097, Seconds: 6.28\n",
      "Epoch   7/30 Batch 1540/1978 - Loss:  2.132, Seconds: 6.14\n",
      "Epoch   7/30 Batch 1550/1978 - Loss:  2.238, Seconds: 6.15\n",
      "Epoch   7/30 Batch 1560/1978 - Loss:  2.238, Seconds: 6.36\n",
      "Epoch   7/30 Batch 1570/1978 - Loss:  2.142, Seconds: 5.87\n",
      "Epoch   7/30 Batch 1580/1978 - Loss:  2.166, Seconds: 6.04\n",
      "Epoch   7/30 Batch 1590/1978 - Loss:  2.155, Seconds: 6.25\n",
      "Epoch   7/30 Batch 1600/1978 - Loss:  2.235, Seconds: 5.94\n",
      "Epoch   7/30 Batch 1610/1978 - Loss:  2.158, Seconds: 6.15\n",
      "Epoch   7/30 Batch 1620/1978 - Loss:  2.193, Seconds: 5.95\n",
      "Epoch   7/30 Batch 1630/1978 - Loss:  2.168, Seconds: 6.52\n",
      "Epoch   7/30 Batch 1640/1978 - Loss:  2.193, Seconds: 6.35\n",
      "Epoch   7/30 Batch 1650/1978 - Loss:  2.171, Seconds: 6.22\n",
      "Epoch   7/30 Batch 1660/1978 - Loss:  2.224, Seconds: 6.54\n",
      "Epoch   7/30 Batch 1670/1978 - Loss:  2.213, Seconds: 6.23\n",
      "Epoch   7/30 Batch 1680/1978 - Loss:  2.298, Seconds: 6.27\n",
      "Epoch   7/30 Batch 1690/1978 - Loss:  2.356, Seconds: 6.11\n",
      "Epoch   7/30 Batch 1700/1978 - Loss:  2.298, Seconds: 6.33\n",
      "Epoch   7/30 Batch 1710/1978 - Loss:  2.256, Seconds: 6.52\n",
      "Epoch   7/30 Batch 1720/1978 - Loss:  2.291, Seconds: 6.67\n",
      "Epoch   7/30 Batch 1730/1978 - Loss:  2.175, Seconds: 6.71\n",
      "Epoch   7/30 Batch 1740/1978 - Loss:  2.311, Seconds: 6.56\n",
      "Epoch   7/30 Batch 1750/1978 - Loss:  2.272, Seconds: 6.59\n",
      "Epoch   7/30 Batch 1760/1978 - Loss:  2.314, Seconds: 6.07\n",
      "Epoch   7/30 Batch 1770/1978 - Loss:  2.201, Seconds: 6.81\n",
      "Epoch   7/30 Batch 1780/1978 - Loss:  2.159, Seconds: 6.81\n",
      "Epoch   7/30 Batch 1790/1978 - Loss:  2.229, Seconds: 6.82\n",
      "Epoch   7/30 Batch 1800/1978 - Loss:  2.275, Seconds: 6.69\n",
      "Epoch   7/30 Batch 1810/1978 - Loss:  2.316, Seconds: 6.72\n",
      "Epoch   7/30 Batch 1820/1978 - Loss:  2.242, Seconds: 6.37\n",
      "Epoch   7/30 Batch 1830/1978 - Loss:  2.290, Seconds: 6.74\n",
      "Epoch   7/30 Batch 1840/1978 - Loss:  2.202, Seconds: 6.63\n",
      "Epoch   7/30 Batch 1850/1978 - Loss:  2.376, Seconds: 6.45\n",
      "Epoch   7/30 Batch 1860/1978 - Loss:  2.207, Seconds: 6.47\n",
      "Epoch   7/30 Batch 1870/1978 - Loss:  2.388, Seconds: 6.64\n",
      "Epoch   7/30 Batch 1880/1978 - Loss:  2.432, Seconds: 6.69\n",
      "Epoch   7/30 Batch 1890/1978 - Loss:  2.385, Seconds: 6.70\n",
      "Epoch   7/30 Batch 1900/1978 - Loss:  2.319, Seconds: 6.57\n",
      "Epoch   7/30 Batch 1910/1978 - Loss:  2.230, Seconds: 7.11\n",
      "Epoch   7/30 Batch 1920/1978 - Loss:  2.305, Seconds: 6.95\n",
      "Epoch   7/30 Batch 1930/1978 - Loss:  2.354, Seconds: 7.00\n",
      "Epoch   7/30 Batch 1940/1978 - Loss:  2.313, Seconds: 6.83\n",
      "Epoch   7/30 Batch 1950/1978 - Loss:  2.325, Seconds: 7.00\n",
      "Epoch   7/30 Batch 1960/1978 - Loss:  2.343, Seconds: 7.11\n",
      "Epoch   7/30 Batch 1970/1978 - Loss:  2.338, Seconds: 7.08\n",
      "Average loss for this update: 2.209\n",
      "No Improvement.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   8/30 Batch   10/1978 - Loss:  1.890, Seconds: 2.29\n",
      "Epoch   8/30 Batch   20/1978 - Loss:  1.683, Seconds: 2.08\n",
      "Epoch   8/30 Batch   30/1978 - Loss:  1.751, Seconds: 2.22\n",
      "Epoch   8/30 Batch   40/1978 - Loss:  1.545, Seconds: 2.39\n",
      "Epoch   8/30 Batch   50/1978 - Loss:  1.611, Seconds: 2.67\n",
      "Epoch   8/30 Batch   60/1978 - Loss:  1.592, Seconds: 2.16\n",
      "Epoch   8/30 Batch   70/1978 - Loss:  1.637, Seconds: 2.45\n",
      "Epoch   8/30 Batch   80/1978 - Loss:  1.465, Seconds: 2.73\n",
      "Epoch   8/30 Batch   90/1978 - Loss:  1.474, Seconds: 2.71\n",
      "Epoch   8/30 Batch  100/1978 - Loss:  1.362, Seconds: 2.51\n",
      "Epoch   8/30 Batch  110/1978 - Loss:  1.544, Seconds: 2.83\n",
      "Epoch   8/30 Batch  120/1978 - Loss:  1.581, Seconds: 2.76\n",
      "Epoch   8/30 Batch  130/1978 - Loss:  1.483, Seconds: 2.76\n",
      "Epoch   8/30 Batch  140/1978 - Loss:  1.671, Seconds: 2.79\n",
      "Epoch   8/30 Batch  150/1978 - Loss:  1.615, Seconds: 3.14\n",
      "Epoch   8/30 Batch  160/1978 - Loss:  1.634, Seconds: 3.25\n",
      "Epoch   8/30 Batch  170/1978 - Loss:  1.719, Seconds: 2.92\n",
      "Epoch   8/30 Batch  180/1978 - Loss:  1.720, Seconds: 2.86\n",
      "Epoch   8/30 Batch  190/1978 - Loss:  1.767, Seconds: 3.05\n",
      "Epoch   8/30 Batch  200/1978 - Loss:  1.714, Seconds: 2.96\n",
      "Epoch   8/30 Batch  210/1978 - Loss:  1.600, Seconds: 3.04\n",
      "Epoch   8/30 Batch  220/1978 - Loss:  1.476, Seconds: 3.11\n",
      "Epoch   8/30 Batch  230/1978 - Loss:  1.505, Seconds: 3.50\n",
      "Epoch   8/30 Batch  240/1978 - Loss:  1.416, Seconds: 3.07\n",
      "Epoch   8/30 Batch  250/1978 - Loss:  1.426, Seconds: 3.56\n",
      "Epoch   8/30 Batch  260/1978 - Loss:  1.384, Seconds: 3.66\n",
      "Epoch   8/30 Batch  270/1978 - Loss:  1.438, Seconds: 2.98\n",
      "Epoch   8/30 Batch  280/1978 - Loss:  1.484, Seconds: 3.37\n",
      "Epoch   8/30 Batch  290/1978 - Loss:  1.492, Seconds: 3.45\n",
      "Epoch   8/30 Batch  300/1978 - Loss:  1.423, Seconds: 3.62\n",
      "Epoch   8/30 Batch  310/1978 - Loss:  1.369, Seconds: 3.77\n",
      "Epoch   8/30 Batch  320/1978 - Loss:  1.407, Seconds: 3.80\n",
      "Epoch   8/30 Batch  330/1978 - Loss:  1.395, Seconds: 3.72\n",
      "Epoch   8/30 Batch  340/1978 - Loss:  1.366, Seconds: 3.88\n",
      "Epoch   8/30 Batch  350/1978 - Loss:  1.436, Seconds: 3.92\n",
      "Epoch   8/30 Batch  360/1978 - Loss:  1.306, Seconds: 3.48\n",
      "Epoch   8/30 Batch  370/1978 - Loss:  1.386, Seconds: 3.50\n",
      "Epoch   8/30 Batch  380/1978 - Loss:  1.182, Seconds: 4.00\n",
      "Epoch   8/30 Batch  390/1978 - Loss:  1.289, Seconds: 3.48\n",
      "Epoch   8/30 Batch  400/1978 - Loss:  1.242, Seconds: 4.60\n",
      "Epoch   8/30 Batch  410/1978 - Loss:  1.490, Seconds: 3.95\n",
      "Epoch   8/30 Batch  420/1978 - Loss:  1.476, Seconds: 4.10\n",
      "Epoch   8/30 Batch  430/1978 - Loss:  1.502, Seconds: 4.00\n",
      "Epoch   8/30 Batch  440/1978 - Loss:  1.441, Seconds: 3.76\n",
      "Epoch   8/30 Batch  450/1978 - Loss:  1.435, Seconds: 4.21\n",
      "Epoch   8/30 Batch  460/1978 - Loss:  1.328, Seconds: 4.04\n",
      "Epoch   8/30 Batch  470/1978 - Loss:  1.347, Seconds: 4.25\n",
      "Epoch   8/30 Batch  480/1978 - Loss:  1.419, Seconds: 3.81\n",
      "Epoch   8/30 Batch  490/1978 - Loss:  1.316, Seconds: 4.17\n",
      "Epoch   8/30 Batch  500/1978 - Loss:  1.486, Seconds: 4.19\n",
      "Epoch   8/30 Batch  510/1978 - Loss:  1.486, Seconds: 4.32\n",
      "Epoch   8/30 Batch  520/1978 - Loss:  1.383, Seconds: 4.04\n",
      "Epoch   8/30 Batch  530/1978 - Loss:  1.339, Seconds: 4.40\n",
      "Epoch   8/30 Batch  540/1978 - Loss:  1.336, Seconds: 3.72\n",
      "Epoch   8/30 Batch  550/1978 - Loss:  1.331, Seconds: 4.46\n",
      "Epoch   8/30 Batch  560/1978 - Loss:  1.373, Seconds: 4.17\n",
      "Epoch   8/30 Batch  570/1978 - Loss:  1.345, Seconds: 4.34\n",
      "Epoch   8/30 Batch  580/1978 - Loss:  1.334, Seconds: 4.37\n",
      "Epoch   8/30 Batch  590/1978 - Loss:  1.311, Seconds: 4.37\n",
      "Epoch   8/30 Batch  600/1978 - Loss:  1.311, Seconds: 4.39\n",
      "Epoch   8/30 Batch  610/1978 - Loss:  1.268, Seconds: 4.56\n",
      "Epoch   8/30 Batch  620/1978 - Loss:  1.331, Seconds: 4.56\n",
      "Epoch   8/30 Batch  630/1978 - Loss:  1.467, Seconds: 4.62\n",
      "Epoch   8/30 Batch  640/1978 - Loss:  1.399, Seconds: 4.48\n",
      "Epoch   8/30 Batch  650/1978 - Loss:  1.311, Seconds: 4.60\n",
      "Average loss for this update: 1.457\n",
      "New Record!\n",
      "Epoch   8/30 Batch  660/1978 - Loss:  1.043, Seconds: 4.64\n",
      "Epoch   8/30 Batch  670/1978 - Loss:  1.032, Seconds: 4.68\n",
      "Epoch   8/30 Batch  680/1978 - Loss:  1.045, Seconds: 4.53\n",
      "Epoch   8/30 Batch  690/1978 - Loss:  1.074, Seconds: 4.66\n",
      "Epoch   8/30 Batch  700/1978 - Loss:  1.198, Seconds: 4.41\n",
      "Epoch   8/30 Batch  710/1978 - Loss:  1.062, Seconds: 4.73\n",
      "Epoch   8/30 Batch  720/1978 - Loss:  1.165, Seconds: 4.45\n",
      "Epoch   8/30 Batch  730/1978 - Loss:  1.180, Seconds: 4.60\n",
      "Epoch   8/30 Batch  740/1978 - Loss:  1.335, Seconds: 4.76\n",
      "Epoch   8/30 Batch  750/1978 - Loss:  1.352, Seconds: 4.82\n",
      "Epoch   8/30 Batch  760/1978 - Loss:  1.502, Seconds: 4.65\n",
      "Epoch   8/30 Batch  770/1978 - Loss:  1.469, Seconds: 4.82\n",
      "Epoch   8/30 Batch  780/1978 - Loss:  1.352, Seconds: 4.86\n",
      "Epoch   8/30 Batch  790/1978 - Loss:  1.451, Seconds: 4.86\n",
      "Epoch   8/30 Batch  800/1978 - Loss:  1.440, Seconds: 4.89\n",
      "Epoch   8/30 Batch  810/1978 - Loss:  1.518, Seconds: 4.76\n",
      "Epoch   8/30 Batch  820/1978 - Loss:  1.450, Seconds: 4.95\n",
      "Epoch   8/30 Batch  830/1978 - Loss:  1.489, Seconds: 4.95\n",
      "Epoch   8/30 Batch  840/1978 - Loss:  1.504, Seconds: 4.95\n",
      "Epoch   8/30 Batch  850/1978 - Loss:  1.504, Seconds: 4.98\n",
      "Epoch   8/30 Batch  860/1978 - Loss:  1.513, Seconds: 4.99\n",
      "Epoch   8/30 Batch  870/1978 - Loss:  1.543, Seconds: 5.01\n",
      "Epoch   8/30 Batch  880/1978 - Loss:  1.563, Seconds: 5.03\n",
      "Epoch   8/30 Batch  890/1978 - Loss:  1.542, Seconds: 5.06\n",
      "Epoch   8/30 Batch  900/1978 - Loss:  1.562, Seconds: 4.96\n",
      "Epoch   8/30 Batch  910/1978 - Loss:  1.573, Seconds: 4.81\n",
      "Epoch   8/30 Batch  920/1978 - Loss:  1.604, Seconds: 4.96\n",
      "Epoch   8/30 Batch  930/1978 - Loss:  1.568, Seconds: 5.03\n",
      "Epoch   8/30 Batch  940/1978 - Loss:  1.731, Seconds: 4.87\n",
      "Epoch   8/30 Batch  950/1978 - Loss:  1.700, Seconds: 4.89\n",
      "Epoch   8/30 Batch  960/1978 - Loss:  1.721, Seconds: 5.09\n",
      "Epoch   8/30 Batch  970/1978 - Loss:  1.670, Seconds: 4.62\n",
      "Epoch   8/30 Batch  980/1978 - Loss:  1.673, Seconds: 5.10\n",
      "Epoch   8/30 Batch  990/1978 - Loss:  1.600, Seconds: 5.25\n",
      "Epoch   8/30 Batch 1000/1978 - Loss:  1.738, Seconds: 5.12\n",
      "Epoch   8/30 Batch 1010/1978 - Loss:  1.603, Seconds: 5.31\n",
      "Epoch   8/30 Batch 1020/1978 - Loss:  1.747, Seconds: 5.18\n",
      "Epoch   8/30 Batch 1030/1978 - Loss:  1.746, Seconds: 5.32\n",
      "Epoch   8/30 Batch 1040/1978 - Loss:  1.777, Seconds: 5.35\n",
      "Epoch   8/30 Batch 1050/1978 - Loss:  1.787, Seconds: 5.22\n",
      "Epoch   8/30 Batch 1060/1978 - Loss:  1.736, Seconds: 5.41\n",
      "Epoch   8/30 Batch 1070/1978 - Loss:  1.680, Seconds: 5.25\n",
      "Epoch   8/30 Batch 1080/1978 - Loss:  1.696, Seconds: 5.26\n",
      "Epoch   8/30 Batch 1090/1978 - Loss:  1.667, Seconds: 5.14\n",
      "Epoch   8/30 Batch 1100/1978 - Loss:  1.702, Seconds: 5.31\n",
      "Epoch   8/30 Batch 1110/1978 - Loss:  1.752, Seconds: 5.32\n",
      "Epoch   8/30 Batch 1120/1978 - Loss:  1.749, Seconds: 5.33\n",
      "Epoch   8/30 Batch 1130/1978 - Loss:  1.641, Seconds: 5.21\n",
      "Epoch   8/30 Batch 1140/1978 - Loss:  1.610, Seconds: 5.55\n",
      "Epoch   8/30 Batch 1150/1978 - Loss:  1.680, Seconds: 5.58\n",
      "Epoch   8/30 Batch 1160/1978 - Loss:  1.615, Seconds: 5.59\n",
      "Epoch   8/30 Batch 1170/1978 - Loss:  1.720, Seconds: 5.26\n",
      "Epoch   8/30 Batch 1180/1978 - Loss:  1.695, Seconds: 5.28\n",
      "Epoch   8/30 Batch 1190/1978 - Loss:  1.715, Seconds: 5.45\n",
      "Epoch   8/30 Batch 1200/1978 - Loss:  1.631, Seconds: 5.49\n",
      "Epoch   8/30 Batch 1210/1978 - Loss:  1.739, Seconds: 5.48\n",
      "Epoch   8/30 Batch 1220/1978 - Loss:  1.712, Seconds: 5.66\n",
      "Epoch   8/30 Batch 1230/1978 - Loss:  1.706, Seconds: 5.19\n",
      "Epoch   8/30 Batch 1240/1978 - Loss:  1.703, Seconds: 5.58\n",
      "Epoch   8/30 Batch 1250/1978 - Loss:  1.694, Seconds: 5.59\n",
      "Epoch   8/30 Batch 1260/1978 - Loss:  1.805, Seconds: 5.55\n",
      "Epoch   8/30 Batch 1270/1978 - Loss:  1.776, Seconds: 5.59\n",
      "Epoch   8/30 Batch 1280/1978 - Loss:  1.801, Seconds: 5.78\n",
      "Epoch   8/30 Batch 1290/1978 - Loss:  1.884, Seconds: 5.65\n",
      "Epoch   8/30 Batch 1300/1978 - Loss:  1.807, Seconds: 5.67\n",
      "Epoch   8/30 Batch 1310/1978 - Loss:  1.899, Seconds: 5.83\n",
      "Average loss for this update: 1.584\n",
      "No Improvement.\n",
      "Epoch   8/30 Batch 1320/1978 - Loss:  1.877, Seconds: 5.58\n",
      "Epoch   8/30 Batch 1330/1978 - Loss:  1.965, Seconds: 5.55\n",
      "Epoch   8/30 Batch 1340/1978 - Loss:  1.948, Seconds: 5.44\n",
      "Epoch   8/30 Batch 1350/1978 - Loss:  1.947, Seconds: 5.45\n",
      "Epoch   8/30 Batch 1360/1978 - Loss:  2.057, Seconds: 5.31\n",
      "Epoch   8/30 Batch 1370/1978 - Loss:  2.008, Seconds: 5.48\n",
      "Epoch   8/30 Batch 1380/1978 - Loss:  2.085, Seconds: 5.99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   8/30 Batch 1390/1978 - Loss:  2.025, Seconds: 5.84\n",
      "Epoch   8/30 Batch 1400/1978 - Loss:  2.070, Seconds: 5.58\n",
      "Epoch   8/30 Batch 1410/1978 - Loss:  2.054, Seconds: 5.75\n",
      "Epoch   8/30 Batch 1420/1978 - Loss:  2.049, Seconds: 5.89\n",
      "Epoch   8/30 Batch 1430/1978 - Loss:  2.195, Seconds: 5.64\n",
      "Epoch   8/30 Batch 1440/1978 - Loss:  2.068, Seconds: 5.77\n",
      "Epoch   8/30 Batch 1450/1978 - Loss:  2.108, Seconds: 5.64\n",
      "Epoch   8/30 Batch 1460/1978 - Loss:  2.137, Seconds: 5.99\n",
      "Epoch   8/30 Batch 1470/1978 - Loss:  2.089, Seconds: 5.69\n",
      "Epoch   8/30 Batch 1480/1978 - Loss:  2.098, Seconds: 5.70\n",
      "Epoch   8/30 Batch 1490/1978 - Loss:  2.186, Seconds: 6.04\n",
      "Epoch   8/30 Batch 1500/1978 - Loss:  2.114, Seconds: 5.74\n",
      "Epoch   8/30 Batch 1510/1978 - Loss:  2.130, Seconds: 5.91\n",
      "Epoch   8/30 Batch 1520/1978 - Loss:  2.147, Seconds: 5.94\n",
      "Epoch   8/30 Batch 1530/1978 - Loss:  2.069, Seconds: 6.29\n",
      "Epoch   8/30 Batch 1540/1978 - Loss:  2.100, Seconds: 6.12\n",
      "Epoch   8/30 Batch 1550/1978 - Loss:  2.216, Seconds: 6.16\n",
      "Epoch   8/30 Batch 1560/1978 - Loss:  2.204, Seconds: 6.34\n",
      "Epoch   8/30 Batch 1570/1978 - Loss:  2.109, Seconds: 5.87\n",
      "Epoch   8/30 Batch 1580/1978 - Loss:  2.132, Seconds: 6.05\n",
      "Epoch   8/30 Batch 1590/1978 - Loss:  2.118, Seconds: 6.30\n",
      "Epoch   8/30 Batch 1600/1978 - Loss:  2.215, Seconds: 5.93\n",
      "Epoch   8/30 Batch 1610/1978 - Loss:  2.134, Seconds: 6.12\n",
      "Epoch   8/30 Batch 1620/1978 - Loss:  2.175, Seconds: 5.98\n",
      "Epoch   8/30 Batch 1630/1978 - Loss:  2.144, Seconds: 6.48\n",
      "Epoch   8/30 Batch 1640/1978 - Loss:  2.170, Seconds: 6.34\n",
      "Epoch   8/30 Batch 1650/1978 - Loss:  2.167, Seconds: 6.22\n",
      "Epoch   8/30 Batch 1660/1978 - Loss:  2.192, Seconds: 6.56\n",
      "Epoch   8/30 Batch 1670/1978 - Loss:  2.188, Seconds: 6.27\n",
      "Epoch   8/30 Batch 1680/1978 - Loss:  2.251, Seconds: 6.26\n",
      "Epoch   8/30 Batch 1690/1978 - Loss:  2.302, Seconds: 6.13\n",
      "Epoch   8/30 Batch 1700/1978 - Loss:  2.248, Seconds: 6.31\n",
      "Epoch   8/30 Batch 1710/1978 - Loss:  2.223, Seconds: 6.49\n",
      "Epoch   8/30 Batch 1720/1978 - Loss:  2.251, Seconds: 6.64\n",
      "Epoch   8/30 Batch 1730/1978 - Loss:  2.177, Seconds: 6.71\n",
      "Epoch   8/30 Batch 1740/1978 - Loss:  2.272, Seconds: 6.55\n",
      "Epoch   8/30 Batch 1750/1978 - Loss:  2.246, Seconds: 6.58\n",
      "Epoch   8/30 Batch 1760/1978 - Loss:  2.272, Seconds: 6.08\n",
      "Epoch   8/30 Batch 1770/1978 - Loss:  2.183, Seconds: 6.76\n",
      "Epoch   8/30 Batch 1780/1978 - Loss:  2.140, Seconds: 6.82\n",
      "Epoch   8/30 Batch 1790/1978 - Loss:  2.238, Seconds: 6.80\n",
      "Epoch   8/30 Batch 1800/1978 - Loss:  2.258, Seconds: 6.68\n",
      "Epoch   8/30 Batch 1810/1978 - Loss:  2.289, Seconds: 6.74\n",
      "Epoch   8/30 Batch 1820/1978 - Loss:  2.213, Seconds: 6.39\n",
      "Epoch   8/30 Batch 1830/1978 - Loss:  2.239, Seconds: 6.76\n",
      "Epoch   8/30 Batch 1840/1978 - Loss:  2.206, Seconds: 6.60\n",
      "Epoch   8/30 Batch 1850/1978 - Loss:  2.368, Seconds: 6.46\n",
      "Epoch   8/30 Batch 1860/1978 - Loss:  2.189, Seconds: 6.50\n",
      "Epoch   8/30 Batch 1870/1978 - Loss:  2.355, Seconds: 6.66\n",
      "Epoch   8/30 Batch 1880/1978 - Loss:  2.408, Seconds: 6.69\n",
      "Epoch   8/30 Batch 1890/1978 - Loss:  2.357, Seconds: 6.70\n",
      "Epoch   8/30 Batch 1900/1978 - Loss:  2.291, Seconds: 6.54\n",
      "Epoch   8/30 Batch 1910/1978 - Loss:  2.197, Seconds: 7.11\n",
      "Epoch   8/30 Batch 1920/1978 - Loss:  2.288, Seconds: 6.97\n",
      "Epoch   8/30 Batch 1930/1978 - Loss:  2.327, Seconds: 6.98\n",
      "Epoch   8/30 Batch 1940/1978 - Loss:  2.278, Seconds: 6.82\n",
      "Epoch   8/30 Batch 1950/1978 - Loss:  2.310, Seconds: 7.01\n",
      "Epoch   8/30 Batch 1960/1978 - Loss:  2.329, Seconds: 7.06\n",
      "Epoch   8/30 Batch 1970/1978 - Loss:  2.307, Seconds: 7.11\n",
      "Average loss for this update: 2.182\n",
      "No Improvement.\n",
      "Epoch   9/30 Batch   10/1978 - Loss:  1.946, Seconds: 2.31\n",
      "Epoch   9/30 Batch   20/1978 - Loss:  1.693, Seconds: 2.10\n",
      "Epoch   9/30 Batch   30/1978 - Loss:  1.734, Seconds: 2.23\n",
      "Epoch   9/30 Batch   40/1978 - Loss:  1.523, Seconds: 2.40\n",
      "Epoch   9/30 Batch   50/1978 - Loss:  1.579, Seconds: 2.67\n",
      "Epoch   9/30 Batch   60/1978 - Loss:  1.574, Seconds: 2.16\n",
      "Epoch   9/30 Batch   70/1978 - Loss:  1.635, Seconds: 2.33\n",
      "Epoch   9/30 Batch   80/1978 - Loss:  1.440, Seconds: 2.73\n",
      "Epoch   9/30 Batch   90/1978 - Loss:  1.472, Seconds: 2.74\n",
      "Epoch   9/30 Batch  100/1978 - Loss:  1.373, Seconds: 2.40\n",
      "Epoch   9/30 Batch  110/1978 - Loss:  1.560, Seconds: 2.88\n",
      "Epoch   9/30 Batch  120/1978 - Loss:  1.565, Seconds: 2.63\n",
      "Epoch   9/30 Batch  130/1978 - Loss:  1.504, Seconds: 2.77\n",
      "Epoch   9/30 Batch  140/1978 - Loss:  1.628, Seconds: 2.96\n",
      "Epoch   9/30 Batch  150/1978 - Loss:  1.609, Seconds: 3.07\n",
      "Epoch   9/30 Batch  160/1978 - Loss:  1.616, Seconds: 3.25\n",
      "Epoch   9/30 Batch  170/1978 - Loss:  1.692, Seconds: 2.85\n",
      "Epoch   9/30 Batch  180/1978 - Loss:  1.710, Seconds: 2.80\n",
      "Epoch   9/30 Batch  190/1978 - Loss:  1.747, Seconds: 3.01\n",
      "Epoch   9/30 Batch  200/1978 - Loss:  1.699, Seconds: 2.95\n",
      "Epoch   9/30 Batch  210/1978 - Loss:  1.593, Seconds: 3.26\n",
      "Epoch   9/30 Batch  220/1978 - Loss:  1.466, Seconds: 3.18\n",
      "Epoch   9/30 Batch  230/1978 - Loss:  1.509, Seconds: 3.55\n",
      "Epoch   9/30 Batch  240/1978 - Loss:  1.381, Seconds: 3.13\n",
      "Epoch   9/30 Batch  250/1978 - Loss:  1.423, Seconds: 3.59\n",
      "Epoch   9/30 Batch  260/1978 - Loss:  1.380, Seconds: 3.61\n",
      "Epoch   9/30 Batch  270/1978 - Loss:  1.438, Seconds: 3.00\n",
      "Epoch   9/30 Batch  280/1978 - Loss:  1.476, Seconds: 3.28\n",
      "Epoch   9/30 Batch  290/1978 - Loss:  1.500, Seconds: 3.44\n",
      "Epoch   9/30 Batch  300/1978 - Loss:  1.419, Seconds: 3.64\n",
      "Epoch   9/30 Batch  310/1978 - Loss:  1.357, Seconds: 3.77\n",
      "Epoch   9/30 Batch  320/1978 - Loss:  1.425, Seconds: 3.80\n",
      "Epoch   9/30 Batch  330/1978 - Loss:  1.389, Seconds: 3.70\n",
      "Epoch   9/30 Batch  340/1978 - Loss:  1.364, Seconds: 3.88\n",
      "Epoch   9/30 Batch  350/1978 - Loss:  1.431, Seconds: 3.97\n",
      "Epoch   9/30 Batch  360/1978 - Loss:  1.312, Seconds: 3.40\n",
      "Epoch   9/30 Batch  370/1978 - Loss:  1.362, Seconds: 3.43\n",
      "Epoch   9/30 Batch  380/1978 - Loss:  1.179, Seconds: 4.00\n",
      "Epoch   9/30 Batch  390/1978 - Loss:  1.268, Seconds: 3.47\n",
      "Epoch   9/30 Batch  400/1978 - Loss:  1.251, Seconds: 4.03\n",
      "Epoch   9/30 Batch  410/1978 - Loss:  1.490, Seconds: 3.94\n",
      "Epoch   9/30 Batch  420/1978 - Loss:  1.470, Seconds: 4.12\n",
      "Epoch   9/30 Batch  430/1978 - Loss:  1.490, Seconds: 4.01\n",
      "Epoch   9/30 Batch  440/1978 - Loss:  1.430, Seconds: 3.74\n",
      "Epoch   9/30 Batch  450/1978 - Loss:  1.408, Seconds: 4.19\n",
      "Epoch   9/30 Batch  460/1978 - Loss:  1.346, Seconds: 3.93\n",
      "Epoch   9/30 Batch  470/1978 - Loss:  1.340, Seconds: 4.24\n",
      "Epoch   9/30 Batch  480/1978 - Loss:  1.397, Seconds: 3.69\n",
      "Epoch   9/30 Batch  490/1978 - Loss:  1.328, Seconds: 4.15\n",
      "Epoch   9/30 Batch  500/1978 - Loss:  1.467, Seconds: 4.18\n",
      "Epoch   9/30 Batch  510/1978 - Loss:  1.461, Seconds: 4.37\n",
      "Epoch   9/30 Batch  520/1978 - Loss:  1.380, Seconds: 3.95\n",
      "Epoch   9/30 Batch  530/1978 - Loss:  1.348, Seconds: 4.43\n",
      "Epoch   9/30 Batch  540/1978 - Loss:  1.316, Seconds: 3.72\n",
      "Epoch   9/30 Batch  550/1978 - Loss:  1.341, Seconds: 4.44\n",
      "Epoch   9/30 Batch  560/1978 - Loss:  1.368, Seconds: 4.16\n",
      "Epoch   9/30 Batch  570/1978 - Loss:  1.349, Seconds: 4.38\n",
      "Epoch   9/30 Batch  580/1978 - Loss:  1.338, Seconds: 4.37\n",
      "Epoch   9/30 Batch  590/1978 - Loss:  1.300, Seconds: 4.36\n",
      "Epoch   9/30 Batch  600/1978 - Loss:  1.306, Seconds: 4.39\n",
      "Epoch   9/30 Batch  610/1978 - Loss:  1.235, Seconds: 4.53\n",
      "Epoch   9/30 Batch  620/1978 - Loss:  1.324, Seconds: 4.57\n",
      "Epoch   9/30 Batch  630/1978 - Loss:  1.449, Seconds: 4.58\n",
      "Epoch   9/30 Batch  640/1978 - Loss:  1.379, Seconds: 4.49\n",
      "Epoch   9/30 Batch  650/1978 - Loss:  1.288, Seconds: 4.64\n",
      "Average loss for this update: 1.45\n",
      "New Record!\n",
      "Epoch   9/30 Batch  660/1978 - Loss:  1.041, Seconds: 4.64\n",
      "Epoch   9/30 Batch  670/1978 - Loss:  1.018, Seconds: 4.67\n",
      "Epoch   9/30 Batch  680/1978 - Loss:  1.058, Seconds: 4.51\n",
      "Epoch   9/30 Batch  690/1978 - Loss:  1.061, Seconds: 4.68\n",
      "Epoch   9/30 Batch  700/1978 - Loss:  1.170, Seconds: 4.42\n",
      "Epoch   9/30 Batch  710/1978 - Loss:  1.066, Seconds: 4.70\n",
      "Epoch   9/30 Batch  720/1978 - Loss:  1.144, Seconds: 4.45\n",
      "Epoch   9/30 Batch  730/1978 - Loss:  1.176, Seconds: 4.60\n",
      "Epoch   9/30 Batch  740/1978 - Loss:  1.298, Seconds: 4.76\n",
      "Epoch   9/30 Batch  750/1978 - Loss:  1.359, Seconds: 4.76\n",
      "Epoch   9/30 Batch  760/1978 - Loss:  1.484, Seconds: 4.65\n",
      "Epoch   9/30 Batch  770/1978 - Loss:  1.435, Seconds: 4.83\n",
      "Epoch   9/30 Batch  780/1978 - Loss:  1.357, Seconds: 4.86\n",
      "Epoch   9/30 Batch  790/1978 - Loss:  1.439, Seconds: 4.89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   9/30 Batch  800/1978 - Loss:  1.425, Seconds: 4.89\n",
      "Epoch   9/30 Batch  810/1978 - Loss:  1.499, Seconds: 4.76\n",
      "Epoch   9/30 Batch  820/1978 - Loss:  1.432, Seconds: 4.92\n",
      "Epoch   9/30 Batch  830/1978 - Loss:  1.475, Seconds: 4.95\n",
      "Epoch   9/30 Batch  840/1978 - Loss:  1.498, Seconds: 4.98\n",
      "Epoch   9/30 Batch  850/1978 - Loss:  1.501, Seconds: 5.01\n",
      "Epoch   9/30 Batch  860/1978 - Loss:  1.530, Seconds: 5.03\n",
      "Epoch   9/30 Batch  870/1978 - Loss:  1.508, Seconds: 5.04\n",
      "Epoch   9/30 Batch  880/1978 - Loss:  1.549, Seconds: 5.05\n",
      "Epoch   9/30 Batch  890/1978 - Loss:  1.539, Seconds: 5.09\n",
      "Epoch   9/30 Batch  900/1978 - Loss:  1.565, Seconds: 4.92\n",
      "Epoch   9/30 Batch  910/1978 - Loss:  1.570, Seconds: 4.79\n",
      "Epoch   9/30 Batch  920/1978 - Loss:  1.577, Seconds: 4.97\n",
      "Epoch   9/30 Batch  930/1978 - Loss:  1.550, Seconds: 5.00\n",
      "Epoch   9/30 Batch  940/1978 - Loss:  1.711, Seconds: 4.86\n",
      "Epoch   9/30 Batch  950/1978 - Loss:  1.702, Seconds: 4.90\n",
      "Epoch   9/30 Batch  960/1978 - Loss:  1.702, Seconds: 5.05\n",
      "Epoch   9/30 Batch  970/1978 - Loss:  1.660, Seconds: 4.59\n",
      "Epoch   9/30 Batch  980/1978 - Loss:  1.670, Seconds: 5.07\n",
      "Epoch   9/30 Batch  990/1978 - Loss:  1.610, Seconds: 5.26\n",
      "Epoch   9/30 Batch 1000/1978 - Loss:  1.716, Seconds: 5.16\n",
      "Epoch   9/30 Batch 1010/1978 - Loss:  1.594, Seconds: 5.29\n",
      "Epoch   9/30 Batch 1020/1978 - Loss:  1.725, Seconds: 5.16\n",
      "Epoch   9/30 Batch 1030/1978 - Loss:  1.727, Seconds: 5.35\n",
      "Epoch   9/30 Batch 1040/1978 - Loss:  1.741, Seconds: 5.35\n",
      "Epoch   9/30 Batch 1050/1978 - Loss:  1.765, Seconds: 5.22\n",
      "Epoch   9/30 Batch 1060/1978 - Loss:  1.743, Seconds: 5.40\n",
      "Epoch   9/30 Batch 1070/1978 - Loss:  1.657, Seconds: 5.27\n",
      "Epoch   9/30 Batch 1080/1978 - Loss:  1.672, Seconds: 5.31\n",
      "Epoch   9/30 Batch 1090/1978 - Loss:  1.652, Seconds: 5.15\n",
      "Epoch   9/30 Batch 1100/1978 - Loss:  1.678, Seconds: 5.34\n",
      "Epoch   9/30 Batch 1110/1978 - Loss:  1.741, Seconds: 5.32\n",
      "Epoch   9/30 Batch 1120/1978 - Loss:  1.723, Seconds: 5.35\n",
      "Epoch   9/30 Batch 1130/1978 - Loss:  1.619, Seconds: 5.19\n",
      "Epoch   9/30 Batch 1140/1978 - Loss:  1.602, Seconds: 5.52\n",
      "Epoch   9/30 Batch 1150/1978 - Loss:  1.664, Seconds: 5.55\n",
      "Epoch   9/30 Batch 1160/1978 - Loss:  1.609, Seconds: 5.57\n",
      "Epoch   9/30 Batch 1170/1978 - Loss:  1.681, Seconds: 5.27\n",
      "Epoch   9/30 Batch 1180/1978 - Loss:  1.700, Seconds: 5.27\n",
      "Epoch   9/30 Batch 1190/1978 - Loss:  1.694, Seconds: 5.44\n",
      "Epoch   9/30 Batch 1200/1978 - Loss:  1.626, Seconds: 5.47\n",
      "Epoch   9/30 Batch 1210/1978 - Loss:  1.717, Seconds: 5.50\n",
      "Epoch   9/30 Batch 1220/1978 - Loss:  1.713, Seconds: 5.69\n",
      "Epoch   9/30 Batch 1230/1978 - Loss:  1.698, Seconds: 5.22\n",
      "Epoch   9/30 Batch 1240/1978 - Loss:  1.679, Seconds: 5.55\n",
      "Epoch   9/30 Batch 1250/1978 - Loss:  1.684, Seconds: 5.58\n",
      "Epoch   9/30 Batch 1260/1978 - Loss:  1.778, Seconds: 5.58\n",
      "Epoch   9/30 Batch 1270/1978 - Loss:  1.781, Seconds: 5.61\n",
      "Epoch   9/30 Batch 1280/1978 - Loss:  1.800, Seconds: 5.81\n",
      "Epoch   9/30 Batch 1290/1978 - Loss:  1.854, Seconds: 5.64\n",
      "Epoch   9/30 Batch 1300/1978 - Loss:  1.777, Seconds: 5.68\n",
      "Epoch   9/30 Batch 1310/1978 - Loss:  1.880, Seconds: 5.86\n",
      "Average loss for this update: 1.571\n",
      "No Improvement.\n",
      "Epoch   9/30 Batch 1320/1978 - Loss:  1.878, Seconds: 5.57\n",
      "Epoch   9/30 Batch 1330/1978 - Loss:  1.973, Seconds: 5.57\n",
      "Epoch   9/30 Batch 1340/1978 - Loss:  1.959, Seconds: 5.43\n",
      "Epoch   9/30 Batch 1350/1978 - Loss:  1.945, Seconds: 5.46\n",
      "Epoch   9/30 Batch 1360/1978 - Loss:  2.029, Seconds: 5.31\n",
      "Epoch   9/30 Batch 1370/1978 - Loss:  2.003, Seconds: 5.50\n",
      "Epoch   9/30 Batch 1380/1978 - Loss:  2.079, Seconds: 5.97\n",
      "Epoch   9/30 Batch 1390/1978 - Loss:  2.007, Seconds: 5.83\n",
      "Epoch   9/30 Batch 1400/1978 - Loss:  2.072, Seconds: 5.54\n",
      "Epoch   9/30 Batch 1410/1978 - Loss:  2.017, Seconds: 5.75\n",
      "Epoch   9/30 Batch 1420/1978 - Loss:  2.033, Seconds: 5.91\n",
      "Epoch   9/30 Batch 1430/1978 - Loss:  2.182, Seconds: 5.61\n",
      "Epoch   9/30 Batch 1440/1978 - Loss:  2.073, Seconds: 5.80\n",
      "Epoch   9/30 Batch 1450/1978 - Loss:  2.074, Seconds: 5.66\n",
      "Epoch   9/30 Batch 1460/1978 - Loss:  2.132, Seconds: 6.02\n",
      "Epoch   9/30 Batch 1470/1978 - Loss:  2.095, Seconds: 5.73\n",
      "Epoch   9/30 Batch 1480/1978 - Loss:  2.102, Seconds: 5.70\n",
      "Epoch   9/30 Batch 1490/1978 - Loss:  2.163, Seconds: 6.03\n",
      "Epoch   9/30 Batch 1500/1978 - Loss:  2.113, Seconds: 5.77\n",
      "Epoch   9/30 Batch 1510/1978 - Loss:  2.103, Seconds: 5.92\n",
      "Epoch   9/30 Batch 1520/1978 - Loss:  2.115, Seconds: 5.95\n",
      "Epoch   9/30 Batch 1530/1978 - Loss:  2.041, Seconds: 6.27\n",
      "Epoch   9/30 Batch 1540/1978 - Loss:  2.085, Seconds: 6.14\n",
      "Epoch   9/30 Batch 1550/1978 - Loss:  2.201, Seconds: 6.16\n",
      "Epoch   9/30 Batch 1560/1978 - Loss:  2.198, Seconds: 6.32\n",
      "Epoch   9/30 Batch 1570/1978 - Loss:  2.090, Seconds: 6.00\n",
      "Epoch   9/30 Batch 1580/1978 - Loss:  2.131, Seconds: 6.11\n",
      "Epoch   9/30 Batch 1590/1978 - Loss:  2.121, Seconds: 6.30\n",
      "Epoch   9/30 Batch 1600/1978 - Loss:  2.178, Seconds: 5.93\n",
      "Epoch   9/30 Batch 1610/1978 - Loss:  2.106, Seconds: 6.13\n",
      "Epoch   9/30 Batch 1620/1978 - Loss:  2.148, Seconds: 6.07\n",
      "Epoch   9/30 Batch 1630/1978 - Loss:  2.112, Seconds: 6.54\n",
      "Epoch   9/30 Batch 1640/1978 - Loss:  2.164, Seconds: 6.37\n",
      "Epoch   9/30 Batch 1650/1978 - Loss:  2.140, Seconds: 6.20\n",
      "Epoch   9/30 Batch 1660/1978 - Loss:  2.158, Seconds: 6.59\n",
      "Epoch   9/30 Batch 1670/1978 - Loss:  2.148, Seconds: 6.17\n",
      "Epoch   9/30 Batch 1680/1978 - Loss:  2.256, Seconds: 6.24\n",
      "Epoch   9/30 Batch 1690/1978 - Loss:  2.321, Seconds: 6.13\n",
      "Epoch   9/30 Batch 1700/1978 - Loss:  2.226, Seconds: 6.29\n",
      "Epoch   9/30 Batch 1710/1978 - Loss:  2.219, Seconds: 6.50\n",
      "Epoch   9/30 Batch 1720/1978 - Loss:  2.224, Seconds: 6.72\n",
      "Epoch   9/30 Batch 1730/1978 - Loss:  2.147, Seconds: 6.74\n",
      "Epoch   9/30 Batch 1740/1978 - Loss:  2.261, Seconds: 6.57\n",
      "Epoch   9/30 Batch 1750/1978 - Loss:  2.241, Seconds: 6.61\n",
      "Epoch   9/30 Batch 1760/1978 - Loss:  2.263, Seconds: 6.11\n",
      "Epoch   9/30 Batch 1770/1978 - Loss:  2.164, Seconds: 6.71\n",
      "Epoch   9/30 Batch 1780/1978 - Loss:  2.116, Seconds: 6.81\n",
      "Epoch   9/30 Batch 1790/1978 - Loss:  2.191, Seconds: 6.79\n",
      "Epoch   9/30 Batch 1800/1978 - Loss:  2.260, Seconds: 6.68\n",
      "Epoch   9/30 Batch 1810/1978 - Loss:  2.280, Seconds: 6.70\n",
      "Epoch   9/30 Batch 1820/1978 - Loss:  2.192, Seconds: 6.45\n",
      "Epoch   9/30 Batch 1830/1978 - Loss:  2.244, Seconds: 6.81\n",
      "Epoch   9/30 Batch 1840/1978 - Loss:  2.179, Seconds: 6.63\n",
      "Epoch   9/30 Batch 1850/1978 - Loss:  2.340, Seconds: 6.45\n",
      "Epoch   9/30 Batch 1860/1978 - Loss:  2.157, Seconds: 6.49\n",
      "Epoch   9/30 Batch 1870/1978 - Loss:  2.349, Seconds: 6.71\n",
      "Epoch   9/30 Batch 1880/1978 - Loss:  2.392, Seconds: 6.68\n",
      "Epoch   9/30 Batch 1890/1978 - Loss:  2.329, Seconds: 6.70\n",
      "Epoch   9/30 Batch 1900/1978 - Loss:  2.259, Seconds: 6.61\n",
      "Epoch   9/30 Batch 1910/1978 - Loss:  2.181, Seconds: 7.11\n",
      "Epoch   9/30 Batch 1920/1978 - Loss:  2.265, Seconds: 6.96\n",
      "Epoch   9/30 Batch 1930/1978 - Loss:  2.296, Seconds: 6.97\n",
      "Epoch   9/30 Batch 1940/1978 - Loss:  2.246, Seconds: 6.84\n",
      "Epoch   9/30 Batch 1950/1978 - Loss:  2.291, Seconds: 7.06\n",
      "Epoch   9/30 Batch 1960/1978 - Loss:  2.285, Seconds: 7.02\n",
      "Epoch   9/30 Batch 1970/1978 - Loss:  2.269, Seconds: 7.04\n",
      "Average loss for this update: 2.165\n",
      "No Improvement.\n",
      "Epoch  10/30 Batch   10/1978 - Loss:  1.881, Seconds: 2.29\n",
      "Epoch  10/30 Batch   20/1978 - Loss:  1.713, Seconds: 2.08\n",
      "Epoch  10/30 Batch   30/1978 - Loss:  1.751, Seconds: 2.34\n",
      "Epoch  10/30 Batch   40/1978 - Loss:  1.525, Seconds: 2.39\n",
      "Epoch  10/30 Batch   50/1978 - Loss:  1.587, Seconds: 2.53\n",
      "Epoch  10/30 Batch   60/1978 - Loss:  1.582, Seconds: 2.07\n",
      "Epoch  10/30 Batch   70/1978 - Loss:  1.617, Seconds: 2.22\n",
      "Epoch  10/30 Batch   80/1978 - Loss:  1.428, Seconds: 2.80\n",
      "Epoch  10/30 Batch   90/1978 - Loss:  1.473, Seconds: 2.76\n",
      "Epoch  10/30 Batch  100/1978 - Loss:  1.349, Seconds: 2.48\n",
      "Epoch  10/30 Batch  110/1978 - Loss:  1.541, Seconds: 2.91\n",
      "Epoch  10/30 Batch  120/1978 - Loss:  1.555, Seconds: 2.65\n",
      "Epoch  10/30 Batch  130/1978 - Loss:  1.469, Seconds: 2.89\n",
      "Epoch  10/30 Batch  140/1978 - Loss:  1.637, Seconds: 2.84\n",
      "Epoch  10/30 Batch  150/1978 - Loss:  1.589, Seconds: 3.09\n",
      "Epoch  10/30 Batch  160/1978 - Loss:  1.618, Seconds: 3.25\n",
      "Epoch  10/30 Batch  170/1978 - Loss:  1.711, Seconds: 2.78\n",
      "Epoch  10/30 Batch  180/1978 - Loss:  1.704, Seconds: 2.66\n",
      "Epoch  10/30 Batch  190/1978 - Loss:  1.751, Seconds: 2.84\n",
      "Epoch  10/30 Batch  200/1978 - Loss:  1.681, Seconds: 2.92\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  10/30 Batch  210/1978 - Loss:  1.600, Seconds: 3.22\n",
      "Epoch  10/30 Batch  220/1978 - Loss:  1.455, Seconds: 3.09\n",
      "Epoch  10/30 Batch  230/1978 - Loss:  1.498, Seconds: 3.51\n",
      "Epoch  10/30 Batch  240/1978 - Loss:  1.390, Seconds: 3.04\n",
      "Epoch  10/30 Batch  250/1978 - Loss:  1.409, Seconds: 3.58\n",
      "Epoch  10/30 Batch  260/1978 - Loss:  1.370, Seconds: 3.60\n",
      "Epoch  10/30 Batch  270/1978 - Loss:  1.419, Seconds: 2.99\n",
      "Epoch  10/30 Batch  280/1978 - Loss:  1.457, Seconds: 3.26\n",
      "Epoch  10/30 Batch  290/1978 - Loss:  1.488, Seconds: 3.43\n",
      "Epoch  10/30 Batch  300/1978 - Loss:  1.415, Seconds: 3.61\n",
      "Epoch  10/30 Batch  310/1978 - Loss:  1.337, Seconds: 3.78\n",
      "Epoch  10/30 Batch  320/1978 - Loss:  1.381, Seconds: 3.81\n",
      "Epoch  10/30 Batch  330/1978 - Loss:  1.384, Seconds: 3.72\n",
      "Epoch  10/30 Batch  340/1978 - Loss:  1.361, Seconds: 3.87\n",
      "Epoch  10/30 Batch  350/1978 - Loss:  1.412, Seconds: 3.94\n",
      "Epoch  10/30 Batch  360/1978 - Loss:  1.292, Seconds: 3.48\n",
      "Epoch  10/30 Batch  370/1978 - Loss:  1.365, Seconds: 3.43\n",
      "Epoch  10/30 Batch  380/1978 - Loss:  1.161, Seconds: 4.13\n",
      "Epoch  10/30 Batch  390/1978 - Loss:  1.275, Seconds: 3.47\n",
      "Epoch  10/30 Batch  400/1978 - Loss:  1.244, Seconds: 4.09\n",
      "Epoch  10/30 Batch  410/1978 - Loss:  1.456, Seconds: 3.93\n",
      "Epoch  10/30 Batch  420/1978 - Loss:  1.462, Seconds: 4.10\n",
      "Epoch  10/30 Batch  430/1978 - Loss:  1.462, Seconds: 4.01\n",
      "Epoch  10/30 Batch  440/1978 - Loss:  1.400, Seconds: 3.76\n",
      "Epoch  10/30 Batch  450/1978 - Loss:  1.394, Seconds: 4.18\n",
      "Epoch  10/30 Batch  460/1978 - Loss:  1.328, Seconds: 3.96\n",
      "Epoch  10/30 Batch  470/1978 - Loss:  1.333, Seconds: 4.24\n",
      "Epoch  10/30 Batch  480/1978 - Loss:  1.388, Seconds: 3.70\n",
      "Epoch  10/30 Batch  490/1978 - Loss:  1.295, Seconds: 4.18\n",
      "Epoch  10/30 Batch  500/1978 - Loss:  1.462, Seconds: 4.17\n",
      "Epoch  10/30 Batch  510/1978 - Loss:  1.469, Seconds: 4.36\n",
      "Epoch  10/30 Batch  520/1978 - Loss:  1.384, Seconds: 3.97\n",
      "Epoch  10/30 Batch  530/1978 - Loss:  1.337, Seconds: 4.40\n",
      "Epoch  10/30 Batch  540/1978 - Loss:  1.312, Seconds: 3.72\n",
      "Epoch  10/30 Batch  550/1978 - Loss:  1.330, Seconds: 4.67\n",
      "Epoch  10/30 Batch  560/1978 - Loss:  1.341, Seconds: 4.16\n",
      "Epoch  10/30 Batch  570/1978 - Loss:  1.342, Seconds: 4.33\n",
      "Epoch  10/30 Batch  580/1978 - Loss:  1.345, Seconds: 4.36\n",
      "Epoch  10/30 Batch  590/1978 - Loss:  1.317, Seconds: 4.36\n",
      "Epoch  10/30 Batch  600/1978 - Loss:  1.318, Seconds: 4.39\n",
      "Epoch  10/30 Batch  610/1978 - Loss:  1.241, Seconds: 4.56\n",
      "Epoch  10/30 Batch  620/1978 - Loss:  1.316, Seconds: 4.56\n",
      "Epoch  10/30 Batch  630/1978 - Loss:  1.430, Seconds: 4.60\n",
      "Epoch  10/30 Batch  640/1978 - Loss:  1.384, Seconds: 4.47\n",
      "Epoch  10/30 Batch  650/1978 - Loss:  1.301, Seconds: 4.63\n",
      "Average loss for this update: 1.442\n",
      "New Record!\n",
      "Epoch  10/30 Batch  660/1978 - Loss:  1.051, Seconds: 4.65\n",
      "Epoch  10/30 Batch  670/1978 - Loss:  1.019, Seconds: 4.65\n",
      "Epoch  10/30 Batch  680/1978 - Loss:  1.048, Seconds: 4.51\n",
      "Epoch  10/30 Batch  690/1978 - Loss:  1.058, Seconds: 4.70\n",
      "Epoch  10/30 Batch  700/1978 - Loss:  1.187, Seconds: 4.44\n",
      "Epoch  10/30 Batch  710/1978 - Loss:  1.051, Seconds: 4.70\n",
      "Epoch  10/30 Batch  720/1978 - Loss:  1.176, Seconds: 4.45\n",
      "Epoch  10/30 Batch  730/1978 - Loss:  1.179, Seconds: 4.60\n",
      "Epoch  10/30 Batch  740/1978 - Loss:  1.308, Seconds: 4.82\n",
      "Epoch  10/30 Batch  750/1978 - Loss:  1.358, Seconds: 4.77\n",
      "Epoch  10/30 Batch  760/1978 - Loss:  1.488, Seconds: 4.65\n",
      "Epoch  10/30 Batch  770/1978 - Loss:  1.443, Seconds: 4.83\n",
      "Epoch  10/30 Batch  780/1978 - Loss:  1.367, Seconds: 5.20\n",
      "Epoch  10/30 Batch  790/1978 - Loss:  1.429, Seconds: 4.86\n",
      "Epoch  10/30 Batch  800/1978 - Loss:  1.441, Seconds: 4.91\n",
      "Epoch  10/30 Batch  810/1978 - Loss:  1.486, Seconds: 4.79\n",
      "Epoch  10/30 Batch  820/1978 - Loss:  1.420, Seconds: 4.92\n",
      "Epoch  10/30 Batch  830/1978 - Loss:  1.465, Seconds: 4.97\n",
      "Epoch  10/30 Batch  840/1978 - Loss:  1.497, Seconds: 4.99\n",
      "Epoch  10/30 Batch  850/1978 - Loss:  1.500, Seconds: 4.98\n",
      "Epoch  10/30 Batch  860/1978 - Loss:  1.519, Seconds: 5.05\n",
      "Epoch  10/30 Batch  870/1978 - Loss:  1.501, Seconds: 5.03\n",
      "Epoch  10/30 Batch  880/1978 - Loss:  1.522, Seconds: 5.05\n",
      "Epoch  10/30 Batch  890/1978 - Loss:  1.533, Seconds: 5.08\n",
      "Epoch  10/30 Batch  900/1978 - Loss:  1.559, Seconds: 4.93\n",
      "Epoch  10/30 Batch  910/1978 - Loss:  1.567, Seconds: 4.84\n",
      "Epoch  10/30 Batch  920/1978 - Loss:  1.590, Seconds: 4.99\n",
      "Epoch  10/30 Batch  930/1978 - Loss:  1.544, Seconds: 4.99\n",
      "Epoch  10/30 Batch  940/1978 - Loss:  1.705, Seconds: 4.86\n",
      "Epoch  10/30 Batch  950/1978 - Loss:  1.694, Seconds: 4.90\n",
      "Epoch  10/30 Batch  960/1978 - Loss:  1.701, Seconds: 5.04\n",
      "Epoch  10/30 Batch  970/1978 - Loss:  1.637, Seconds: 4.63\n",
      "Epoch  10/30 Batch  980/1978 - Loss:  1.665, Seconds: 5.12\n",
      "Epoch  10/30 Batch  990/1978 - Loss:  1.578, Seconds: 5.45\n",
      "Epoch  10/30 Batch 1000/1978 - Loss:  1.702, Seconds: 5.15\n",
      "Epoch  10/30 Batch 1010/1978 - Loss:  1.593, Seconds: 5.31\n",
      "Epoch  10/30 Batch 1020/1978 - Loss:  1.709, Seconds: 5.16\n",
      "Epoch  10/30 Batch 1030/1978 - Loss:  1.722, Seconds: 5.33\n",
      "Epoch  10/30 Batch 1040/1978 - Loss:  1.752, Seconds: 5.35\n",
      "Epoch  10/30 Batch 1050/1978 - Loss:  1.775, Seconds: 5.23\n",
      "Epoch  10/30 Batch 1060/1978 - Loss:  1.733, Seconds: 5.41\n",
      "Epoch  10/30 Batch 1070/1978 - Loss:  1.664, Seconds: 5.24\n",
      "Epoch  10/30 Batch 1080/1978 - Loss:  1.687, Seconds: 5.25\n",
      "Epoch  10/30 Batch 1090/1978 - Loss:  1.670, Seconds: 5.12\n",
      "Epoch  10/30 Batch 1100/1978 - Loss:  1.661, Seconds: 5.34\n",
      "Epoch  10/30 Batch 1110/1978 - Loss:  1.730, Seconds: 5.34\n",
      "Epoch  10/30 Batch 1120/1978 - Loss:  1.714, Seconds: 5.34\n",
      "Epoch  10/30 Batch 1130/1978 - Loss:  1.616, Seconds: 5.18\n",
      "Epoch  10/30 Batch 1140/1978 - Loss:  1.601, Seconds: 5.57\n",
      "Epoch  10/30 Batch 1150/1978 - Loss:  1.655, Seconds: 5.54\n",
      "Epoch  10/30 Batch 1160/1978 - Loss:  1.585, Seconds: 5.55\n",
      "Epoch  10/30 Batch 1170/1978 - Loss:  1.671, Seconds: 5.26\n",
      "Epoch  10/30 Batch 1180/1978 - Loss:  1.695, Seconds: 5.32\n",
      "Epoch  10/30 Batch 1190/1978 - Loss:  1.688, Seconds: 5.45\n",
      "Epoch  10/30 Batch 1200/1978 - Loss:  1.621, Seconds: 5.50\n",
      "Epoch  10/30 Batch 1210/1978 - Loss:  1.711, Seconds: 5.50\n",
      "Epoch  10/30 Batch 1220/1978 - Loss:  1.696, Seconds: 5.69\n",
      "Epoch  10/30 Batch 1230/1978 - Loss:  1.670, Seconds: 5.22\n",
      "Epoch  10/30 Batch 1240/1978 - Loss:  1.672, Seconds: 5.57\n",
      "Epoch  10/30 Batch 1250/1978 - Loss:  1.686, Seconds: 5.60\n",
      "Epoch  10/30 Batch 1260/1978 - Loss:  1.770, Seconds: 5.62\n",
      "Epoch  10/30 Batch 1270/1978 - Loss:  1.763, Seconds: 5.62\n",
      "Epoch  10/30 Batch 1280/1978 - Loss:  1.777, Seconds: 5.77\n",
      "Epoch  10/30 Batch 1290/1978 - Loss:  1.837, Seconds: 5.64\n",
      "Epoch  10/30 Batch 1300/1978 - Loss:  1.775, Seconds: 5.70\n",
      "Epoch  10/30 Batch 1310/1978 - Loss:  1.856, Seconds: 5.86\n",
      "Average loss for this update: 1.566\n",
      "No Improvement.\n",
      "Epoch  10/30 Batch 1320/1978 - Loss:  1.894, Seconds: 5.52\n",
      "Epoch  10/30 Batch 1330/1978 - Loss:  1.965, Seconds: 5.59\n",
      "Epoch  10/30 Batch 1340/1978 - Loss:  1.918, Seconds: 5.46\n",
      "Epoch  10/30 Batch 1350/1978 - Loss:  1.948, Seconds: 5.43\n",
      "Epoch  10/30 Batch 1360/1978 - Loss:  2.033, Seconds: 5.32\n",
      "Epoch  10/30 Batch 1370/1978 - Loss:  1.986, Seconds: 5.49\n",
      "Epoch  10/30 Batch 1380/1978 - Loss:  2.062, Seconds: 5.97\n",
      "Epoch  10/30 Batch 1390/1978 - Loss:  2.012, Seconds: 5.84\n",
      "Epoch  10/30 Batch 1400/1978 - Loss:  2.064, Seconds: 5.56\n",
      "Epoch  10/30 Batch 1410/1978 - Loss:  2.018, Seconds: 5.74\n",
      "Epoch  10/30 Batch 1420/1978 - Loss:  2.033, Seconds: 5.93\n",
      "Epoch  10/30 Batch 1430/1978 - Loss:  2.165, Seconds: 5.63\n",
      "Epoch  10/30 Batch 1440/1978 - Loss:  2.058, Seconds: 5.76\n",
      "Epoch  10/30 Batch 1450/1978 - Loss:  2.074, Seconds: 5.65\n",
      "Epoch  10/30 Batch 1460/1978 - Loss:  2.104, Seconds: 6.00\n",
      "Epoch  10/30 Batch 1470/1978 - Loss:  2.060, Seconds: 5.68\n",
      "Epoch  10/30 Batch 1480/1978 - Loss:  2.082, Seconds: 5.69\n",
      "Epoch  10/30 Batch 1490/1978 - Loss:  2.193, Seconds: 6.05\n",
      "Epoch  10/30 Batch 1500/1978 - Loss:  2.124, Seconds: 5.74\n",
      "Epoch  10/30 Batch 1510/1978 - Loss:  2.121, Seconds: 5.95\n",
      "Epoch  10/30 Batch 1520/1978 - Loss:  2.112, Seconds: 5.98\n",
      "Epoch  10/30 Batch 1530/1978 - Loss:  2.029, Seconds: 6.28\n",
      "Epoch  10/30 Batch 1540/1978 - Loss:  2.088, Seconds: 6.14\n",
      "Epoch  10/30 Batch 1550/1978 - Loss:  2.199, Seconds: 6.17\n",
      "Epoch  10/30 Batch 1560/1978 - Loss:  2.191, Seconds: 6.35\n",
      "Epoch  10/30 Batch 1570/1978 - Loss:  2.106, Seconds: 5.86\n",
      "Epoch  10/30 Batch 1580/1978 - Loss:  2.139, Seconds: 6.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  10/30 Batch 1590/1978 - Loss:  2.127, Seconds: 6.26\n",
      "Epoch  10/30 Batch 1600/1978 - Loss:  2.202, Seconds: 5.91\n",
      "Epoch  10/30 Batch 1610/1978 - Loss:  2.111, Seconds: 6.10\n",
      "Epoch  10/30 Batch 1620/1978 - Loss:  2.141, Seconds: 5.99\n",
      "Epoch  10/30 Batch 1630/1978 - Loss:  2.111, Seconds: 6.49\n",
      "Epoch  10/30 Batch 1640/1978 - Loss:  2.164, Seconds: 6.34\n",
      "Epoch  10/30 Batch 1650/1978 - Loss:  2.132, Seconds: 6.23\n",
      "Epoch  10/30 Batch 1660/1978 - Loss:  2.162, Seconds: 6.56\n",
      "Epoch  10/30 Batch 1670/1978 - Loss:  2.164, Seconds: 6.24\n",
      "Epoch  10/30 Batch 1680/1978 - Loss:  2.232, Seconds: 6.26\n",
      "Epoch  10/30 Batch 1690/1978 - Loss:  2.288, Seconds: 6.12\n",
      "Epoch  10/30 Batch 1700/1978 - Loss:  2.222, Seconds: 6.31\n",
      "Epoch  10/30 Batch 1710/1978 - Loss:  2.205, Seconds: 6.49\n",
      "Epoch  10/30 Batch 1720/1978 - Loss:  2.222, Seconds: 6.69\n",
      "Epoch  10/30 Batch 1730/1978 - Loss:  2.147, Seconds: 6.72\n",
      "Epoch  10/30 Batch 1740/1978 - Loss:  2.255, Seconds: 6.57\n",
      "Epoch  10/30 Batch 1750/1978 - Loss:  2.228, Seconds: 6.61\n",
      "Epoch  10/30 Batch 1760/1978 - Loss:  2.252, Seconds: 6.07\n",
      "Epoch  10/30 Batch 1770/1978 - Loss:  2.167, Seconds: 6.80\n",
      "Epoch  10/30 Batch 1780/1978 - Loss:  2.102, Seconds: 6.87\n",
      "Epoch  10/30 Batch 1790/1978 - Loss:  2.191, Seconds: 6.81\n",
      "Epoch  10/30 Batch 1800/1978 - Loss:  2.250, Seconds: 6.68\n",
      "Epoch  10/30 Batch 1810/1978 - Loss:  2.287, Seconds: 6.72\n",
      "Epoch  10/30 Batch 1820/1978 - Loss:  2.195, Seconds: 6.41\n",
      "Epoch  10/30 Batch 1830/1978 - Loss:  2.227, Seconds: 6.77\n",
      "Epoch  10/30 Batch 1840/1978 - Loss:  2.162, Seconds: 6.60\n",
      "Epoch  10/30 Batch 1850/1978 - Loss:  2.317, Seconds: 6.49\n",
      "Epoch  10/30 Batch 1860/1978 - Loss:  2.159, Seconds: 6.48\n",
      "Epoch  10/30 Batch 1870/1978 - Loss:  2.326, Seconds: 6.64\n",
      "Epoch  10/30 Batch 1880/1978 - Loss:  2.372, Seconds: 6.69\n",
      "Epoch  10/30 Batch 1890/1978 - Loss:  2.333, Seconds: 6.72\n",
      "Epoch  10/30 Batch 1900/1978 - Loss:  2.256, Seconds: 6.56\n",
      "Epoch  10/30 Batch 1910/1978 - Loss:  2.163, Seconds: 7.13\n",
      "Epoch  10/30 Batch 1920/1978 - Loss:  2.266, Seconds: 6.97\n",
      "Epoch  10/30 Batch 1930/1978 - Loss:  2.314, Seconds: 6.98\n",
      "Epoch  10/30 Batch 1940/1978 - Loss:  2.230, Seconds: 6.83\n",
      "Epoch  10/30 Batch 1950/1978 - Loss:  2.294, Seconds: 7.01\n",
      "Epoch  10/30 Batch 1960/1978 - Loss:  2.282, Seconds: 7.07\n",
      "Epoch  10/30 Batch 1970/1978 - Loss:  2.287, Seconds: 7.08\n",
      "Average loss for this update: 2.16\n",
      "No Improvement.\n",
      "Epoch  11/30 Batch   10/1978 - Loss:  1.885, Seconds: 2.28\n",
      "Epoch  11/30 Batch   20/1978 - Loss:  1.670, Seconds: 2.11\n",
      "Epoch  11/30 Batch   30/1978 - Loss:  1.763, Seconds: 2.24\n",
      "Epoch  11/30 Batch   40/1978 - Loss:  1.522, Seconds: 2.39\n",
      "Epoch  11/30 Batch   50/1978 - Loss:  1.591, Seconds: 2.62\n",
      "Epoch  11/30 Batch   60/1978 - Loss:  1.574, Seconds: 2.07\n",
      "Epoch  11/30 Batch   70/1978 - Loss:  1.643, Seconds: 2.36\n",
      "Epoch  11/30 Batch   80/1978 - Loss:  1.436, Seconds: 2.75\n",
      "Epoch  11/30 Batch   90/1978 - Loss:  1.440, Seconds: 2.64\n",
      "Epoch  11/30 Batch  100/1978 - Loss:  1.327, Seconds: 2.45\n",
      "Epoch  11/30 Batch  110/1978 - Loss:  1.560, Seconds: 2.82\n",
      "Epoch  11/30 Batch  120/1978 - Loss:  1.576, Seconds: 2.59\n",
      "Epoch  11/30 Batch  130/1978 - Loss:  1.471, Seconds: 2.84\n",
      "Epoch  11/30 Batch  140/1978 - Loss:  1.650, Seconds: 2.94\n",
      "Epoch  11/30 Batch  150/1978 - Loss:  1.609, Seconds: 3.08\n",
      "Epoch  11/30 Batch  160/1978 - Loss:  1.623, Seconds: 3.29\n",
      "Epoch  11/30 Batch  170/1978 - Loss:  1.702, Seconds: 3.02\n",
      "Epoch  11/30 Batch  180/1978 - Loss:  1.717, Seconds: 2.66\n",
      "Epoch  11/30 Batch  190/1978 - Loss:  1.738, Seconds: 2.85\n",
      "Epoch  11/30 Batch  200/1978 - Loss:  1.696, Seconds: 2.97\n",
      "Epoch  11/30 Batch  210/1978 - Loss:  1.581, Seconds: 3.32\n",
      "Epoch  11/30 Batch  220/1978 - Loss:  1.465, Seconds: 3.49\n",
      "Epoch  11/30 Batch  230/1978 - Loss:  1.479, Seconds: 3.50\n",
      "Epoch  11/30 Batch  240/1978 - Loss:  1.394, Seconds: 3.07\n",
      "Epoch  11/30 Batch  250/1978 - Loss:  1.418, Seconds: 3.55\n",
      "Epoch  11/30 Batch  260/1978 - Loss:  1.369, Seconds: 3.60\n",
      "Epoch  11/30 Batch  270/1978 - Loss:  1.444, Seconds: 2.97\n",
      "Epoch  11/30 Batch  280/1978 - Loss:  1.482, Seconds: 3.28\n",
      "Epoch  11/30 Batch  290/1978 - Loss:  1.489, Seconds: 3.45\n",
      "Epoch  11/30 Batch  300/1978 - Loss:  1.400, Seconds: 3.60\n",
      "Epoch  11/30 Batch  310/1978 - Loss:  1.361, Seconds: 3.78\n",
      "Epoch  11/30 Batch  320/1978 - Loss:  1.400, Seconds: 3.99\n",
      "Epoch  11/30 Batch  330/1978 - Loss:  1.384, Seconds: 3.82\n",
      "Epoch  11/30 Batch  340/1978 - Loss:  1.359, Seconds: 3.88\n",
      "Epoch  11/30 Batch  350/1978 - Loss:  1.426, Seconds: 3.93\n",
      "Epoch  11/30 Batch  360/1978 - Loss:  1.316, Seconds: 3.40\n",
      "Epoch  11/30 Batch  370/1978 - Loss:  1.379, Seconds: 3.44\n",
      "Epoch  11/30 Batch  380/1978 - Loss:  1.174, Seconds: 3.99\n",
      "Epoch  11/30 Batch  390/1978 - Loss:  1.280, Seconds: 3.48\n",
      "Epoch  11/30 Batch  400/1978 - Loss:  1.245, Seconds: 4.02\n",
      "Epoch  11/30 Batch  410/1978 - Loss:  1.478, Seconds: 4.01\n",
      "Epoch  11/30 Batch  420/1978 - Loss:  1.466, Seconds: 4.10\n",
      "Epoch  11/30 Batch  430/1978 - Loss:  1.474, Seconds: 4.00\n",
      "Epoch  11/30 Batch  440/1978 - Loss:  1.436, Seconds: 3.74\n",
      "Epoch  11/30 Batch  450/1978 - Loss:  1.413, Seconds: 4.21\n",
      "Epoch  11/30 Batch  460/1978 - Loss:  1.324, Seconds: 3.95\n",
      "Epoch  11/30 Batch  470/1978 - Loss:  1.316, Seconds: 4.26\n",
      "Epoch  11/30 Batch  480/1978 - Loss:  1.397, Seconds: 3.70\n",
      "Epoch  11/30 Batch  490/1978 - Loss:  1.320, Seconds: 4.14\n",
      "Epoch  11/30 Batch  500/1978 - Loss:  1.474, Seconds: 4.18\n",
      "Epoch  11/30 Batch  510/1978 - Loss:  1.475, Seconds: 4.34\n",
      "Epoch  11/30 Batch  520/1978 - Loss:  1.352, Seconds: 3.95\n",
      "Epoch  11/30 Batch  530/1978 - Loss:  1.311, Seconds: 4.39\n",
      "Epoch  11/30 Batch  540/1978 - Loss:  1.307, Seconds: 3.69\n",
      "Epoch  11/30 Batch  550/1978 - Loss:  1.317, Seconds: 4.44\n",
      "Epoch  11/30 Batch  560/1978 - Loss:  1.364, Seconds: 4.17\n",
      "Epoch  11/30 Batch  570/1978 - Loss:  1.337, Seconds: 4.35\n",
      "Epoch  11/30 Batch  580/1978 - Loss:  1.314, Seconds: 4.34\n",
      "Epoch  11/30 Batch  590/1978 - Loss:  1.293, Seconds: 4.38\n",
      "Epoch  11/30 Batch  600/1978 - Loss:  1.299, Seconds: 4.37\n",
      "Epoch  11/30 Batch  610/1978 - Loss:  1.221, Seconds: 4.54\n",
      "Epoch  11/30 Batch  620/1978 - Loss:  1.321, Seconds: 4.56\n",
      "Epoch  11/30 Batch  630/1978 - Loss:  1.428, Seconds: 4.58\n",
      "Epoch  11/30 Batch  640/1978 - Loss:  1.372, Seconds: 4.46\n",
      "Epoch  11/30 Batch  650/1978 - Loss:  1.287, Seconds: 4.60\n",
      "Average loss for this update: 1.443\n",
      "No Improvement.\n",
      "Stopping Training.\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/device:XLA_GPU:0'):\n",
    "    learning_rate_decay = 0.95\n",
    "    min_learning_rate = 0.0005\n",
    "    display_step = 10\n",
    "    stop_early = 0 \n",
    "    stop = 3 \n",
    "    per_epoch = 3 \n",
    "    update_check = (len(sorted_texts_short)//batch_size//per_epoch)-1\n",
    "\n",
    "    update_loss = 0 \n",
    "    batch_loss = 0\n",
    "    summary_update_loss = [] \n",
    "\n",
    "    checkpoint = \"./model_economy_final/real_title.ckpt\" \n",
    "    with tf.Session(graph=train_graph, config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "    #     writer = tf.summary.FileWriter('./model_IT/graph_proto', train_graph)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    #     writer.flush()\n",
    "    \n",
    "        for epoch_i in range(1, epochs+1):\n",
    "            update_loss = 0\n",
    "            batch_loss = 0\n",
    "            for batch_i, (summaries_batch, texts_batch, summaries_lengths, texts_lengths) in enumerate(\n",
    "                    get_batches(sorted_summaries_short, sorted_texts_short, batch_size)):\n",
    "                start_time = time.time()\n",
    "                _, loss = sess.run(\n",
    "                    [train_op, cost],\n",
    "                    {input_data: texts_batch,\n",
    "                     targets: summaries_batch,\n",
    "                     lr: learning_rate,\n",
    "                     summary_length: summaries_lengths,\n",
    "                     text_length: texts_lengths,\n",
    "                     keep_prob: keep_probability})\n",
    "\n",
    "                batch_loss += loss\n",
    "                update_loss += loss\n",
    "                end_time = time.time()\n",
    "                batch_time = end_time - start_time\n",
    "\n",
    "                if batch_i % display_step == 0 and batch_i > 0:\n",
    "                    print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
    "                          .format(epoch_i,\n",
    "                                  epochs, \n",
    "                                  batch_i, \n",
    "                                  len(sorted_texts_short) // batch_size, \n",
    "                                  batch_loss / display_step, \n",
    "                                  batch_time*display_step))\n",
    "                    batch_loss = 0\n",
    "\n",
    "                if batch_i % update_check == 0 and batch_i > 0:\n",
    "                    print(\"Average loss for this update:\", round(update_loss/update_check,3))\n",
    "                    summary_update_loss.append(update_loss)\n",
    "\n",
    "                    if update_loss <= min(summary_update_loss):\n",
    "                        print('New Record!') \n",
    "                        stop_early = 0\n",
    "                        saver = tf.train.Saver() \n",
    "                        saver.save(sess, checkpoint)\n",
    "\n",
    "                    else:\n",
    "                        print(\"No Improvement.\")\n",
    "                        stop_early += 1\n",
    "                        if stop_early == stop:\n",
    "                            break\n",
    "                    update_loss = 0\n",
    "\n",
    "            learning_rate *= learning_rate_decay\n",
    "            if learning_rate < min_learning_rate:\n",
    "                learning_rate = min_learning_rate\n",
    "\n",
    "            if stop_early == stop:\n",
    "                print(\"Stopping Training.\")\n",
    "                break\n",
    "    #         writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NMtmMK4ThPOC"
   },
   "outputs": [],
   "source": [
    "def text_to_seq(text):\n",
    "    \n",
    "    text = clean_text(text)\n",
    "    return [vocab_to_int.get(word, vocab_to_int['<UNK>']) for word in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217
    },
    "colab_type": "code",
    "id": "uBCGY2Ztn89c",
    "outputId": "ea6d4a29-6cb3-4a1c-e310-ba0b45379cba",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model_economy_final/real_title.ckpt\n",
      "Original Text: ⓒ 한국거래소 신종 코로나 바이러스 감염증 코로나 19 사태 와 관련 해 금융 위원회 로부터 감사 사업 보 고서 제 출시 한 연장 을 승인 받 은 기업 에 대해 기업지배구조 보고서 공시 시한 이 오 는 7 월 15 일 까지 연장 된다 31 일 한국 거래소 는 기업지배구조 보고서 가이드라인 에서 이 같 은 내용 을 발표 했 다 또한 기존 발표 된 가이드라인 의 10 개 핵심 원칙 을 세분 화 해 이 사회 활동 내역 과 이사 선임 감사 인 에 대한 정보 요구 수준 을 강화 했 는데 이 는 2019 년 도 보고서 에 대한 전수 점검 결과 전반 적 으로 가이드라인 에 따라 충실히 기재 하 고 있 으나 일부 기업 에서 미흡 한 부분 이 발견 됨 에 따라 이 를 수정 보완 한 것 이 다 의무 제출 200 사 자율 제출 9 사 등 총 209 사 중 금융 위원회 지정 양식 보고서 로 공시 한 금융 회사 39 사 를 제외 한 비금 융사 170 사 에 대한 전수 조사 를 실시 했 다 특히 핵심 지표 준수 현황 과 본문 의 내용 이 불 일치 하 는 경우 제도 적 장치 의 도입 여부 만 기재 하 고 상세 한 설명 이 미진 한 경우 가이드라인 에서 기재 대상 의 의미 가 불명확 하 거나 다른 부분 과 중복 등 으로 혼란 이 초래 되 는 경우 를 해소 하 는 방향 으로 개정 됐 다 또한 설문 조사 결과 다수 기업 들 이 작성 사례 를 요청 함 에 따라 세부 원칙 별 모범 사례 및 기피 사례 를 구분 제시 함 으로써 상장 기업 이 올해 보고서 작성 시 참고 할 수 있 도록 했 다는 설명 이 다 거래소 는 가이드라인 을 구성 하 는 10 개 핵심 원칙 에 대해 기존 세부 원칙 23 개 필수 기재사항 30 개 에서 세부 원칙 27 개 필수 기재사항 60 개 로 개정 했 다 또 필수 기재사항 에 구분 번호 를 부여 해 기재 누락 방지 및 보고서 간 비교 가능 성 을 높여 이용자 의 편의 를 높였 다 마지막 으로 최근 법 개정 사항 등 을 반영 하 고 기재 요구사항 에 대한 설명 이 불 명확 한 부분 에 대해 개선 했 다 경영 활동 의 중심 인 이사회 의 활동 및 개별 이사 와 관련 된 정보 공개 요구 를 대폭 강화 하 고 감사 및 외부 감사 인 관련 정보 요구 수준 도 제고 하 도록 했 다 또 이 사회 의 전문 성 과 다양 성 확보 를 위해 개별 이사 의 전문 성 에 대한 구체 적 인 설명 성별 및 겸직 여부 를 명시 하 도록 했 다 이사 선임 과 관련 해서 는 주주 권익 에 침해 가 발생 할 수 있 는 사항 의 범위 를 과거 횡령 배임 또는 자본 시장법 상 불공정 거래 행위 혐의자 까지 포함 하 도록 대폭 확대 했 다 또한 최근 상법 시행령 개정 사항 을 반영 해 장기 재직 당해 회사 6 년 초과 혹은 계열 회사 포함 9 년 초과 사외 이사 존재 시 그 현황 과 사유 를 공개 하 도록 했 다 또 사외 이사 의 독립 적 활동 을 지원 하 는 정책 과 그 실시 여부 를 엄격히 구분 해 기재 하 도록 했으며 사외 이사 에 대한 보수 산정 또는 재선 임시 평가 결과 의 반영 여부 등 과 재 선임 이사 후보 의 이사회 활동 내역 을 명확히 설명 하 도록 정보 공개 요건 을 강화 했 다 이 사회 의 심의 의결 권한 범위 감사 의 주주 총회 참석 여부 및 외부 감사 인 에 대한 정보 제공 정기 주총 6 주 전 감사 전 재무제표 제출 여부 등 의 적시 성 여부 에 대해 정확 한 정보 제공 이 이뤄지도록 개선 했 다\n",
      "\n",
      "Text\n",
      "  Word Ids:    [42863, 11050, 725, 24, 396, 19857, 24, 25, 411, 332, 2072, 280, 98, 6251, 11901, 4378, 263, 450, 10887, 1430, 207, 86, 2392, 545, 5769, 172, 22, 677, 83, 20505, 15745, 2839, 853, 14848, 303, 974, 222, 355, 157, 189, 158, 481, 2392, 1695, 1304, 158, 255, 2494, 222, 15745, 2839, 13703, 54, 303, 7630, 22, 824, 545, 4331, 452, 224, 15025, 59, 4331, 672, 13703, 249, 1108, 183, 4199, 7444, 545, 45864, 168, 280, 303, 265, 3196, 8455, 838, 1421, 2096, 4378, 425, 83, 505, 652, 3546, 782, 545, 625, 452, 1351, 303, 222, 1911, 147, 180, 2839, 83, 505, 6974, 283, 5073, 7905, 288, 66, 13703, 83, 4403, 20363, 692, 432, 278, 1039, 4207, 95, 677, 54, 8955, 86, 4837, 303, 2432, 42753, 83, 4403, 303, 992, 2399, 7650, 86, 990, 303, 224, 1305, 513, 5250, 154, 3720, 513, 329, 154, 239, 32, 9065, 154, 244, 98, 6251, 1868, 15653, 2839, 121, 853, 86, 98, 518, 3674, 154, 992, 3561, 86, 63846, 14016, 13897, 154, 83, 505, 6974, 1803, 992, 163, 452, 224, 28886, 4199, 567, 8999, 2662, 838, 7755, 249, 824, 303, 1363, 18961, 432, 222, 11554, 2189, 288, 5655, 249, 601, 2721, 133, 692, 432, 278, 24849, 86, 1854, 303, 30257, 86, 11554, 13703, 54, 692, 336, 249, 9205, 36, 17663, 432, 2006, 4069, 4837, 838, 7762, 239, 66, 1849, 303, 11852, 258, 222, 11554, 992, 4100, 432, 222, 5620, 66, 510, 875, 224, 15025, 12548, 1803, 5073, 7279, 677, 88, 303, 16171, 3524, 992, 2778, 880, 83, 4403, 10572, 7444, 4389, 5761, 3524, 20, 522, 3524, 992, 9834, 8206, 880, 34856, 226, 677, 303, 1009, 2839, 16171, 584, 28763, 989, 211, 1039, 2953, 452, 4160, 1854, 303, 224, 2494, 222, 13703, 545, 2821, 432, 222, 1108, 183, 4199, 7444, 83, 20505, 59, 10572, 7444, 4272, 183, 2760, 66730, 2114, 183, 54, 10572, 7444, 3563, 183, 2760, 66730, 343, 183, 121, 510, 452, 224, 1048, 2760, 66730, 83, 9834, 1435, 992, 9384, 280, 692, 11794, 2498, 20, 2839, 12, 7727, 557, 1324, 545, 5496, 3650, 249, 4345, 992, 6156, 224, 6223, 66, 5541, 442, 510, 7496, 239, 545, 2400, 432, 278, 692, 33170, 83, 505, 1854, 303, 1363, 1504, 86, 4837, 83, 20505, 2349, 452, 224, 113, 3196, 249, 4598, 425, 3393, 249, 3196, 20, 4802, 1421, 332, 2072, 672, 652, 852, 3546, 992, 1957, 625, 432, 278, 4378, 20, 3260, 4378, 425, 2072, 652, 3546, 782, 180, 9024, 432, 2953, 452, 224, 1048, 303, 265, 249, 3195, 1324, 838, 300, 1324, 1028, 992, 328, 4802, 1421, 249, 3195, 1324, 83, 505, 8453, 288, 425, 1854, 23040, 20, 15746, 2721, 992, 17596, 432, 2953, 452, 224, 1421, 2096, 838, 2072, 8767, 222, 665, 9703, 83, 8735, 36, 314, 989, 211, 1039, 222, 7496, 249, 8866, 992, 9689, 2360, 8826, 19056, 8357, 8640, 186, 3239, 928, 2376, 42663, 481, 2602, 432, 2953, 1957, 965, 452, 224, 15025, 5541, 19821, 9485, 510, 7496, 545, 2400, 280, 351, 17056, 7852, 518, 135, 147, 4210, 31961, 8612, 518, 2602, 329, 147, 4210, 2200, 1421, 17326, 584, 3020, 2662, 838, 5154, 992, 852, 432, 2953, 452, 224, 1048, 2200, 1421, 249, 9325, 288, 3196, 545, 103, 432, 222, 266, 838, 3020, 163, 2721, 992, 50148, 9834, 280, 692, 432, 2953, 42861, 2200, 1421, 83, 505, 1422, 3665, 19056, 6194, 1654, 3258, 5073, 249, 2400, 2721, 239, 838, 715, 2096, 1421, 5422, 249, 3393, 3196, 8455, 545, 16993, 1854, 432, 2953, 652, 852, 3743, 545, 625, 452, 224, 303, 265, 249, 3951, 6805, 12547, 8866, 4378, 249, 665, 1065, 1983, 2721, 20, 3260, 4378, 425, 83, 505, 652, 1886, 2766, 110, 135, 708, 382, 4378, 382, 25685, 513, 2721, 239, 249, 14535, 1324, 2721, 83, 20505, 9596, 86, 652, 1886, 303, 26354, 2349, 452, 224]\n",
      "  Input Words: ⓒ 한국거래소 신종 코로나 바이러스 감염증 코로나 19 사태 와 관련 해 금융 위원회 로부터 감사 사업 보 고서 제 출시 한 연장 을 승인 받 은 기업 에 대해 기업지배구조 보고서 공시 시한 이 오 는 7 월 15 일 까지 연장 된다 31 일 한국 거래소 는 기업지배구조 보고서 가이드라인 에서 이 같 은 내용 을 발표 했 다 또한 기존 발표 된 가이드라인 의 10 개 핵심 원칙 을 세분 화 해 이 사회 활동 내역 과 이사 선임 감사 인 에 대한 정보 요구 수준 을 강화 했 는데 이 는 2019 년 도 보고서 에 대한 전수 점검 결과 전반 적 으로 가이드라인 에 따라 충실히 기재 하 고 있 으나 일부 기업 에서 미흡 한 부분 이 발견 됨 에 따라 이 를 수정 보완 한 것 이 다 의무 제출 200 사 자율 제출 9 사 등 총 209 사 중 금융 위원회 지정 양식 보고서 로 공시 한 금융 회사 39 사 를 제외 한 비금 융사 170 사 에 대한 전수 조사 를 실시 했 다 특히 핵심 지표 준수 현황 과 본문 의 내용 이 불 일치 하 는 경우 제도 적 장치 의 도입 여부 만 기재 하 고 상세 한 설명 이 미진 한 경우 가이드라인 에서 기재 대상 의 의미 가 불명확 하 거나 다른 부분 과 중복 등 으로 혼란 이 초래 되 는 경우 를 해소 하 는 방향 으로 개정 됐 다 또한 설문 조사 결과 다수 기업 들 이 작성 사례 를 요청 함 에 따라 세부 원칙 별 모범 사례 및 기피 사례 를 구분 제시 함 으로써 상장 기업 이 올해 보고서 작성 시 참고 할 수 있 도록 했 다는 설명 이 다 거래소 는 가이드라인 을 구성 하 는 10 개 핵심 원칙 에 대해 기존 세부 원칙 23 개 필수 기재사항 30 개 에서 세부 원칙 27 개 필수 기재사항 60 개 로 개정 했 다 또 필수 기재사항 에 구분 번호 를 부여 해 기재 누락 방지 및 보고서 간 비교 가능 성 을 높여 이용자 의 편의 를 높였 다 마지막 으로 최근 법 개정 사항 등 을 반영 하 고 기재 요구사항 에 대한 설명 이 불 명확 한 부분 에 대해 개선 했 다 경영 활동 의 중심 인 이사회 의 활동 및 개별 이사 와 관련 된 정보 공개 요구 를 대폭 강화 하 고 감사 및 외부 감사 인 관련 정보 요구 수준 도 제고 하 도록 했 다 또 이 사회 의 전문 성 과 다양 성 확보 를 위해 개별 이사 의 전문 성 에 대한 구체 적 인 설명 성별 및 겸직 여부 를 명시 하 도록 했 다 이사 선임 과 관련 해서 는 주주 권익 에 침해 가 발생 할 수 있 는 사항 의 범위 를 과거 횡령 배임 또는 자본 시장법 상 불공정 거래 행위 혐의자 까지 포함 하 도록 대폭 확대 했 다 또한 최근 상법 시행령 개정 사항 을 반영 해 장기 재직 당해 회사 6 년 초과 혹은 계열 회사 포함 9 년 초과 사외 이사 존재 시 그 현황 과 사유 를 공개 하 도록 했 다 또 사외 이사 의 독립 적 활동 을 지원 하 는 정책 과 그 실시 여부 를 엄격히 구분 해 기재 하 도록 했으며 사외 이사 에 대한 보수 산정 또는 재선 임시 평가 결과 의 반영 여부 등 과 재 선임 이사 후보 의 이사회 활동 내역 을 명확히 설명 하 도록 정보 공개 요건 을 강화 했 다 이 사회 의 심의 의결 권한 범위 감사 의 주주 총회 참석 여부 및 외부 감사 인 에 대한 정보 제공 정기 주총 6 주 전 감사 전 재무제표 제출 여부 등 의 적시 성 여부 에 대해 정확 한 정보 제공 이 이뤄지도록 개선 했 다\n",
      "\n",
      "Summary\n",
      "  Word Ids:       [24, 25, 6096, 19973, 13750]\n",
      "  Response Words: 코로나 19 한은 환수 가로막\n",
      "  원제: 거래소 기업지배구조 보고서 가이드라인 개정\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/device:GPU:0'):\n",
    "    random = np.random.randint(0,len(tokens_texts))\n",
    "    input_sentence = tokens_texts[random]\n",
    "    text = text_to_seq(tokens_texts[random])\n",
    "    ans = tokens_summaries[random]\n",
    "\n",
    "    checkpoint = \"./model_economy_final/real_title.ckpt\"\n",
    "\n",
    "    loaded_graph = tf.Graph()\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
    "        loader.restore(sess, checkpoint)\n",
    "\n",
    "        input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "        logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "        text_length = loaded_graph.get_tensor_by_name('text_length:0')\n",
    "        summary_length = loaded_graph.get_tensor_by_name('summary_length:0')\n",
    "        keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "\n",
    "        answer_logits = sess.run(logits, {input_data: [text]*batch_size, \n",
    "                                          summary_length: [np.random.randint(10, 12)], \n",
    "                                          text_length: [len(text)]*batch_size,\n",
    "                                          keep_prob: 1.0})[0] \n",
    "\n",
    "    pad = vocab_to_int[\"<PAD>\"] \n",
    "\n",
    "    print('Original Text:', input_sentence)\n",
    "\n",
    "    print('\\nText')\n",
    "    print('  Word Ids:    {}'.format([i for i in text]))\n",
    "    print('  Input Words: {}'.format(\" \".join([int_to_vocab[i] for i in text])))\n",
    "\n",
    "    print('\\nSummary')\n",
    "    print('  Word Ids:       {}'.format([i for i in answer_logits if i != pad]))\n",
    "    print('  Response Words: {}'.format(\" \".join([int_to_vocab[i] for i in answer_logits if i != pad])))\n",
    "    print('  원제:', ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = Rouge()\n",
    "reference = ans\n",
    "hypothesis = \" \".join([int_to_vocab[i] for i in answer_logits if i != pad])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'R' : 겹치는 단어 / 원제의 단어\n",
    "    - R이 1이라면 원제의 모든 단어를 사용하여 제목을 만들었음을 의미.\n",
    "    - 아무리 긴 문장을 만들어도 단어를 다 사용했다면 1이 나오는 문제 있음.\n",
    "    \n",
    "- 'P' : 겹치는 단어 / 요약문의 단어\n",
    "    - 모델이 생성한 문장 중 실제 제목과 연관있는 단어 비율.\n",
    "    \n",
    "- 'ROUGE-1'과 'ROUGE-2'의 차이\n",
    "    - ROUGE-1은 unigram을 비교\n",
    "        - 예) the, cat, was, found, under, the, bed\n",
    "    - ROUGE-2는 bigram을 비교\n",
    "        - 예) the cat, cat was, was found, found under, under the, the bed\n",
    "        \n",
    "- 'ROUGE-L' : 최장 길이로 매칭되는 문자열을 측정\n",
    "    - ROUGE-2와 달리 연속적 매칭을 요구하지않음\n",
    "    - 유연한 성능 비교가 가능\n",
    "    \n",
    "- 'ROUGE-S' : window size가 주어지면, window size내의 단어쌍들을 묶어서 비교.\n",
    "    - 예) cat in the hat / window size = 2\n",
    "        - cat in, cat the, cat hat, in the, in hat, the hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rouge-1': {'f': 0.4999999950347222,\n",
       "   'p': 0.5454545454545454,\n",
       "   'r': 0.46153846153846156},\n",
       "  'rouge-2': {'f': 0.36363635867768596, 'p': 0.4, 'r': 0.3333333333333333},\n",
       "  'rouge-l': {'f': 0.5714285667120181, 'p': 0.75, 'r': 0.46153846153846156}}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = rouge.get_scores(hypothesis, reference)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "S2S_ko.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "471px",
    "left": "1478px",
    "right": "20px",
    "top": "112px",
    "width": "351px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
